\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{positioning, shapes.geometric, arrows.meta}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{Automated LaTeX Code Generation from Handwritten Mathematical Expressions \\
Category: Computer Vision}

\author{
  Jayaprakash Sundararaj \\
  \texttt{osjp@stanford.edu}  \\
  \AND
  Akhil Vyas \\
  \texttt{avyas21@stanford.edu}  \\
  \AND
  Benjamin Gonzalez-Maldonado \\
  \texttt{bengm@stanford.edu } \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\begin{center}
\includegraphics[width=3cm, height=0.7cm]{CS230}
\end{center}

\maketitle

\begin{abstract}
Training a model that learns handwritten mathematical expressions from images and generates equivalent LaTeX code. The goal is experiment and study different model architectures (CNN, LSTM, etc) and hyper-parameters, evaluate the with different evaluation metrics, and share our finding.
\end{abstract}

\section{Introduction}	

Converting handwritten mathematical expressions into digital formats is time consuming, specifically LaTeX code. Our goal is to train a ML model that is capable of encoding handwritten notes and converting to the source code seamlessly. The input to our algorithm is an image of a handwritten mathematical expression. The challenge of our project is to convert an image to a text LaTeX sequence which will require the use of both computer vision and NLP techniques. We will use concepts related to these areas that we learn from this course to train the model. We will explore different evaluation metrics (text based, and image based), and share our findings.

% Explain the problem and why it is important. Discuss your motivation for pursuing this
% problem. Give some background if necessary. Clearly state what the input and output
% is. Be very explicit: “The input to our algorithm is an {image, amplitude, patient age,
% rainfall measurements, grayscale video, etc.}. We then use a {SVM, neural network, linear
% regression, etc.} to output a predicted {age, stock price, cancer type, music genre, etc.}.”
% This is very important since different teams have different inputs/outputs spanning different
% application domains. Being explicit about this makes it easier for readers. If you are using
% your project for multiple classes, add a paragraph explaining which components of the
% project were used for each class.

\section{Related work}
% [You should find existing papers, group them into categories based on their approaches,
% and discuss their strengths and weaknesses, as well as how they are similar to and differ
% from your work. In your opinion, which approaches were clever/good? What is the stateof-the-art?
% Do most people perform the task by hand? You should aim to have at least
% 5 references in the related work. Include previous attempts by others at your problem,
% previous technical methods, or previous learning algorithms. Google Scholar is very useful
% for this: https://scholar.google.com/ (you can click “cite” and it generates MLA, APA,
% BibTeX, etc.)]
\cite{schechter2017converting} investigated a variety of methods like neural networks, CNNs, Random Forests, SVMs, OCR, CGrp, and SA. However, most state of the art the methods utilize encoder-decoder architectures involving CNNs and LSTM architectures like \cite{genthial2016image}. In recent works like \cite{bian2022handwritten}, both left-to-right and right-to-left decoders are utilized. In our work, we will explore different hyper-parameters and model architectures such as attention mechanisms which were never tried before. 

\section{Dataset and Features}

We will use the datasets from two main repositories: \texttt{Im2latex-100k} (\cite{kanervisto_2016_56198}) and \texttt{Im2latex-230k} (\cite{gervais2024mathwritingdatasethandwrittenmathematical}). These datasets consist of images of mathematical formulas paired with their corresponding LaTeX code (two features). The Latex code is variable length.

\subsection*{Im2latex-100k}
The \texttt{Im2latex-100k} (\cite{kanervisto_2016_56198}) dataset, available at \href{https://zenodo.org/records/11230382}{Zenodo}, contains 100,000 image-formula pairs. Each image is in PNG format with a fixed size, and the formulas are extracted from ArXiv papers. This dataset is a cleaned-up version from the Cornell KDD competition (KDD Cup 2003).

\begin{itemize}
    \item Number of Examples: 100,000
    \item Data Format: \texttt{<image file name> <formula id>}
    \item Source: ArXiv mathematical papers
\end{itemize}

\subsection*{Im2latex-230k}
The \texttt{Im2latex-230k} (\cite{gervais2024mathwritingdatasethandwrittenmathematical}) 
 dataset, also known as \texttt{Im2latexv2}, contains 230,000 samples. It includes both OpenAI-generated and handwritten examples, further enhancing the diversity of the data. This dataset is available at \href{https://im2markup.yuntiandeng.com/data/}{Im2markup}.

\begin{itemize}
    \item Number of Examples: 230,000
    \item Data Format: \texttt{<image file name> <formula id>}
    \item Source: ArXiv papers, OpenAI-generated, and handwritten samples
\end{itemize}

% Describe your dataset: how many training/validation/test examples do you have? Is there
% any preprocessing you did? What about normalization or data augmentation? What is the
% resolution of your images? How is your time-series data discretized? Include a citation on
% where you obtained your dataset from. Depending on available space, show some examples
% from your dataset. You should also talk about the features you used. If you extracted
% features using Fourier transforms, word2vec, PCA,
% ICA, etc. make sure to talk about it. Try to include examples of your data in the report
% (e.g. include an image, show a waveform, etc.).


\section{ Methods }
In this project, we will use a Convolutional Neural Network (CNN) (\cite{OSheaN15}) combined with sequence-to-sequence (Seq2Seq) (\cite{6795963}) model to convert handwritten mathematical expressions into LaTeX code. The input to our algorithm is an image  \( I \), and the output is the corresponding LaTeX code, denoted as a sequence of tokens \( T = \{t_1, t_2, \dots, t_n\} \).

The Seq2Seq model, which consists of an encoder (the CNN) and a decoder (RNN, or LSTM), is trained for generate the LaTeX code token by token. At each time step \( t \), the decoder predicts the next token \( t_t \) given the previous tokens and the context vector \( c_t \). The probability distribution for the next token is computed as:
\[
P(t_t | t_{1:t-1}, c_t) = \text{softmax}(W_o h_t)
\]
where \( h_t \) is the hidden state of the decoder at time step \( t \), and \( W_o \) is a weight matrix.


\begin{tikzpicture}[
    node distance=1cm, 
    every node/.style={rectangle, draw, minimum height=1.2cm, minimum width=1cm, align=center},
    arrow/.style={-Stealth, thick}
]

% Nodes
\node (input) [rounded corners] {Input Image \(I\)};
\node (cnn) [right=0.8cm of input] {CNN (Encoder)};
\node (lstm) [right=0.8cm of cnn] {LSTM (Decoder)};
\node (output) [right=0.8cm of lstm, rounded corners] {Output LaTeX Sequence \(T\)};

% Arrows
\draw [arrow] (input) -- (cnn);
\draw [arrow] (cnn) -- (lstm);
\draw [arrow] (lstm) -- (output);

\end{tikzpicture}

We'll use the cross-entropy loss function to optimize the model during training. Also, we plan to explore different attention mechanisms and extracting salient features as we iterate on the experimentation.


\section{Experiments and Evaluation}

We will use the following metrics to evaluate the performance of the model for the LaTeX code generation task. The text based metrics compare the original LaTeX code with generated LaTeX code, however, the image based metrics compare the PDF images generated from original and generated LaTeX code.

\begin{itemize}
    \item Text Metrics: \textbf{BLEU Score} (\cite{papineni-etal-2002-bleu})
    \item Text Metrics: \textbf{Levenshtein Distance}
    \item Image metrics: Compute the \textbf{accuracy} between images generated from original latex code and generated latex code. 
\end{itemize}

The experimentation will involve choosing different CNN layers (striding, pooling), learning rate, mini batch size. 

% \section{Conclusion/Future Work}
% Empowered by a large existing dataset with thousands of examples, as well as ample existing research and methodologies with similar problems, we will be able to effectively develop a model to accurately convert notes to LaTeX, while also adjusting existing architectures to experiment and ultimately determine an optimal architecture. With more time, future steps might involve processing entire documents into LaTeX rather than single lines at a time and outputting an entire file of LaTeX instead of a single line. It would also be useful to develop a larger and more variable dataset that can improve performance with messier handwriting, photo quality, distance, etc.
% Summarize your report and reiterate key points. Which algorithms were the highestperforming?
% Why do you think that some algorithms worked better than others? For
% future work, if you had more time, more team members, or more computational resources,
% what would you explore?

% \section{Contributions}
% The contributions section is not included in the 5 page limit. This section should describe
% what each team member worked on and contributed to the project.

% \section*{References}
% This section should include citations for: (1) Any papers mentioned in the related work
% section. (2) Papers describing algorithms that you used which were not covered in class.
% (3) Code or libraries you downloaded and used. This includes libraries such as scikit-learn, Tensorflow, Pytorch, Keras etc. Acceptable formats include: MLA, APA, IEEE. If you
% do not use one of these formats, each reference entry must include the following (preferably
% in this order): author(s), title, conference/journal, publisher, year. If you are using TeX,
% you can use any bibliography format which includes the items mentioned above. We are excluding
% the references section from the page limit to encourage students to perform a thorough
% literature review/related work section without being space-penalized if they include more
% references. Any choice of citation style is acceptable
% as long as you are consistent. 

\medskip

\nocite{*}
\bibliography{sample}
\small

% [3] \url{http://ivc.univ-nantes.fr/CROHME/datasets.php}

% [4] \url{http://www.isical.ac.in/~crohme/CROHME\_tasks2.html}

% [1] Wang, H., \& Shan, G. (2020). Recognizing handwritten mathematical expressions as LaTex sequences using a multiscale robust neural network. arXiv preprint arXiv:2003.00817.


\end{document}