{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/osjayaprakash/deeplearning/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xZgd8NVaFjfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/shahrukhkhan/im2latex100k/versions/7\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "root_dir = kagglehub.dataset_download(\"shahrukhkhan/im2latex100k\")\n",
    "# path = kagglehub.dataset_download(\"gregoryeritsyan/im2latex-230k\")\n",
    "\n",
    "print(\"Path to dataset files:\", root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6TKz3RA-FuOp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:47:09.182611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 02:47:10.203545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Tesla K80, compute capability 3.7\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Python Platform: Linux-5.15.0-1071-aws-x86_64-with-glibc2.29\n",
      "Tensor Flow Version: 2.13.1\n",
      "\n",
      "Python 3.8.10 (default, Nov  7 2024, 13:10:47) \n",
      "[GCC 9.4.0]\n",
      "Pandas 2.0.3\n",
      "Scikit-Learn 1.3.2\n",
      "SciPy 1.10.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:47:11.989281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:12.028055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:12.028922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:12.029969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten,\n",
    "                                     Dense, GRU, Embedding, Bidirectional,\n",
    "                                     TimeDistributed, Concatenate, RepeatVector, LSTM, MultiHeadAttention, LayerNormalization, Add, Dropout )\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "from tensorflow.python.ops.numpy_ops import np_config  \n",
    "import einops\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "np_config.enable_numpy_behavior()\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.optimizer.set_jit(False)\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "#print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo6QNApGGgIk"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256  # We'll resize input images to this size\n",
    "IMG_SHAPE = (256, 256, 1)\n",
    "EMBEDDING_DIM = 256\n",
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wAMBW0OFF_VN"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess the input image: Resize and normalize.\"\"\"\n",
    "    image = tf.image.resize(image, (image_size, image_size))  # Resize to (50, 200)\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    \"\"\"Load and preprocess a batch of images.\"\"\"\n",
    "    # Use Gray scale\n",
    "    images = [preprocess_image(tf.io.decode_image(tf.io.read_file(path), channels=1))\n",
    "              for path in image_paths]\n",
    "    return tf.stack(images)\n",
    "\n",
    "def prepare_sequences(latex_texts, max_seq_length):\n",
    "    \"\"\"Convert LaTeX texts to padded sequences of tokens.\"\"\"\n",
    "    sequences = [text_to_sequence(text) for text in latex_texts]\n",
    "    return pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4NIjhtisIi1W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:47:23.156815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:23.157683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:23.158518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:23.828769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:23.829581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:23.830409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 02:47:23.831126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10786 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# %%prun\n",
    "\n",
    "df = pd.read_csv(f\"{root_dir}/im2latex_train.csv\", nrows=n)\n",
    "\n",
    "train_image_paths = []\n",
    "train_latex_texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    train_image_paths += [f\"{root_dir}//formula_images_processed/formula_images_processed/{row.image}\"]\n",
    "    train_latex_texts += [\"[START] \" + row.formula + \" [END]\"]\n",
    "\n",
    "train_images = load_and_preprocess_images(train_image_paths)\n",
    "# Enable Numpy behaviour of TF\n",
    "# tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
    "\n",
    "# vocab_size, max_seq_length = fit_tokenizer(train_latex_texts)\n",
    "\n",
    "# train_sequences = prepare_sequences(train_latex_texts, max_seq_length)\n",
    "# #train_sequences = np.expand_dims(train_sequences, -1)\n",
    "# print(\"train_images:\", train_images.shape)\n",
    "# print(\"train_sequences:\", train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GjYzQHDSAAI1"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, standardize = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WpXdTtsaAAI1"
   },
   "outputs": [],
   "source": [
    "tokenizer.adapt(train_latex_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WJgjaXn2AAI3"
   },
   "outputs": [],
   "source": [
    "latex_labels = tokenizer(train_latex_texts)\n",
    "train_sequences = np.asarray(latex_labels)\n",
    "input_labels = train_sequences[..., :-1]\n",
    "output_labels = train_sequences[..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "az_S8OFzIvc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 149)\n",
      "(1000, 256, 256, 1)\n",
      "{'': 0, '[UNK]': 1, '}': 2, '{': 3, '_': 4, '^': 5, '2': 6, '(': 7, ')': 8, '=': 9, '1': 10, '-': 11, ',': 12, '[START]': 13, '[END]': 14, '+': 15, '\\\\frac': 16, 'i': 17, '0': 18, 'x': 19, 'a': 20, 'n': 21, '\\\\,': 22, '.': 23, 'd': 24, '\\\\mu': 25, 'm': 26, '\\\\partial': 27, 'e': 28, 'p': 29, 'k': 30, 'c': 31, '\\\\;': 32, '~': 33, 'j': 34, 'A': 35, 's': 36, '\\\\alpha': 37, 't': 38, 'r': 39, 'l': 40, '\\\\right)': 41, '\\\\left(': 42, '\\\\nu': 43, '3': 44, 'g': 45, '\\\\prime': 46, '4': 47, '\\\\': 48, '|': 49, '\\\\pi': 50, '\\\\mathrm': 51, '\\\\phi': 52, 'b': 53, '\\\\beta': 54, '[': 55, ']': 56, '\\\\delta': 57, 'z': 58, 'q': 59, 'D': 60, 'T': 61, '\\\\cal': 62, 'R': 63, '\\\\int': 64, '\\\\gamma': 65, '\\\\psi': 66, '\\\\\\\\': 67, 'N': 68, 'B': 69, 'y': 70, '\\\\operatorname': 71, '&': 72, 'S': 73, 'f': 74, '\\\\theta': 75, '\\\\lambda': 76, '/': 77, '\\\\bar': 78, 'L': 79, 'M': 80, 'h': 81, '\\\\hat': 82, 'H': 83, 'E': 84, 'F': 85, '\\\\tilde': 86, '\\\\sqrt': 87, '\\\\sum': 88, 'I': 89, '\\\\xi': 90, '\\\\rho': 91, 'u': 92, '\\\\sigma': 93, 'C': 94, '\\\\tau': 95, '\\\\quad': 96, 'o': 97, 'P': 98, 'G': 99, '\\\\eta': 100, '\\\\Phi': 101, '\\\\epsilon': 102, '\\\\bf': 103, 'V': 104, '\\\\pm': 105, 'X': 106, 'K': 107, '\\\\omega': 108, '\\\\Delta': 109, '\\\\end{array}': 110, '\\\\begin{array}': 111, 'Q': 112, ';': 113, '*': 114, 'Z': 115, 'J': 116, '\\\\infty': 117, '\\\\Gamma': 118, 'U': 119, '\\\\cdot': 120, '\\\\rangle': 121, '\\\\overline': 122, '\\\\right]': 123, '\\\\left[': 124, 'W': 125, '5': 126, '6': 127, '\\\\vec': 128, '\\\\qquad': 129, '\\\\varphi': 130, '\\\\Lambda': 131, 'v': 132, '\\\\kappa': 133, '\\\\equiv': 134, 'w': 135, '\\\\dagger': 136, '\\\\Psi': 137, '8': 138, '\\\\langle': 139, '\\\\Omega': 140, '\\\\varepsilon': 141, '\\\\zeta': 142, '\\\\}': 143, '\\\\Sigma': 144, '\\\\{': 145, '\\\\!': 146, '\\\\vert': 147, '\\\\chi': 148, '\\\\ldots': 149, '\\\\dot': 150, 'Y': 151, '>': 152, '\\\\nabla': 153, '\\\\hspace': 154, '\\\\cdots': 155, '\\\\rightarrow': 156, '\\\\mathcal': 157, '<': 158, ':': 159, '\\\\left\\\\{': 160, '\\\\Pi': 161, '\\\\times': 162, '\\\\sim': 163, '\\\\right|': 164, '\\\\prod': 165, '\\\\:': 166, '7': 167, '!': 168, '\\\\otimes': 169, '\\\\operatorname*': 170, '\\\\left|': 171, 'O': 172, '\\\\wedge': 173, '\\\\right\\\\}': 174, '\\\\ell': 175, '9': 176, '\\\\widehat': 177, '\\\\to': 178, '\\\\Big': 179, '\\\\widetilde': 180, '\\\\in': 181, '\\\\displaystyle': 182, '\\\\approx': 183, '\\\\boldmath': 184, '\\\\mathbf': 185, '\\\\dots': 186, '\\\\ast': 187, '\\\\vartheta': 188, '\\\\stackrel': 189, '\\\\right\\\\rangle': 190, '\\\\mp': 191, '\\\\Theta': 192, '\\\\textstyle': 193, '\\\\right.': 194, '\\\\Bigr': 195, '\\\\Bigl': 196, '\\\\triangle': 197, '\\\\star': 198, '\\\\neq': 199, '\\\\left\\\\langle': 200, '\\\\hbar': 201, '\\\\underline': 202, '\\\\simeq': 203, '\\\\left.': 204, '\\\\Xi': 205, '\\\\sp': 206, '\\\\nonumber': 207, '\\\\vspace': 208, '\\\\mid': 209, '\\\\dag': 210, '\\\\big': 211, '--': 212, '\\\\varrho': 213, '\\\\propto': 214, '\\\\longrightarrow': 215, '\\\\leq': 216, '\\\\geq': 217, '\\\\forall': 218, '\\\\ddot': 219, '\\\\biggl': 220, \"'\": 221, '\\\\textrm': 222, '\\\\small': 223, '\\\\perp': 224, '\\\\oint': 225, '\\\\mapsto': 226, '\\\\imath': 227, '\\\\cong': 228, '\\\\circ': 229, '\\\\biggr': 230, '\\\\bigg': 231, '\\\\Bigg': 232, '\\\\slash': 233, '\\\\scriptsize': 234, '\\\\rbrack': 235, '\\\\overrightarrow': 236, '\\\\oplus': 237, '\\\\lbrack': 238, '\\\\varpi': 239, '\\\\tiny': 240, '\\\\thinspace': 241, '\\\\smallskip': 242, '\\\\scriptstyle': 243, '\\\\rightleftharpoons': 244, '\\\\overleftarrow': 245, '\\\\leftrightarrow': 246, '\\\\le': 247, '\\\\jmath': 248, '\\\\binom': 249, '\\\\bigr': 250, '\\\\bigl': 251, '\\\\Rightarrow': 252, '\\\\wp': 253, '\\\\tt': 254, '\\\\subset': 255, '\\\\rightharpoonup': 256, '\\\\phantom': 257, '\\\\parallel': 258, '\\\\mathop': 259, '\\\\longleftrightarrow': 260, '\\\\left\\\\vert': 261, '\\\\it': 262, '\\\\hline': 263, '\\\\ge': 264, '\\\\footnotesize': 265, '\\\\d': 266, '\\\\c': 267, '\\\\breve': 268, '\\\\bot': 269, '\\\\bigtriangleup': 270, '\\\\b': 271, '\\\\Upsilon': 272, '\\\\Re': 273, '\\\\Longrightarrow': 274, '\\\\L': 275, '\\\\Im': 276}\n"
     ]
    }
   ],
   "source": [
    "print(latex_labels.shape)\n",
    "print(train_images.shape)\n",
    "vocab_dict = {name: id for id, name in enumerate(tokenizer.get_vocabulary())}\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] \\phi ( \\Gamma ) ( m ; p _ { i } ) = \\int _ { 0 } ^ { \\infty } \\frac { F _ { \\Gamma } ( r ; m ; p _ { i } ) } { r } d r , [END]\n",
      "[ 13  52   7 118   8   7  26 113  29   4   3  17   2   8   9  64   4   3\n",
      "  18   2   5   3 117   2  16   3  85   4   3 118   2   7  39 113  26 113\n",
      "  29   4   3  17   2   8   2   3  39   2  24  39  12  14   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "[ 52   7 118   8   7  26 113  29   4   3  17   2   8   9  64   4   3  18\n",
      "   2   5   3 117   2  16   3  85   4   3 118   2   7  39 113  26 113  29\n",
      "   4   3  17   2   8   2   3  39   2  24  39  12  14   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGiCAYAAAC/NyLhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACleUlEQVR4nOy9eZBlZ3nf/z133/et95nRjJbRAliArIL4RyIFSRBijP4wmDjgUFAmEhWQt8iFwVCpyMGuxLGDTSWVArsKsE2VMQVlk1KEJYw9YJARoJE0a89Mb7f77vt+z++PzvPoPafv7b63u2e6b8/zqZqambuc7Z7zft/neZ9F03VdhyAIgiBMCJaDPgBBEARBGAcRLkEQBGGiEOESBEEQJgoRLkEQBGGiEOESBEEQJgoRLkEQBGGiEOESBEEQJgoRLkEQBGGiEOESBEEQJgoRLkEQBGGiODDh+uxnP4tjx47B5XLhvvvuwz/+4z8e1KEIgiAIE8SBCNef//mf44knnsAnP/lJ/NM//RNe85rX4KGHHsLGxsZBHI4gCIIwQWgHUWT3vvvuwxve8Ab8j//xPwAA/X4fc3Nz+MhHPoL/+B//440+HEEQBGGCsN3oHbbbbTz//PN48skn+TWLxYIHH3wQZ86cGfidVquFVqvF/+/3+8jn84hGo9A07bofsyAIgrC/6LqOSqWC6elpWCzjOf9uuHBls1n0ej0kk0nD68lkEq+88srA7zz11FP41Kc+dSMOTxAEQbiBLC0tYXZ2dqzv3HDh2g1PPvkknnjiCf5/qVTC/Pw8lpaWEAgEDvDIBEEQhN1QLpcxNzcHv98/9ndvuHDFYjFYrVasr68bXl9fX0cqlRr4HafTCafTueX1QCAgwiUIgjDB7Ga554ZHFTocDtx777145pln+LV+v49nnnkG999//40+HEEQBGHCOBBX4RNPPIH3ve99eP3rX483vvGN+P3f/33UajX80i/90kEcjiAIgjBBHIhw/fzP/zwymQw+8YlPIJ1O47WvfS2++c1vbgnYEARBEAQzB5LHtVfK5TKCwSBKpZKscQmCIEwgexnHpVahIAiCMFGIcAmCIAgThQiXIAiCMFGIcAmCIAgThQiXIAj7wgTGeR0K5LqNjwiXIAh7Rgbf3SHXbXeIcAmCINxgRLD2hgiXIAj7hgzIO6PrOnRdR7/fP+hDmVhEuARB2Df6/b6I1w70+310u125VntAhEsQhD2h6zpX+JbBeGd0XUev10Ov1zNcO2F0RLgEQdg3RLR2hoRLrtXuEeESBGHfIItLBuXh9Pt9dhfKddodIlyCcEQhARnnz27pdDqo1WpYX19Hu92WAXkAdE16vR6azSY2NjZQqVTQ6XQO+MgmjwNpayIIwvWn3+8b1k+2ExP63G7XW1qtFqrVKtLpNILB4MCO5cImvV4PrVYLq6ursFqtcDqdsNvtB31YE4VYXIJwBKFwawoCUP9t/kPv7SY8m8Sw2Wwim83i3LlzqNfrEuq9Dd1uF/V6HefOncPGxgYajcZBH9LEIcIlCEeUbre7RaS63e6WPyRauxUbTdNQq9WwsbGBF154AZVKRVyFA6Br0u12US6X8cILL2B5eRm1Wo3fl+s2GuIqFIQjCEWuWSwWdv/1ej1+T3UJ9vt9WK1WWCzjzWNpoNU0Dc1mE8ViEVeuXEGj0ZABeBu63S6azSauXLmCfD6PVqt10Ic0cYhwCcIRhSwpTdOgaRqHYJuFy2KxQNd1WK3WXecVNRoNFAoFLC4uinANgK4HTSgajQYWFxeRy+XEVbgLRLgE4Yih6zq63S7S6TQPmJqmGZKDzeJksVjgcDgwPz8Pq9U61r5arRYymQzW1tbQbDaHWnY3M5qmGcRc0zQ0Gg3k83nkcjmDm1Cu2c6IcAnCDcY8gI37/ijbb7fbWFpa4lwhEhEaQM3RhhaLBW63G7Ozswb34ij7ajQayGazKBaLiMVicLlcMvhug91uh9vtRjweR7vdRi6X4/VFuW6jIcIlCDcQ88x6kIiof9Pnxt1Ht9vF8vIyWq0WuwxtNhuLl0q/34fFYoHf7x85QEM9zkajgVwuh2KxiGQyCYfDIQPwAOia2Gw2eL1exONx9Ho9FAoF/g2E0RDhEoQbxKBE32EW117cRrquo9ls4uzZs6jVauh0Ouh2u3C5XAPdgN1uFxaLBZFIBO94xztgt9tH2i8dZ6VSwbVr17C+vo7bb78dHo+H181EwIzoug6n04lgMIhjx46h3W5jZWUFvV5v4KRCGIwIlyAcEIPEySxsuxn8dV1Hp9PBpUuXUKlU0Gw20el04PP52OpS99Fut2G1WhGPx0cOyVYtLqqYkc1mMTU1BafTKQPwAOi62+12+Hw+TE9P49KlS9jY2JC8tzER4RKEG8wo1tReF+p7vR6KxSIymQwLVygUgt1uh8ViMQyUzWaT/z3uAErCVS6X0Ww2EY1G2VUokYWDsVqtcLlciEajuHDhApd9stlsYqmOiAiXIBxRdF1HqVRCsVgEAPh8Ptjt9i3iVK1W2epSLa5RBtB+v49cLodarQZd13mNSxgOlXlKpVIAgGKxiHK5DIfDAZtNhuRRkKskCDcYsxioAmF+bzczb03T4HA4cNtttyGdTqNYLMLr9eL48eMIhUKGoBBd1/HKK68gm83uap2l1+thZWUFNpsNiUQCoVBorHD6mw31dw6Hw3A4HOh0OlhbW4Pf74fT6RSLawREuAThBjKqMO114LLb7ZienuZity6XC6lUCrFYjHO6yC1F4jaum5C2k8vlYLfb4fF44PV6OTpOBt/haJrGFjBFFnY6HXGxjogIlyDcAEaJJNyvqDKyuE6cOAG32w0A8Hg8mJ+fx/T09Bbhunz5MtbW1tDtdsfef7/fx9raGux2O2KxGILB4J4qcNxM0Jpjp9PB+vo6l34S4doZES5BuEEMG8jN7Uf2OuBbLBY4nU7ceeed8Pv9PLs/deoUjh8/zuWfrFYr+v0+fvSjH+HSpUsolUpj77vb7WJpaQl+vx/JZBLhcJjXaUS4tqK6aaPRKJxOJ1qtFpaXl9FutwGIcI2CCJcg3CDUzrdkXVmtVsMAr7Ya0TQNNpttV2tGVqsVwWCQw9/dbjfC4TCi0egW4QoGg3C73VylnBglMKPb7aJYLCIajXIfLhGsndE0DS6XCy6XCzabDcViEZ1OR0RrRES4BOE6oFpR9De1EanX6yxaTqfTUGmi3W6j3W7zeofX6x27diCwaXXRdzVNg8fjQTAYRDgcHihcPp8P1Wp1LHclNUSsVCpwOp0IhUJwOBySkzQiDocDbrcbLpcLhUJBOkePgQiXIOwzVAFc0zRDGZ9ms4lyuYx0Og1N0+B0OhGLxRCJRPhzFBpdqVRgsVgwMzMDh8Oxq5Yj9Xod3W4XVqsVgUAALpeLq2JQG5N+v494PI6pqSl+nb4/TMDMrUxqtRoCgQASicSOx3Q9rLFh273eRWu32++o+4zFYkilUshkMmg0GlL6aUREuARhn6Huw+Zk0na7ze3trVYrPB4PPB4Ph6gDQK1WQ6FQQDabhd1uRyQS2dUsnKq203F4PB5OcKUiuvR3IBBAJBJBu90ea9Bst9uo1+toNpvweDwIBAJ7rvqxX5hrPh5G96Wu6wgEAgiHw7zGRb3RhO0R4RKEfYaEiyCR6HQ6qNVqyGazsFqt8Pv9iEajhsG+VquhWCwim83CZrNhbm5u7H1TAAANhLSeYrfbYbfbAYBFS9M0+P1+hEIhNJtNHuBHER0SrlarBbfbjUAgwOe70/HthVG3MQluN7r25XLZ8HsJ2yPCJQj7DFVnp/UlGkCp2eKVK1dgs9kQCoWQSCQM/atKpRLS6TSuXr0Kh8OBkydP7moWbnYVhkIhOJ1OQ2UGCosnVyGAHfdDlTUsFovBVejz+W6Iq3CQGI3i1rweDLPmxtlnNBpFIpFANptFvV5nF7OwPSJcgrAP0GBVrVbRbDZRrVZhtVrh8/ng9/vR6XRQLpeRzWZRLpd5fYnWNqxWK1qtFgqFAkqlEiqVCux2OyqVCqrVKsLh8NjH02g0uPK76iqkNTiyulwuF7xeL3w+n8Hi2olWq8WRiG63m3PGBl0X82vA9vlso5zfqN8fp0XMbq25cS0l+qzX60UwGES9Xkej0UCz2YTP55M8uB0Q4RKEfaRaraJcLqNYLLJ14/f70Wq1UCqVkM1m2SUEgIXLYrGgXC6jXC6jVqvxZ6rVKiqVysjCpboKm80mW0dOp9OwfkVWntVqhc1mg91uH7nGIA3a7Xabj53ckMP6i5kFkY5rHFEbdJ7m19S1xXG2O059xkEV9Em4xhUbl8sFt9uNTqfD64XqfkS8BiPhK4KwjywvL+P8+fP4yU9+gp/85CdIp9PQdR3lchlLS0u4evUqAHDb9gsXLiCbzWJjYwMXLlzA2toaqtUqgE1xyeVyWFlZ4e2Pum5DfbI6nQ6HxpOg9ft9XpsCwNGE47ojG40GyuUyvF4vHA4Hf18dwCknjV6n3K9ut4t+v78lmINeG3aedA7U2Zleo+9THpx6HP1+n18ftl3a9yih/LQfdd+dTseQfzcqTqcTHo+H8+GoIPIkrM8dJGJxCcIeUQeZ1dVVXLp0CUtLS7Db7Rx8UalUsLa2hqtXryKRSLBbqFgsIp/Pw+l04tKlS0in05ygTMK1uro6lmDRIFytVnlbVEOQBud6vQ5gsxQUBY+MK1zkEnW73dwuRYXOAYAhzJ5ERA0QUd8DMNAaU+n1epxcTZAoqp2eSbhIULZzMZqFdBh0nGRRq0I6SsK4mt/ncrng8Xi4DQ0JF21XLK7BiHAJwj6Sz+extraGa9euwW63o1gs8npTLpfDxsYGYrEYh8bXajVu9kjFbm02G9xuN3q9HiqVCnK53NjHQcEZlGhMFS1I2FqtlsE1aM4522nbwKuuQqr+MGiQVfOSaP9k+ZiDTlRX306o1pMqfKr4qOe7kyU0rnCZLTMKqhjX4iIXbb/fR61W4wmFiNb2iKtQEPaRtbU1XL58GefOncMrr7yCjY0NtrjS6TSWlpYAgK2txcVFzttaXFxEJpPhChb9fh/5fJ7djaOgrtWoUWpkWdGg22g0uDbesPJTO22fLC4KtR8kOKqrkL5HltEgVyF9frvzVcVv0OuDxOt6uQrp/51Oh89pHJxOJ09SaI1T2BmxuARhH6DBOJPJIJPJoFQqodvtolKpsHDRwnsoFEIoFOKIQaqkkc1m4XK5OLfH4XCgUqmgXC6jXq/D5XKN7M7r9/uoVCrsOvP7/bBarej1etyx2OVyAdh047ndbng8nrEEslaroVQqIRgMbqnu0e/30W63sbGxAbvdzr2mqCqIruvw+/3wer2cuNxsNrG2toZgMAiPxwOfz7dlv51Oh63TYDDI9f7q9Trq9TpqtRpsNhsCgQB8Ph+63S5KpRIajQYn/KrJ0iTUlUoF9Xod7XYbx44d2yLCdF0oeKZarWJ2dpbLXuVyOQ6CoWaa21mOtF8KjLHZbGg2m2g0Gob9CYMR4RKEfYCsGMqdcjqdaDabPDNvNptwOBxc6JYGWqfTiUKhAE3T0Ol0MD09jWg0inA4DJfLxYv+lUpl5PUTsh6azSb6/T73yrJYLCxctVqNhZSEy+v1Atg5V4r+brVaqNfrHGqvfrfX66HRaGB9fR02mw2tVguhUAj5fB65XA6apqHdbqPX67GI1Ot1LC0tsbtwUFh4p9NBtVrF6uoqWq0WizxZqpRqkEwmYbFY0Gg0sLGxwet9dG39fr/heMklW6/XMT8/bxAd+ky/30ehUOCUhUgkwmW88vk8dF2H0+nkScIg8Rtk0VosFnbbSp3H0RDhEoQ9oA5q1WoV9XqdC9eWSiX+XL1eh8/nw/T0NJLJJIDNors+nw+ZTIZDuGdmZpBMJhGJRDhPiiwbn883codcEqh+vw+bzQav1wubzYZut8suqUgkAk3brEBPAQJm1535XM3uxmq1ikgkYhAucqVVq1VcvXoVdrsdzWYTFosF6+vrWFlZgdVqRafTAQDMzs7y9bt48SIcDgdcLhdisZjhONT6iIuLi2g0GohGo7BarVhZWeHgBjoGl8uFYrGIlZUV5PN5tFotFrDZ2VneJgAUCgUsLy+jVCrhjW9848Cw/n6/j/X1daTTaeRyOczPz6NcLmNjY4MrXzgcDiSTSXg8Hk4P2O6+of24XK6BYfzCYES4BGEfoKiwcrkMXdcRi8Vw7do1HphqtRq7to4dO4ZIJAKHw4G///u/x/LyMkf8nTp1ClNTU/B6veh0OsjlcqjVahzUMch9pqKu6dAgbbfbEQqFYLPZUK/Xkc1mkcvluNKFw+FAIBDgWb86qBKUd0X7IKEpFAq45ZZbuNMyfafT6aBQKOCll16CzWZDuVyGw+HAlStXcO7cObhcLkPOUr/fR7FYxPPPP89J2wsLC1sCMOha/OhHP0KpVML09DRcLhfOnz+PjY0NFAoFtFotLmW1traGV155BWtra6jVajh+/Dj6/T5e+9rXGq5bOp3GuXPnsL6+jkcffZSDTdRr0ev1cOXKFW68effdd2N9fR1XrlxBvV5HuVyG0+nELbfcwhazev3U81BfV6M+RbhGQ4RLEPYADTy9Xo+rH/R6vS2uqGazCa/Xi0gkgmQyiWAwiEajAZ/PZ6hdODMzg1QqBafTiV6vx0JFa2Y7Ra6ZXYW6rsNqtXKLEwoCoDU31eIyWwjDLC41wblWq8Hr9XLVebJ2ut0uarUalpaWeB1ndnYWGxsbuHbtGrxeL/cIo21Xq1VcunQJp0+f5ooc6jFomsaJ3FeuXOGk55mZGaytrWF1dRXZbBaNRgNzc3OoVqucTnD16lVUKhVYrVbEYrEtwRuFQgErKytYXl4e6q7r9/vY2NjA0tISlpeXUa1Wkc/nsbKyglarxWkNlUqFrdedricdg8vlkijCMRDhEoQ9ologrVYLFouF1znU9SC3241oNIpYLMZBAz6fD0tLSxz5F4/HkUgkYLVaoes6PB4PAHCgxSiorjyKKqRw+E6nw0EhlIBMa1zbVc4wr3FRiD+tcal5WsCmG5QCT8hVWS6XkcvlkE6n4fV6EQ6HOYqO1riuXr2KfD5vCFJQw91pTWl1dRV+vx8+n4+tsHQ6jWw2i2KxiEwmg0qlgnw+j/X1daytraFYLMLn8/F6lLrdcrmM9fV1Fq5hEYt0/CsrKyiXyygUCshkMuj1elhfX4fD4UC5XN7yW5lFzHxdSbhkjWs0RLgEYY9QOPT6+jp6vR48Hg+vZZHl1Wq1EA6HEYvFEAqFAGxaUTMzM/inf/onribh8/kQCAS4irvb7YbFYkGtVuPBcLt1ExqM1ahCEplms8mVOChUHtgULpfLNZabis650WgYqmbQvinyr91uo1arYWVlhYMker0e1tbWkEwmDZYVRRVSFKAZcidmMhmO4HS73VheXkalUuFGjIVCAel0GsvLy8hkMmx50jFls9kta1hkna2vr28J3yfBLxQKKBaLHIG4urrKAR3k+qTXaZ87XUMAHIhCkZiAsfqIsBURLkHYB9rtNtbX1+F0OhEIBJBKpXDy5ElDgIFaVomsoLm5OTidTg7zJktIbT8ybmNBsohI7FqtFtbW1mCxWNilptYuVC0pdbBULZJh1SbU3Cn6DlXmSKfTiMfjXLsxl8vB5XJhZmYGP/nJTwxlm2hb2w32vV6PI/rm5+dRr9eRy+WwtLQEt9uNZDKJcDiM9fV1VKtVrKysoFqtIhQKsSuTjm1YovMw9yj9vv1+Hx6PB7FYjCMbQ6GQ4TyXlpa2CK/5+qnXnNzENBGQda6dkQRkQdgHOp0O8vk87HY7gsEgYrEY5ubm2LqiAVAdLO12O+f8kAXTbDYNbqZxKlqokIXR7XbR6XSQyWSQzWaRzWaRyWS2uP4G/VGPYZgFQHUA1c+RxZXNZhGNRuF2uzmQw+l0IpFIcKj+KOeh7qtUKqFarSKVSnGaQDqdhsvlQjQa5WANCsWv1+sIBoOYnp5GLBYzrP2NY9VQoIyu67w2l8lkOCQ/mUyyQKbTabRarS0BGYP2Rb+vx+NBp9MxfE8YjgiXIOwRXd8sobS8vAy32414PI75+Xnceeed3OeKBkp13cvlcuHEiRNwu93szlMTlYHxLC5z3T8qsttqtXDlyhVcu3aN/6hRgvRdtXbgMCFT/6a1ILViBEVH1mo1XLt2DdPT0wgEAsjlcuzaO3HiBIvqOGs63W4XuVwO+Xye+5RlMhlcvHgRXq8Xc3NzOH36NK+nLS4uolQqIR6P47bbbsPCwgIHjYyTaA1sBtcsLy9D13X4fD5MTU3hypUraDQaiMViuOWWW5BIJLjmJO1jmDWrbttqtSIQCHCjURGunRHhEoR9oN1uY2VlBU6nE7FYDDMzM7j99ts5Z4uEQl0LcrlcOH78OK8vkXBR0ATwqitpHMhVWK1W2YqjSLilpSVDki99ngRyO+EaZnGZK7VTDtny8jKmp6fh9/tRKBSwsbEBt9uNhYWFLZblKHS7XWSzWRQKBZw4cQK9Xg/ZbBaXLl2C1+vFzMwMbr31VhauK1euoFKpIBqN4tSpU5wvRq7CUSwuOq92u43V1VX0+334fD4kk0l2CUYiERw/fhzJZBJOpxOLi4uo1+uGazJsXxTVGQgEuKP0uNbgzYiscQnCHiE3X6lU4uTTQCCARCLB4exqfpW6luN2u+F0OjmKsFqtotFosCVCrjjVUhs2oKkJwr1ej4MVKKiBSi6VSqUtVThof9utZ9ExbDeg0v5pvcbn88Fut3NtRJfLhUgkwiKuHgNFN1IJJDP9fh+tVgudTgfBYJDFvtvtwu12IxQKIRKJwG63c4UNYLMfWjQaRSAQQL/fR6fT2WLV2Gw2TnwedH7klqSyTh6Ph8PefT4fwuEwfD4frFYrCoUCJ1fTcdPvQGuXKrRNmmQIOyPCJQh7hAZqquxOkYEAODHXZrNxNXUq+ErNIqmZIABks1mUSiV+r9lsotPpwO12j2x59Xo9dDoddDodFhrqzExrRC6XyxD+Tp9Xc8RUtyMdJ4mKak2oleHJ4iKBpnJQlBRMYfA+nw9ut5uTdClYJR6Pw+fz8bGpx6D2u/L5fHyelEBNUZsWi4WDUjweD8LhMCKRCJeQUkWF8Hg8CIVCiEajhlw19ZrWajU4nU44HA7YbDY0Gg3YbDauPUn5bKVSicWRAjuoCG8gENjioqVzJ+ESV+HOiHAJwh6gtZ5ut4t6vc4dhcnFR4OU1+vlUGlayygWi7h8+TK8Xi/i8TivRc3MzOD48eNot9vIZDIoFouIxWIsgjtB0YmUH0Qh9TQg2mw2RKNRtgZpPaxUKqHVarFlRcdOVgqJCgkAbYusKqLf78PhcGB6ehrtdhvtdhsWiwXz8/OIRCLweDw4deoU5ufnEY1GAWxaHaFQCK973eswNzeHYDBouMYkXKFQCLFYjCMm3W43br31ViQSCfj9flgsFj6HUCiE22+/nS28Qc0n6d/JZBKnTp2Cz+fbEm1Jx+dwODg/r1Qqwev1IhQKIRwOs6tVzWejfVWrVe5kfdtttw2NZmw2myJcIyLCJQh7RHXNqWtSajNEu92OSqXCIkfdg2ldzO/3w2azYWNjA7lcDo1Gg2vr1Wo1BAKBge6zQdDMXdM07vekWhAUfk1WHg2apVKJawqq50EV0L1er6G0Ew3WagIyAK6PGIlE0Ov1uOJHJBJhqySVSiEajXKeG23n2LFjiEajnHitike324XX64Xf70er1eJixtPT01xOiwSKtjc7O8vllMxdiwlN0xAMBjE1NWWYeJg/Q33S6PdzOp3wer08AVDXCVVo7bJQKAxsgEmQyItw7YwIlyDsERIuqpqhDvwkGG63mwcwss4KhQKuXbvG4dVWqxXpdBobGxuo1WpoNBooFouoVqsIBoNs1WyXU0WuKcojcjqdhqoMtFYVCARYuIDNIsD5fB61Wo0tRRJKWq8i4QFeXROjclKqqJJwxeNxTqy2WCxsFTmdTszOziIej7NlpWmbtQVPnTqFRCLB+1HddhTRFwqFOJyecuFCoRDcbjefP1mC8/Pz7J4z9wZTCYfDmJ2dZauNUF2gVBqLKo9QRX2/388W6iB3LhUGzmaz20ZRmoVLgjOGI8IlCHuABKHT6aBer/OATwMdDZher5erM7TbbVQqFayvr+Ps2bOIxWJsFf34xz/GyZMneS0qm80in88jHo+P7CokS41chWQR0bqLzWZDLBYz1FMsl8tIp9MolUpsdajCValUkEqlWGhIJO12u6HSB/Cqq3B2dhbtdpsFnVyFbrcbt912G+bn5zlB22KxIBwO495770UkEuE1QjUYpNfrIRwOsyu03W7D4/Fw9KbP52Nrk9yhd955JyKRCBqNBq9tmXPjNE1DKpWC1+tFq9UaanGRq5CsU6o9uZOrkFqmLC0tsbtykChRP65h7wuvIsIlCLtEtXBU15U5sAAAiwdVbVcL3ZKA0AI+JevSYEvRdqNWDycLifbr9/u5Hxc1o6ToPYIi9qi6BbnBgE1XVz6fZ7FQz91qtW5xFQKbFedDoRBbXJqmIRAIwOVysajQ/+laUXCGx+MZWjeRLByyTOx2u+E7tD+Hw8FrUE6nk4NihkHnQMdK0L/pPL1eL1fsJ2tWPYdBNJtNFq/tjqHb7W77vvAqIlyCsEdarRa7rmjQB4z5O+pgXCqV2KLq9/vw+/0cwm21WtFsNpHP5znKz+l0wm63DwwaUFFzjigXzOPxIBgMclABDdDm+oLk7szn8xyA4fV6DRGTamFes3CprkKyxILBINcVBMBuQgqcUNfM6Dt+v99wbOYACa/XC6/Xy3UhqWWLmlJAOWput9sQOKJaO+Zr6HQ6YbPZhlbet1qtfE2sVitHZpIQD/ot6PdotVq8xmV2FaoTEfoNhJ0R4RKEXaCuKRWLReTzeQ5AoDJP6vpIOBzG1NQU6vU6FhcXsbq6inK5jKmpKczMzMDlcnH1h36/j/Pnz8PtdsPj8WBhYWGsY6PCtr1eD/F4HMeOHcOxY8fQbrfRbDYNIkjHR/taXl5GKpXidTfKlcrn88hms1wUlwZ4u92OcDhssJCo43IsFsPa2hq76LxeL7tEaY1NFU81tH6Yqy4YDKLZbOLatWsAXrXsyBqlQrWBQIADPOh1NWLPvA9yHw7aL1mDMzMziMViuHz5MndAVnuYDaoqD7xawHd5ednQrHPQZ8VFOBr7Xjnjt3/7t7dk299+++38frPZxGOPPcbhuI8++ijW19f3+zAE4YZBoc40INOAqRIIBFjU1tbWOAAjGo0iHo8jmUxywAIALC8vI5/Pw+VyIZVKARheFZ7eo/fJVdjv9xEKhZBKpZBKpTA9PY14PM7ioFpwTqcTTqcTmUyGK0tQ2xCbzcZWorndiM1m44hIwmazweVyIRgMsusOAFsn1PaF8qFUhkXm0Xa9Xi88Hg+7LMkSUgM4KHJPTSZWS1MNq2AxqLwWfdbhcHAwic1mY4uLrDnat5rIrZaLohYr21lUUi1jdK5Lyac777wTa2tr/Oc73/kOv/exj30MX//61/GVr3wFzz33HFZXV/Gud73rehyGIFw31IGpXq+zJUKuLBVKvA2FQvD7/chkMtwOg5JXo9EoEokENzmkQAmHw8G5TqOiFmulqhGxWIwHXrUKBx0fhc0Xi0UWPToXco1ROSrV2qRIO1WArFYrnE4nt+qgwVp1q1H/L7PFNUy0aF8UbKIGmtD6HQlHr9fj9Tz19zIXLzZv3xy0oUKJxlQdo16vD3S3qvcFQcJF4fDC3rkurkKbzcazRJVSqYT//b//N770pS/hX/yLfwEA+PznP4877rgD3/3ud/HTP/3T1+NwBOG6UiqVUCgU2FVIEXGEpmkIhUJIJpPIZDJ4+eWXUSqV4Pf72RIKBoNot9uYmZnBxsYGLl++DIfDgampKczNzY10HOoaF1lN0WgUCwsLmJ+fh67ruHr1Krf4UF2ZJAhra2uYnZ3lZF9aR8rn8xwuD7waDk8DuhroQflOdrudq9MD4C7LFouF89LMLkuzqKqorkJy+1HVDCruS3lyJGhq8jIFdAwTre1cd+QqpMohpVIJHo8HTqfTYG0NcgNSWkM6nR7qTjTvT9ie62JxXbhwAdPT0zhx4gTe+973sj/6+eefR6fTwYMPPsifvf322zE/P48zZ85cj0MRhOuG6goiFxrVHgSMwRkul4vLElEVBo/Hw0Ln9/tZ3EgIXC6XoQzRqMfUarW4WCvV0fP7/YaySCRedJwOhwNOp5MrUqiBEGS5tNvtHQvjUp4YHX+tVkO73YbD4TCsrZG1Msw1Z4ZEjYIoyuUy7HY7X2+K7qQGmRQ0oh5XpVKBzWbjz5uto+1cdeSSpAAO6mitugpbrRYajcaW7VAwDAmpsHf23eK677778IUvfAG33XYb1tbW8KlPfQr/7J/9M7z44otIp9O8mKqSTCaRTqeHbrPVahkqZlO7b0E4DKhhzObCsQS5rqiOoc1mQzAY5Kg3sj6ohBCthwUCgS2ux51QXYVUONZut3PtwHg8bhAuAJy3pZ6Hw+FAr9fjbQCD19moeK4aCUiuOxISEgv1eoyL6kasVqssXLSvXq/Hgk3CSfT7ff6O+Vx2EhPVLUppDxTtqbpISTjN2yOrMBaLbWtRqsckbM++C9cjjzzC/77nnntw3333YWFhAX/xF39hyNQfh6eeegqf+tSn9usQBWFfoXWV7dpRkIiEQiEcO3aMGxCqa06apiEWi2FhYQH1eh0nTpzgCvOq9TYMSoZWK2eQJUXWBdUJpOg+Qg1moPUup9OJXq/HhWrVxFwSi06ng3K5zG5A1RXX7/dRKBS4XJT5eoxrfdDnqb2J1+vlKvHA5gQ3n8/ztVbLRvV6Pc5RCwQCfK3GFdBarcbltMzBGbQOqAqsrusIhUKYmZnBXXfdteW6C7vjuofDh0Ih3Hrrrbh48SL+5b/8l2i32ygWiwara319feCaGPHkk0/iiSee4P+Xy+WR/f57QTLYrz9H4RrT2sow4SK3lMfjQTQaxcmTJ9Hr9TjknGbtmrZZ7LXZbHLlCepnNeo1orBvaulBSbIkXH6/H6dPn2ZLhfarutDIKnE6nRxdSNGIas8uCngoFotcEYO2B2z+ttlslkVaDerYDbSO1el0sL6+jmAwaEg9aDab3N1ZTaAGNsVuY2ODS0YBMFiXKsOOUdM0bjujaZqhij1VH8nn81usqlAohOPHj3NAynZ5eGJxjcZ1byRZrVZx6dIlTE1N4d5774XdbsczzzzD7587dw7Xrl3D/fffP3QbTqcTgUDA8GdSkZvz6KGGQQNbK4/TZxwOB3fPnZ6e5jJFqostGAwimUxifn4e09PTCIfDAwvODoLEpNPpcLi43W5nS0jXdbhcLszOzg60uMh6oJJPZHm5XC5OklbXxUi4aF2MzlOF3lNzquhvc/LtTs8F7bvX66FYLHLumxqUUi6Xh7oKS6US99IatMY1ClT8mFIIVFdhs9lErVYzTF5oshCPx3H8+PGB/biE8dl3i+tXf/VX8Y53vAMLCwtYXV3FJz/5SVitVrznPe9BMBjEBz7wATzxxBO8KP2Rj3wE999/v0QU3sRMutWlWlzbuZ5IuGZmZjjYgMLN6fzJAvP7/QiHwzvO0gl1IKa1FuDVBon0HnUgdjgchmOlNS2yviiYAgA3aaSgDdof8GqfqkGh5rquc2+xQblt40KCQMJFlTGIdruNUqnEwkVFdwGwZUjrjLQmNsp9p66FqcLldrtZuGg9r1KpGH5PCpChivjm3mXD9iVsz74L1/LyMt7znvcgl8shHo/jzW9+M7773e9yYuV/+2//DRaLBY8++iharRYeeugh/NEf/dF+H8a+sR+DqtyMRo7S9aABq16vw+l0GtY3zPcNWT/UfmPQ58j95Pf74fV6tx3kzNBnms0m6vU6lyRSZ/kulwvJZHLLzF8tEEv/pkHZ4/EgmUyi0WgY3IHktisWixzybraiarUauxt3snJordAcJq9CVl6lUmFxIqgYMLkKSdQoWKVSqXBSdaVSQb1e56hJErJhXYqJSqXC64dUwor2Ua/X0Ww2uehwr9fjmpNUrd7slhxm9Qrbs+/C9Wd/9mfbvu9yufDZz34Wn/3sZ/d719eFUSOPdtoGZezvJpqKvjvJVomKuh60m+txWKDBmxbs1XWj7b4zqEKD+f3tBu+dtk8V0ikx2JyzNays0iBBIeslEonwNmk7wKbF1Wg0BobJkyVqtVq3iPUg+v0+Go0Gu/KGWSSqa1b9jBoko4oaBav0+32uGZnP59FsNgFsWrmU59XtdrcVj1arxZ2laVtk5dI1oBY0nU6Hk83VII5h15omNiTwR+V5vx5IrcId2A/hogeYHsZxByR6WEdt3X7YMQv5JD6gqvVQq9VQr9e5ggOw9b5RXUfq+Q77v3qPjDOIkXBR23qza2pYhXm16oPZKqJ6hO1229A0kayUQZXX6fv0O5Nw0XuDngESLrJcB93vgyw29drScahRkpRfRQEydrsduVwOrVaLBYWiMdvtNlfrN58LsClctH6oFk6mMHkAXMOQAkLW19c58IWEfNA5UCSnsDMiXANQbyoKmd3LDIiy+Yc9jKN8fxIH92GQS4bWeSb13GggKhaLKJVKCAQCe55cmIVrVLeqGqbearVQq9UQDAZ5wCShBTDwmtNkQv0bAOcrTU9Po9vtGqKByVVYLpe5BJP52aHeXGqDRrOo0zmTq48SfYddH9ruoNepAjx1lQbAa0+6rvM1WV5eRqvVYqHodrscXBGPx4dOWKvVKofDU9dlXdcNa1/T09PweDxotVpYXFxEOp2Gx+MxrIWq14n24Xa7DVa7WF3DEeHaBhqY9nrzDHrQxuGoRSKq9eQmFbon+v0+r5eYB2dzdCH9jqNYU+YaeKOucVksFnaNzczMbLG4VAFTUQVrkMUVjUZ5rQp4NYF4UHCGeZtUAoqs62Hn0+/3OWF52PNiFlY6Lzpm1VVILkeqJNLv93kNcW1tDe12G4lEgvdNltl2NJtNtrio2r2u62xxaZqGaDQKh8OBTqfD9VqpJYpahNeMWFyjI8K1DeQ+MBfT3M126ME214nbCbX1+VGh1+vx9aBItkmdWVJwRqvVQiKR2DeLazfXgwbvTqeDTqczsG3IKN+nf9PfNpsNgUCAoxLVbdGgPcgCIhGhqiHb3cNqaH2r1TK0SVGFTi3aaz4n9b6y2+0sAtTOBQBX+KCCt/Q6iT0JnHlyQbTbbQ5EoXGBLC56TqlkF0UyFgoFDihR25qYr7u6xiVsz00nXINmvYPodrtotVpYX19HJBLZ0s5bfXDNmNc1ut0u6vU6isUiZmZm2M++3TGS6ySbzQJ4NZF0kqHzoi6/lEhujnybJHRd51k4VcGg1/cyAI37XbLqm80mBxD4fD5O2jUHaAw6D3Vb6v8pPJ/+DbwaeWixWLjhpbodEiEq0uv1etniGmRNdTodVKtVrKysIBQKcbi5KqCaphna26sh+xQgQdGG9AfYzL2qVqvQdZ2Fa319HeFwmL9bKBSwvr6OYrHIwjkoMZkmKfR/sgAzmQxarRZcLhempqY4dL5SqXCIfqVSGXjd6R5yOp2GEP5RoW2ME4E66RydafyIqLOy7Wi1WigWi3jllVeQzWZ5BjnKH/ODSfklZ8+eRS6X41neKMdw/vx5rK2toVQqAZjcUHLzrLVQKODs2bNcEmhSUQcN6ku13cCx06AyyNoaxVIi66FYLPL9RZUvKOJtL/eOGglJ26HIPXPeErD5G2ezWQ5vHxbaT59vNpvI5XK4ePEirly5glwuZ3iO6HPVahWFQgHAq2tCwKuBHVTWiZKuAbAlBWy6YKnTMxU37vf7WFpawuLiIi5fvoxyuWxIqFatPZqk6LqOYrGIer2OdruNq1evotfrIRAIIJlMotvtGgJNAGwJYCF6vR4XACbhGnV5QK2IT2PPpI4R43DTCdcgH7n5fQBsFSwvL6NcLvMNMuiPKliDFl9p9nn16lWUy2X2kW93DOTyWVpaQi6X43YS231vUqCZ8dWrV1GtVgfmAE0KasSouo4DDO75ZGYUIRv1OOg+a7fbHPZOLrhBLqpRITE1W23buba63S5KpRKvce3kLaA8q9XVVaTTaa6AARjvC+p9RgEY5A4kl2WxWITdbjdE7pH3hI6bCu56vV643W62mNLpNNLp9Lbh/Z1OhycBFKhB0YNU15GStVutlmEis93zXq/XDUng46CORUdtPXwYN5Vw0cx0J4tL0zSuqfjKK6/w7M9sVdG/aR2q0+kMnPV0Oh2DxaUuAG93kzWbTVy8eBFra2soFos7fn5S6HQ6yOfzOHv2LPL5/ERbXMCrkadqJ+C9iMQor6nQfU2WrFqSaJDFNe6xUdCHGlwBgAMgyOJSAzva7TZyudy2FpcauUcW16VLl3DlyhX2cqjnCGxaXPQseDyeLRYXdY1WLS416MJqtaLf77PFRVU0lpeXsbi4iMXFRVQqlYElrMilR5X3y+UyGo0GTzA7nY7B4qrX6wZhN1tc6uvlclksrjG4qYQLMAZKDEK92fP5PF588UWsr6/z9+gGUf9tFrNBrsJ8Po8f/vCH7AtX92Xev+o+OXv2LK5du2YQrkm9MdVrm8vl8MILLxgSQSfx3Eg0NE3jcPhxXIWD/r+bNQ4K587n8zxIU3V3CigY59qaj0EVLnpf7TFlFiNaH6a2KCRcg8LwgU1LippsXrhwARsbGwYhJMrlMnK5HIBN4aKEaEoE39jYGOgqpKLDFosFvV4P2WyWO1L3+31cuXIF58+fx/nz51EqlQxeAFVgms0mX8t8Po9qtYpWq4VLly7xOufU1BTa7TZqtZqh6/Mwq7fX66FUKhkiIUeFPEjqWDRpz9BuuKmCM2gmSDOmYTcIJVUWi0VUKhVefCYXhTmsWL1RyEVDbgxqKUF+bLVkzHbQjZ/L5dBut49My2/VDVoul1Eul0de8zss0D1A9xP9nxb0b/TiON1fagNJClwg4Ro32lG1htS/1dcpcm/QhIN6Y9Hn1NJMg64PHWetVkOhUOBgCvPxtFotNJtNrsahugpJvNUwdYKqkXQ6Hc7LVDsYk7iVSiXU6/Whk1sKe7dYLKjVaqhWq7DZbKjVatx3i6rS67rOjSeplNSwNAAqC6VGU46Cmn9GJaPM645HkZtOuFqtFqrVKmKx2BbxMq9vZbNZtNttvvFyuZzhwTULl/qQU95GLBZjoaPBmpIht4OiEGmh+CjdhGq0ZaVS4QnBpEEDDg1yOwVmXE8oGICqk1PkaqfTQaPRMFgge0V1FbpcLkNUIUHXhgRmJ0uCnjFyq5MAm6HqFmSd0NoZWR2tVguBQMDQuJIEweFwcKQjWYskXCQAdA3NwRm0LVrTstlsHCjS6/XQ6XS4uzRVUKHJTLfbNTTjJGgMoYnyKGuBZijZPJ/Pc9pBv9/nqiFHlZtKuCiaKJvNYn5+fmhIfLVaRTqdxqVLl7CwsMCLrWfPnjU8pMMW4bvdLiKRCBKJBCKRCJxOJ8LhME6cOIFiscgulGFuIU3TsLKygitXrkDTNpsLUujuJEPn6vF4EIvFcPz4cWQyGWQymW2vx2GDBtRer4f19XW2uvYaubeX41EjCn0+H6anp+FwOHgC5HA4dpUEP8wzoWkavF4vAoGAwbWmWmb9fp8/EwwGBz4v9G+Xy4VgMIiZmRnOu6KgF/Wa0jn4/f4t3aEtFgvsdjtmZ2cRCoXYsgqFQtwiZmVlBX6/H8ePHzeszyWTSUSjUba6KIBKPU5N07hKiN/vR6FQwOXLl+FyuRAOh3Hs2DGcOHECwOY97vP54HK5UK1W4fF4MD09bWhhQ+dDQTXUpmWc36VQKCCdTmNxcRG9Xg/BYBDz8/OG85+EZ2pcbirhomiiQqHA7o1BET+0yEs5XJTxvrS0xA+UOhMz3xjNZhPJZJJnkVSpOplMolaroVgscs0yFdXlUigUsLGxAQBc0do8+5sk1AVn9XqQu3ASH7B+v28Y5PZSHWWvdDodjiikgdRqtbL7cJQ1LvP7232eXIAul4stFFWYSLhcLpchbH0Y5OIKh8MsDoPc43RMbrcbXq/XUGlC0zZLiNEzS8dDLWJ8Ph8HWqVSKTidThYuCtRwOp1D+4tRbla32+XPZTIZ9rAkEgkkk0lomsbn7XA4OO8sHA4blhlUi4siEOl8tnvO1WOi5YSlpSW0Wi1EIhH4fD5DhOGkPVejcNMJF/nQVTffoIVlsoze+MY3wuPxoNfrYWlpiW/cXq/HCZhqjyKKNqIutrTeQDd2rVbjhETz/glN05DP57GxscHRamrfoUmH6sglEgkWLnO1gsMM/Wa0qE6W1kFGdFFbDVW47HY7rxvtJFxmd/d26Pqr1TA8Hg9HD5q/T5M2CqIY5qEANqP93G43otEo8vk8W3Dm41OFy+12DxSuaDTK7jpd3+yHFQ6H4fV6kc1mYbFYMDU1xTlWur5Zw5CEiyIF1e3S381mk7tXUxuTbrcLr9fLwgWAXYMOh8NQhd7cz4wsdUp6HrXkE92DqnDVajU0Gg2kUqltS0sdBW4q4SJX4erqqsFqAow/cLVaxfr6Oi5fvoxHH30UoVAInU4HZ8+eRaVS4fBim81m6FtEOSLZbJZ9zr1ejweSEydO4Lvf/S67CoctoGqahtXVVXYVRqNRRCIRfm9SZ1Ak7NTC/sSJE/i7v/s7dhUOK6x62KB7pd/vY2Njgy0u82B7I6HJVqPR4Hp8drudJ0o04G7HuFGHHo8Hfr8fly9fHugq7PV68Pl87CoctA3aJ7kKp6encfbsWfZsDDpGTdtM9g4GgwbXGlmBMzMzBldZOBw2uAp1Xceb3vQmrp7f7XbZre/xeFAoFAb+lhaLha3BZDKJQqGASqWCarWK1772tTh+/Di7Cr1eL/x+P1wuF3sUpqamDJMzGi8odN7pdPL5bPeMq79TsVjE0tISXnzxRRSLRSwsLCCZTHJ0oQjXhEOBGTRL2o5SqcSJsZFIxJArQouhlMBILgF6SFSrjnJRaDYZi8U4+bZWqxnK1ag3Ki1Ql8tlw0zwqEAL9rFYDM1mk6+HGgl22MWZfmtq6UGD5kGhaZu5VuT+ogmC3W4fuXXMuNfc5XLB7/dzZXTVXWixWODxeLjT+bAanWoABT0j5oac6ueofFQ8Hmfhof25XC5EIhGEw2EOTlA9HnS/UdFg9ZgCgQCvmQ2rnt/pdNBut7keIa1f6brO+w0EAnycFFFIx085ZOq5UY1Ecj+qwRmjiBeJE6XvhEKhLetzR5EjL1yqKFAOBjWCGwa5+qxWKwKBAIeo0sNAUYf0f7UtOVlyzWYT1WqVF5jtdjtCoRAPdtVqdeAslG5Cak6YTCbhdrsntpbfIGiADQaDHFFVq9UMLTMmBXLB0W98UIJrtVoNYef0fwqR3qkx5W4seYrMozJI5IWg/ZNl7ff7B/agMh8/BTHR2pB5zQwAr4+qAkefcbvdiMfj8Pv9hrByOs5YLIZiscgBI2oUqMfjgdfr5R5bZuEk4aJzpAkliU4oFDJMMKmWI4XOk3VFXRFofVutXL+beqTk8bFYLGg0GhOXWrJbjrxwAa/OSqjFuLkMi/mBXVtbQ6vVQiqVQigU4lns6dOn+YZbX19n18axY8f4uyRqur5ZVJOEjHzvVqsV9XodKysrSCaTXJZHPVYqQNvr9XD77bdzsVTg8FsiO6HOnGOxGOfDLC8vY2pqige4STpPCrRRi+zeSMi6SaVSvBZKM3wKsR7UNl6FLDba3k4iB2wGPUSjUTQaDZTLZcPkw+VyYWZmhgMhdsJqtcLr9SKZTGJqasrg6lP3GQ6HMTc3t2XCaLFYEIvFcM8993DPLfV43W43Tp06BYfDgVgshmAwaHimQqEQwuEwIpEI4vH4FgHpdrvs8vN6vZifn8f09DSi0SgqlQpmZ2c5IISe936/j3K5zNGL+Xwe0WjUIFzlchmZTIa3O4qrUB27QqEQkskkZmZmcOnSJQ4UOercFMIFgG+idru94+xzfX0dnU4H8XgcoVCIZ3+nTp3iYIJerwe/349UKoXbbruNH7BOp4Nr164hk8mwxUU3cjgchs1mQ7PZRDqd5jUHc25YtVrlFgi33HLL2Nn0k4DNZuMoq0ajgY2NjQONytsrFERjDne+EdAgHovFDMJFAkQWx3aTH/rsdtGy6v6AzXWcUCiEZrOJer2ORqOBcDjM7rlkMgm73Y54PL7jOZA7PZFIIJFIbGnKSfsMBAKYmppiF716/JFIBLfffjuCwSBbXKoL9fjx4+zeVHunAeA1s1AoxCks6n6p7xg99zRhJeGamppiIaXnnSavJFxUeV7NMatUKoYSVqOGw9N+/H4/YrEYUqkUC6L6Wx9VjrxwqbN3tdfOoKAMolgsotvt8uIvWUXT09NYXFzk8Hiv14toNIq5uTkWKEpEpJqENBhbrVZ+WDqdDgqFwtCBmkx+XdcxMzNzpJIJyWVitVrZCmi1WlsiPScFOlYaeA9iwNA0jV2var8sGrTJ6trp2FTX3HaipQZU+Hw+zruiGonApkUdDofhcDgGusTNkMAGg0EEAoEtg68qltFolCP21Pd9Ph9mZ2cNa1/0nsPh4MGdohzNbkav1wufzwe/37/FNU+V4em8o9EoUqkU/H4/arUai505cpKCLiwWC6+bq889tT4BYCgaPCoejwfBYJAjFikU/6iMF8M48sJFUJh6p9PZsRzKxsYGWq0WZmdnefbW6/Vw6tQpXL58GVeuXDFYXLfeeisLV7vdRjKZxOLiIlfHphkYuT9GsbhodkcW11GbQZHFZbFYUK/XuR4kMFnCRagTkxsNDbyxWGxLWDqJFwVpDMPsHtwudF1dF1ItLirxpFpc9LmdoICdeDyOZDLJbldz7qJqTQ2yuO644w7DujQdr9PpxLFjxxAKhbi/2DCLKxqNbhEQKtlGE1ayuOjczRYXWa+0Fm6xWNjiUqM7K5UK8vk8B9Oo12oni7ff7yMQCCAejxssLrUu5FEVsJtCuGiNq1arod/vs+vPPEBSyZlsNguXy4VUKsV1v/r9PhKJBEKhEDe4c7vdCIVCSKVSHH7a6XQwPT3N+S20b5rpBYNBblo3KLdGDbF2Op2Ix+Nj1y+bBOh6+P1+zpk7qMoTu4WiTOn3JVfhQQwYFIBhTphV/73dMVF0G1nEFEGnrn2Zt+twODgQolwuI5/P82eoazK1GDEfjwpdPzV9hAJdzOudFLihbpe2TZaWal2qFiAJkmqVmANr+v0+fD7fltQMKtdGQRSRSASRSASdTofLTKnXif5Na+r9fn9gB+RSqYRCocCh+BRZO+r9Q9U2qKo8ifJRFSziyAuXehNQNOGgCD26aclNR/57Mr/JlWG1WvmBocVvKqdDDyDdhOYHngY3p9PJ62TmXLJ+v8+N8ia5M/BO0DUNBAKo1+sDr8dhRw2HV9MiDgJVmMZNJgbAhVotFgsnxLbb7aGTJvIiUHh6s9lEqVQyiIHNZhurfiMFQAxy59O/6fVBOX/Uz2rYb0DvA1uvDU1C6vX6ljVwShKmiS+tmdG50XM/aDJM40q32+WJgHpeZKmqojrO5I2KINAEmVzW21nMR4Gj5X8yYb4BqDK0uXI0/bvX66FarXLBy2g0argB1NBT9TUy0Sn0OB6P86zNPBBTeDDVdzMfY6/XQzqdhqZpI8+cBt3odCNfTwtm2LZH3aeu6xzBpVagOOwPGw0uVAGd1uyoPf1BYf69R70HaAAtFouwWCxsfalrtIMgF2QkEkG9Xkc2m+X3xnFV0fG1Wi2k02kubK2+Z97uIOi97c53UMSkpm3236NycKog0TGQxUU5e+S1GWSRqtu1Wq1slbVaLR4vaLuUwDw7O8sFf0e5VubrS5Y/TSQGXaNJ8mbsxJEWLjOUPOhyuQb+iJT4W61WYbfbMTU1Zbg5ydqiG4/+T24aEq7Z2VkuuUPQg0Ah9rSORnleRLfbxdWrVzkKkb5HA7r5uEe90Q8r09PTCAQCyGQyhhYhhx0azGgCQi6ag4gqpOPZ6c9236WwbBIuasEzTEDIyqC1rHK5jJWVFcNnxhFxXd9sLXL16tVtq3wME4qdREQ9pkFh9p1Oh13W6uSUvCDUGJPyuNR8Laq8P0hkqexWs9nk3FB1japYLKJQKODUqVO8ZjfOM0tjAlUaoXxRs8V12MeBcTnSrkLzgE/JgGb3B32GQl6bzaYhXBswhheb813Um9FisSAajQ70kwPgYp/UZdUsSL1ejwcQNS/EPJNWB8Zhg/31vlkH7Ve95qNAEWLq9TjskPun3+8bIlUPOohm2LXb6bcgi6tUKrGrkAZa88RK/Q4NzIFAgAtTj7rPQdtqtVo8odvtuez0/jDRo/xLsjrNrsJOp8NCTmKlbm/QPU8WF7kJaeJMQSe07k7BHaMkH5stLoImfcO6TR81biqLi1yFw9a4SLhoBq3OXIBX/cmqq1AVL3IXUnXqQT2QQqEQPB4PRzjSwKDOwqgQqM/nG9vnTahujsOKrutcnoeEa9hAeRgh4aK6kwe5xqUyaKKz0+ep0ovNZuP+UpSSMew7tNZEkYXFYnFocMgoUGfs7br4juIq3A71+VX3QX3LKpXKlnU5ClZRLVDVsh70m6vCrnZPV9fuaLyp1+tcrX7c55XGByrzRKWzxFU4oZgfXGq7rWnalhYL9FlyF1DkYSAQMMxczA+GKlyqO5FCa/1+/5ZBJBKJwO/3c3UMtUQLWYW5XA52u33b/Jdh61qDKpSrLQ622944a1PmtY/dFPXUNI0reVcqFZTLZbRaLd7HYUSdWfd6PRSLRRYuc5miSYJ+TypG63a7d0wIp4F5enoaALh6hnkyNur+6fnbTT6Tekw7vT/oM9VqlS0i8xpXo9FAqVRCPp9ntyH9G9hqxdE+qDQUlXKiyazFYkG/3+ciBQA4X3McaP9UH5XKZqn5ZObfcNgYMWkcWeEiyK1DM0jKLB90A1NUk91u5wjCcVAHbrfbjUAgsOXGocRJaolRr9cN3yeB3a5SNH1OPUfg1S6y5lkvfX67gWgUcVP3r26Ltr/dTFlFPR8Kb+52u1zdnLY5ynHs9s9+QC42GqgPekBQJ1fmPzt9hxKYT548idnZWcRiMYMQD/oesDlox+NxngxS8v64x63eP6FQyHDvX89JAP1eZG1SWTb12MiFSInC9Xod165dG9oqho6XQvApQTiRSLAoUn+/VqvFtRNHHW/UiRM9e5TorC6D0HigQtd51Gf1sHKkhUuNMiK/PUUEqZ8hKOTVbrdvSdgcZnqrLjn1D5WWMQ9m1EKcEhppoFYFptvtcmvyQec0yOKhY1BvVrOg7ZdwDWoJQ26+Ude36DMUVkwlucjiou0OYy+itR/iRb+DWbgOA7sZ7Ckna2FhAVNTU4hGowOLzQ4iEomwtUAuX2A8q1m9P4PB4EhJy3tFfXbJYqEGnPT7apqGer3ORQGATQtsZWXFsCY76BpRoEQgEODEZlW41tbWuLcXVejZ7rczP3P0vNFk15zLN2icUMcCEa5DhtniIPO+0WhwqSH1BjG7Cl0uF1wu18AZ0DDRUN/XdZ1L06iDpKZtJkm6XC50u11ks1mexQGvtvFuNptwuVzcIoG+q+5jUOg4iZ56fOpsdliklmqRjSpc6rbIoh13FqdpGnexpetBrpOdRIv+vtGiZb5vaN2DrvNBDQbbWVujWF0ejwfxeBz33HMPTp06xa6r7UK9gc01lZmZGfj9fgDg3mrjHjutAWmahng8jmAwOPIkaNixjfoZXddRKBQ4ktgccFEsFpHJZLgsWblcxssvv8wBEebt0h+Hw8GTgEQigbm5OZ4Q93o9XLhwAc1mE36/H5FIZOx+dPSsU6g+jVtqJChNIlQrjarcT7JwHemoQtXioMVXi8XCi5fmm7fdbqNUKrGfeKeFdrPFpUJVNcxi4HK54HQ60ev1UCgUeBZHAkvJn1QHTj0Pdb/DXIXUNsH8OkVDDYPeH1W41CAKXde5lNYoM3QV1VWYz+fZdbrTcexVgHY7KBI0I6dCysDW9YQbzW7Ph1yFZDnRWgkJl7reo35H1zdD4pPJJLxeLzKZDPL5/BaLa5R1J3ViRevA9N6NcBWWy2XYbDYuDEzvaZrGVUHK5TKATbfipUuXDK5C9Tjpb4fDgXg8zhXnp6amuEJHr9fD4uIi2u02N9qk/W7n3VFFtdvtcvGDWq3Gvxkdt3mCCcDgJhThOmSoYkJC0Gg00G63+YfsdrtbahZSlA+5Cgc9sDvtl7ZHa1xmHA4Hz6zq9brBNabOhhwOx5YgEuBVEW42m7wORw9Cq9Viy0cNu6X6aJqmcd6ZKmJUAcLpdLK7YRiUTEnVIqjiAAWZ6Lq+Y1089Zqr+6PW84cZ+o3J8qaW64MiSCcJqiBP67IUpDBKXpTf7+fAg2q1OrbFBbz6zNrtdu4cfL1Rn9dOp2Oooq+6CqmHH5V7AmCo9DIMCtSiP2pZqF6vh1wuBwDckHaUiFSaNJJnhopUNxoNHrdoYkvjBE3CdV3nHDkaB3aq23pYOZLCBbw6wFerVZRKJWQyGTSbTW5aSB2M1UG63W6jWCzC7XbzjWS+MYfdqObXvV4vIpHIFguEkpStVis34CM6nQ4X16UoJDMUlpvL5dDv93mGXKvVUKlU2NedSCRgt9vZBUeugU6nA5/PZ+jZ0+l0sL6+jkAgwCWpBp1bv99HqVTi4sFutxuNRoOrLthsNs5fGyS6gyDhslqtaDQaA1umH0Z6vR7n5vh8vl0ljx4WyK3l8/kQCATYBaY2xjQPburA7vf7EQ6Hue4kVb4Yd0CknLCdhGtUS24cKP2Enlnz70hNZSm4yuytUVHdqD6fD9FolDtBU14XpQ/4fD7E4/GRoijp+aWqJrlcDq1Wi4sm+Hw+vm7tdhvlcplbptC6fjab5URlAFtqPk4KR0646GGiCMHLly8jl8txZYb19XVcunQJDocDMzMzHPJOs+dcLsedUMfB/IDTDWmunUbmPomS6l6qVqtcNsfj8QxsCFepVLC6uooLFy5genoasVgMXq8XV65cMUTl3X333fB6vahWq7h8+TLX1KOq1jSzpACDF154AcePH0cqleL1BfV8yJV6+fJlbGxsQNc3ewGtra2hXC6jUqmg0+lwc0h1lme+RrQ9+psiPcdlr66+vUB17TTt1SaENADs1Q15EFDNTZqJm+sFDkJ1F05PT6NUKhkqoI+zZkNrzydOnBjZ+tgravBWPB7n2pnmcw6FQpidncVdd93FlhlN8Ia59dSoy0QigWAwyPuiuo79fh/JZBILCwsj3zPlchnpdBq5XA7r6+uIRqO4du0aKpUKJ/NrmoZSqYSlpSVcu3YNc3NzfH++/PLL0LTNlKCpqSluLTRp9+yREy6i2+1yu4xMJsNWB/3wiUQCsViM68ypi5zUR2hcVyHw6oBIrkJ11qr6qQfd9JQECmzOkAbNOqlCwfLyMpeZ0nUdq6urvEbU6/UwNzfHeUYrKyuoVqu8+B2LxXh75Eq8evXqFktMhdwP6+vrXNrnzjvv5EXrer3OOTxTU1OG67bdQ0HCRcmXo1zvw/CAkRsGeHWScRiSj3eL6kIml/IoM3FN28xfpO7B1PVAFa5RBkUaTJPJ5LZh+NcDTdPYjTdoXdnj8SASiWB+fp7dbF6vdyQ3G4mhWs2CrCZgM4KSnsdRrhNZWisrK1zXMZvNGlyFFAmZyWSwtLTE19Nut/O4Qe5JdT3yMDxXo3LkhIt+ACqYu7S0hPX1deRyOU4cXF5eRiqVMsx0yKIol8vsKqTtqew0A6X3qVfRoPUeEi4zjUaDTXtzh1eCbsiLFy/yLBmAoetyu93G6dOnoes6MpkMrly5glKphGazCYvFwo0vVb/3+fPn2aWhnrc5AGNlZQWLi4vQtM38lnQ6jUwmg1arhWw2C13XceLEiS2Rf9tdP+rFNC432uKic9E0jevPAeDmg9uV/znMqEKl1t4bNdCGJkOJRAKXL182rCWPis1mg8fjwdTU1LYV3vcT9dxCoRBXPzG/7/f7kUgkcMsttxgmnuqkdNj2qVuEakVSQV9gMxCFJnrbTe7o71qthvX1dVy7dg3Ly8toNpvI5XKo1+twOBw8Ia7VatjY2MDly5d5OcHtdmNxcRFOp5PFlJYQJs3FfWSFixJaz58/j5WVFRQKBbTbbaTTaTQaDSSTSdxxxx3si6cBPJvNwu/3Dy2bshPqzW6OTqQbRA3JV2+YarWKTCYDYLCrkEJxl5aW8JOf/MQggC+//DLS6TRXL3jjG9+IXq+H5eVlvPTSS8hmsxyxd9ttt215GH7wgx8glUohlUoZriNBKQXnz5/HK6+8ApvNhlKphMXFRaytraHb7WJpaQn9fh+nT582RFMOeyjpetAMkF4b5zofFGQda5qGcDjMiauTJloEBWKYS5oR252X1WrF7OwsGo0GvvnNb25bJHcYFMhw8uRJ7oF3o9A0jRsxDhIjaj5Jzywd7yiWodVqRTKZNFhn6gQ1lUrh+PHjY7kKr127hpdeeglXr17l571UKvFkV9M2Q/ivXLmCH/3oR2yJeb1enD17Fm63m8vS0ZqyCNchoNfrodFocLg5BWXQgiQFLNRqNbTbbQ4hVSN1xs2pMEOzxkHuBLIwzLlVqguBgjhUSLiq1SqazSby+Tzy+Ty8Xi9H5LVaLRQKBRQKBTgcDpRKJW6pUK/XeXZmjrzM5XK8XXV/9Ll2u82JmPV6HRaLBfl8nr9DszxqjDdOaDjNBrfLNVOhnLNxKzQQmvZqr6jdQtGVAHimS/k5kzYIAK9aXebJ1LDgDPV9TdO2VH8YV8ApQMRcQWLQNq7HxICskkH3hFoJXt33sKah5mtmTr+h6+vz+bg03E7QxLFQKKBSqaBer3PBhGq1ymtv5IGhgA2qak/PJEVIWiwWdu1PIkdKuOiG6HQ63B+o2+3yYEJiYLFYUCgUUC6X0Wg0EAgEDOHNZOKr2x3VZUJQtI56s9PxkY+c1gIIqiJN7gjzMei6zmWiLJbNVuAUTELrVwBQq9WQz+dht9tRqVT4oaE2Co1GwzA4UfkpEvJB50wRmo1Gg/sKZbNZLsTqcDj4MxTxOOrvRcI1ahIzXSdVZMdB0zSO2hx3EFSFnPavumgmFfXYd3MelHc46tqPGbrfb2S3b/W5JFfeIBclTULN69WjujPpnMzPVCgUgt/v3zEwidaXKXiMBMdms/Faa7fb5ZJPNMGl+7NQKHDVe6vVikqlAovFwtX/J5EjJVzAq7XF1tbW8Morr3A0D8164vE4vF4vLl++jJWVFUxNTSGZTAJ4tSrGqDP/UY5l0GsOhwPBYJC7n6rrTWpJFjWyiyyj5eVl9Ho93HLLLVhaWuL/0xpDpVLBxsYGVlZWOHft5MmTiMViWFtbQ6FQ2CJcw1DXdIrFIpaXl2G1WhGPxwGAXYapVAqJRAKrq6vodru4cOECOp0Or1vttA7gcrkQDAY5nH+niQKF36uNC8excsh9QzUrd0Oz2USlUuFjneTAjN2iWmf0jN16662GvmTjbu8goIndsP3TgE+f3WnNdiecTidCoRDuuOMODr8fti3aF3lYXnrpJXQ6HSSTSUxPT7P1pWkai2Cv18O1a9dgtVpxyy234OrVq0in07BarTh9+jSef/75bSv/TwJHRrjUB6heryOfz2NpaQkLCwts/dAPHggE8P3vfx+5XI59zTRI0+xmv4Rr0DoBremQ9WK+gUjAzJF53W6XS+rMzc3hwoUL2NjYgKZpmJ+fh9/vR71eh8vl4mAUp9OJO+64A36/H71eDysrK9t2mB10XckNSO1WgsEgPxwzMzOIRCKYm5tDIBBAtVrF8vIyL/qqD6X5waRtU11Guh47QeHE6XTasK1Roeu/lweXSojROUyytbUXVLeYw+HA7OwsPB7PltYgo27noBglyMI8oRr3mOnzlGS9sLCwpcv5sG1S4Njy8jLnm8ViMVy6dImvtcfjgdPpRL/f53FhZmYGZ8+eRbFYRDAYxPz8PH70ox+hVquJcB02KClvbW0Nd9xxB0cK1Wo1RKNRhEIh7jyq1goEtpZT2i3mmZn6Og2c2WzWUO9MFU9zpA+JWS6X4zI75CKwWCy48847kUqlOLqoUCig1WohFAohkUiwqFDS8zguAk3TeM2QQoa73S5eeeUVLCwsIBQKIZVKIRAIoFwuY319fWBJqmHbpjyuTCYztOK2SqvVMuS8jfsAUmmfcb+n/ibqGhcwXrffowgJVyqV2nUe1kGK13aTj/22qCmCcnp6esemj3TPUXGCtbU1nDhxAsFgELOzs2xJAeBIx36/j0KhgEAggEQigVqthmKxiHg8junpaVgslpGes8PMkRIumuFXq1Wsr6/j/PnzeOc73wmXy8UD49zcHLvNVldXt7ibyLK5nq5CmnFdvXrVEChBx2Cu5k4i2Ol0sLq6iqmpKdxyyy1csbparSIajeKWW27hAr1ra2tsaZ44cYJ94fV6fay1ITq2UqmE1dVVWK1WRKNRtNttXLhwAW9+85uRTCZx6tQpxGIxXLlyBZcuXWJB3mkwIldhIBDAlStX2IVhHijMFnUul8PVq1e3vD8KdrsdMzMze/Lvq67CSS2bs5/Q73jy5En4/f6xA18O+vptt3/V2lLXuHZ7zNRChspAjbKdZrOJQqGA8+fPY25uDolEAnfeeScuXbrEa28UDd3v97G8vIy77roLx48fR6FQ4By52267DXa7fdfrw4eFIydcnU4H2WyWu7FS8cpOpwO32829cYDN0NJCobBlO+Yabfv9UFksFrhcLjQaDcMCqRoePmifFE1HwR3tdpsj6ygku1ar8cKrpmnsVmi32wMDQsgCjMfjCAQCbJ2aLUaKeKSSNxTRRMWEqbOzruuGTrgUbk9lnQZ1n6ZF+Vqttm3bdoKCaKjoKe1nVJxO58jrfMOgSvzA9S8EOwqHYfZMybZq4vFRQb2fza+NC6WxUEufYZ4Z9XXK36pWq3A4HFy4l9yywGb0IhXw1nWdQ+DpmbLb7YjFYpzDdaMqlFwPjpxwUTh4vV6H1+vligbFYhF2u50rKLtcLkOlCoIsor2Gww87PtqHxWJh4TE/FIOSlskSo4KumqZx3y6qeOHxeAxtTex2O8LhMLsjqEKI2dKw2+1IJpPcrXXYsVN9QurgCoB7CakJmWb3Z7Va5QdVFS51FkvXg0R1JyEgy5jEflzX516jqQY17DwoBrmWDwpVtA7D8VxP9nJ+FKUIDO4qYL6fqGIORUBSfUyaEAJGVyEV86Z7ndyTbrebO7RPcrWXIyFcdAP1+33U63UsLS2hUqng2LFjCIfDaDQaPMjTrD+RSLDIqYJC1sxuW4ePA603mWdxdEPSrEuNKqTahBSNGI1Gcfz4cYRCIT5myk2LxWI4efIkW3eDRIGiwe68807Mzs4iFAoZPkPHRg+a3+/nclqUh0KJ0hROrFogvV4P6XSa3SIU+jvooadctJ0GBHoodyNaw/a9Fw7a4lLdy8L1Yy/uQRX6nbZbjjAnglNCsdfrRTAY5KRoEitd1+HxeNgbkkgkuAg2sOmRSaVSsNvtmJ6eRr/fRywW44nGQXsMxuVICBfR7/dRrVZx5coVVCoVnDx5EtFoFJlMhluFUBb51NQUV1lXB2cSLsq9GBZksR/HSutNdAPTfqhFhPlm6vf7fENSTcJoNMrh7lRgs9lswu12I5FI4Pbbb4fH40GhUNjSVI7+7fF48NrXvhbz8/OIRqMDz5vqCQYCAWQyGdTrdQQCAa4KTqJvnsH1ej2srq5y8d14PD7weur6ZuUS1VU47GFSW8OMGh1p3td+/KaH5WFXUziu1/26HQexz4Ngp552O0HXie5ZNYHePFGkiSutr+XzeRaucDjMRXPVljqqcKVSKbjdbu73Fw6HMT09zY0/7XY7UqnUjl2XDytHQrhUq6TZbCKTyXBZJ2q1QMJFfYZCoRBXflCh9acbkQiprlHReQCbJr+5AgHd8FRZgCIDfT4fUqkUfD4fHzO12ggEApienubSNOoMT53NORwOLCwsIB6PG8pMmfN0qF0JtQqnJF6yogYlcPb7feTz+S0dnQdBVcVpfyrq/80FYMcRouvxgB70Qy8W1/WH7i+aHOzHtkbppA6A01EymQycTicnetNE2+wqbDab7IGhggI+nw/hcBh2u53rkQaDwYlsaQIcEeEier0e6vU6VldXYbfbMTc3h2AwiHQ6za5Cqp4RjUbx8ssvo1QqsTjQwEszl+sNNX40h8BT+RlzDli/30ckEuFAiX6/j2AwiLm5OS4SqmkaJ/+Gw2EcO3aME4GHuQpdLhduvfVWBINBrhloDtElF6vP52PhosaDlMg7qChrv99HJpPhosPq+ZhpNpsjhenuZQ1yP6wt89rWXnJ79gOyuKh6ykFbXGbr4aiwV1eh6vqntadB3gWKVFUngaVSCSsrK1hYWEAgEOAJrPosuFwu2O12TvuhiTywKVLxeBw2m429NpFIRFyFh4Fut4uNjQ2Uy2VEo1HMzMxwlFyr1WJBooKeFBmnDmbUME6NrgOuz0NI3Y7NMzC/37+lrQrd7PF4HMVikRdqA4EApqamOGNe13Ve4wqFQtxniCIuCXPiKLkPhgkCCWEoFOLrRkKnrgeaBY+Snr1er6F9gxk6vlFKPlHCcigU2lUbcgrQ2e3DSkFAVLGA0i0OilqtxmkRByEaIlyjQdeJJsjqvW4WLgqq8vl8aLVaqFQqKBQKuPvuuw2NSynSmMqOkQWVSCS4gS6wOaZQIWhaJw8GgxKccRigUHhgc6ClVhPUrdbj8XAkDw3m5tBwSoY9iJppBHVmNs/kqUJ1vV5nEXI4HBw1SFYZWZck0gAMRWkHueHU2oqDjo2EULW4yM+uDlyD/s7n81zOaT+g2oaxWGxXRW0pTHjcQUidKTcaDRYKt9ttEK4bPXulZGxKTr8ZIvpuNDutu44DeSaGRaaqa+4+n88QxKWuf1NaSLvd5pJ25D70+Xyo1WrswbDb7ex5oUnbuNVNDhNHRrhoxr6xscHuPp/PxxWRKY/JHI5t/jcJF1kR19v1Yp5p0SKrOayYxMXn83F7dGDTelAj9UiMKfycXqfrMGi/wNbK1/QZOnePx4NwOGwQLgqNV/dtvla6rnOFEopwoteHXYudHiaqPJJIJLYEtuyEOgHY7UPb7/e5Ej51sx7UgPB6og6k5XIZGxsbWFpa4pQHsbgOF+o1UYWLXlNdzpR2QlZVo9FAp9OBpmmGMUzXda5HSs8ijRN+vx/5fN4gXDQeUCeDSU6cPzLCBWyukVy5cgU2mw2BQIAXIamuXCgU4lkHLWab3XQUuHEjXD9mi4oEhhZW1QGBknQjkQhXZafZPgU9kGjRGhfd+PQ6id2gdRlzLybzMQYCAczOziIcDqPb7aJarXIxVfUamgerfr+P1dVVzM7Ocnmt7Qa0UR4k+n2PHz/Ov9s4gQk2m81Q3HQcaMAplUpcu5ESsMc5h/1C13Vks1lcvnwZL7/88oEI183CfoXDq5a7Ct2P/X4ffr+f251QFwtN2+z9Rt3Ce70eTwhpDKDk5kgkgrW1NU60dzqdvH7t9XrR6/W2VLufJBE7MsJFFtf6+jqbyjQLprpyapi7+j2CBnB13eZ6/ph0A5tnqKqrUBUumklRtJCmaYYbUnUVqr15ABgsrkHnNGz2Ra95PB5Eo1G+6ev1uqHhnznAhNB1HblcztBmYbvrMQo2mw1+v39X9QZpP7uxuNQZcbVaRbFY5G3RIHMQFhcVHKbyPxSsIRxOVIFSIS9Lt9vFbbfdxqHstDRA99ogi0ttJ0PPB1UMArDF4tL14RV6JoEjI1zApkDlcjleA6FKEt1ul0s+2Wy2bRf0acaiBincqEgt2gdFFaqo6282mw3tdputMLohKZeHZlNqHyCyxMwJwur2t7uJqb4aXT+y6lRXIS0Um92flK+muir3AgWUBINB3se4399LcEa/30ez2UStVmOrd1DL9xtFo9HghXhKzBbhmjxU4apUKlzEmVyFAPj5p/GLmsTS/UzC5fF4uBoNbZvGAxKuQdV5JkXIJjOkZABkcam5Di6Xiy2QbrfLgtDr9Th/a9yw6oNywdAN6Xa7oeubjeIoh4qEiyLdzD5yYFPUK5UK3G63QdDM/vVBkKuBSsTQ9VT7LqkCpVqQtKCs7nevqM1BbzR0biRcwKtRiur7grAf6LrOgU0WiwWRSITvNQBc3zMQCPCklCbu/X4flUqFvS8keDSpVSedk8aRsrjUxFga4NXoNvqhyDIjVxtBrja6UVST/HpgjsCjNaZKpcKLqupASBFj1FQuHA4bxIkCBsiiUAdT+g6F+mvaZkLydk3sBh1vpVLhVioej8ewxlUqldBoNAzNL3Vdx2233Yb5+XkOh9/r4n2tVkOlUuFzHXc7FouFK43sZp1LfeDJwjwIwaJ9hkIhzMzM4LbbbpM1rglGdSEmEgleAqDO5JqmIRQK8QTQYrFwVCG5ClWoFREtPdB4MsyymqRJ15EQLjWwodVqcUTdoDUXssCogaQ6eyEXC/mN6bXryaBorHq9bqioob6v65t5WpVKBT6fj88T2LxRyQqg0lZEp9NBuVzm7wDY1aBLx0buOlWEarWaweIiH/78/Dz3BBv1WmxHs9lEuVxGJpPZVfi3arnuBvWc1dYzNxJ1fz6fD7FYDLOzsxIOP8GowhUKhXhSSm528xqXpmlcfYcmYeqEkfK/yNNkXns2B2hNEkdCuAg1aECNqFND3YFNV1MulwNgFC7ahjl024zZUtotg24kTdOGWlx0UzYaDWSzWczMzHD0o6ZphoABj8cz0OJKJBL8nVFrr6kzNTo2CoBRXYXFYhHVapXzwej8Tp8+jWPHjiEej/PxjHJNBr1O55nNZrG0tDT2QE2uFL/fv+vfTx1gDrrMkqZp3IGaLHZVUIXJQbXkk8kkfD4fdH2zu0Kr1YLFYuGoQvo85XipFhc9q/V6HYVCgcVu0P0+qROcIyFc9EP1ej3UajWuZEAPsDroAeB8L9WdBrxqcakRcOYfdqf/j4s66NKMqVQqGULX1b+BTcsql8vhnnvuYcuB8nmovxi1MCCazSay2Sxuv/12dkF0u92xapXR2hoFeagPi65v5mtRPzByRQLAXXfdhWQyaSjgO2z76t/DqFaryGQyuHr1Ks8kxxmoHQ4HV8jeDWZXofrajUK9hlQRgSohiMU1maiWfDQa5UjVarXKrsJhwkXFFlSLq1arIZfLcTFd8/N1U1lc3/72t/G7v/u7eP7557G2toavfvWreOc738nv67qOT37yk/hf/+t/oVgs4k1vehP++I//GKdOneLP5PN5fOQjH8HXv/51WCwWPProo/jv//2/Gwq87ga1aoQaoUOWBVXRaLVaqNVq3MPK7PpRo3jodWI/f2xzODxBbViGWVydToctSzXgod1us+Cq4fD9fp9z2chKoqRsmomNGqRB7VFoHU0VDnqP3JfUddnv9/O13u5aDIt4VPdP7pFGo4FKpbKrgdrhcHDE1m6gyRCtmw5K3r6R0OTFnJcjTBbqb6cGXVGfOvKk0GSRonv7/T5cLteWaFK1+asaiWhGvZcnhbGFq1ar4TWveQ3+3b/7d3jXu9615f3PfOYz+IM/+AP8yZ/8CY4fP47f+q3fwkMPPYSXXnqJrZv3vve9WFtbw9NPP41Op4Nf+qVfwoc+9CF86Utf2tPJ6LrOgQP041IVdWrO2G63UavVUK/XB1YsJ3cjDWyDfuhB4rWbgUttAqcO2MOESz3HZrO5JWxfFS6Xy8U3PuWxNZtNjj6q1WpoNBrcFoFEbNAgrLoKa7UaW2pkcdExUUSj3+9n67dcLnOKgdktO+h6DHt41N+BBLFare5KuCgPbpzvqL+F2kV62DW7kdD6BgXdiGhNPpTqAoCjeGmySCLU6XRYuHw+Hz/naqNZqnAzKHpatcBowj8p4jW2cD3yyCN45JFHBr6n6zp+//d/Hx//+Mfxsz/7swCAP/3TP0UymcRf/dVf4d3vfjdefvllfPOb38T3v/99vP71rwcA/OEf/iHe9ra34fd+7/cwPT2965OhWTvlMgGbkW7dbpdnKoVCAel0Gv1+H/F4HKlUyjDToTWuYcmy1EZAtRB2Aw005nwi8mkPq5JO1o3q5qLv0wwM2HQVkkuwXC6jXq9D13VEIhH0ej0sLy9zBKbT6US5XObivoPchyQO5Cq0Wq0Ih8Ow2WzcW4wWiWdmZtBsNlGpVHD16lUucKxWlzBDs0mqGDIKu5007EVkaJ2P8ugCgYAhCOYgoFQJ4egwrAQbrR/3+30UCgV0Oh04HA6EQiGUSiX+t+pJoZZHNMkzezU6nQ6q1SoCgcC2k8fDxL7e7YuLi0in03jwwQf5tWAwiPvuuw9nzpzBu9/9bpw5cwahUIhFCwAefPBBWCwWfO9738PP/dzP7fk4yD2oaZrBfWWxWFCv11EsFrm3lbnVBiWXDkuWNbu01MFz3AGR2hCoNwpFA5kTSFWrR3VVmY+dXlcTk6lBI7BZuqnX6yGXy6HZbGJubo4Xct1u98BBWN13q9Viq5YsLormpIod1HqlWCxifX0d9Xodt956645rSqoFOcjaVKH36CEelWHu2VGga9tsNnkGTAmh6uTnICIMzbXnxOqaPMz30KD7SF3HoskopabUajWu0aoKF40zw+7LbreLRqMxMKT+sLKvwpVOpwEAyWTS8HoymeT30uk0EomE8SD+X+04+owZyg4nyuXyls+oAxKZvgC4UzCtxzQaDYNwhcNhfvAB7BgObx4g9jJI0UCtboOEkxJsza6f7QZp1RJzu90sXOT61HWd3Xi5XI6vK61PkeAPQ13jUvO4qA4itVeIRqNot9soFotYW1tDpVLh9jGDoPM3P2CjiBf9dtc7im6Qu1QVLvrMQTAJM2RhfNR7nwRLDXyqVqvodrtcdJrSYWjso4ksuRfp/h0WqXxQSf27YSL8C0899RQ+9alP7fg5+mFVi6tcLqPb7fJsolwuY21tDf1+H7FYDFNTU1tchVTUchCqcKnm/DizePoshe2bBx61HYEZEic1f0i1xOh7Ho+HgyEqlQrq9ToAIBaLoVqtYnV11VBpo1QqIRqNDhQAOg5KzqaowkgkwvXQ6vU6d14mV2E6ncaFCxdQLBZRKBR2LENENSJ3uo5mS3dcdmtxAa+6clutFjRNY7fMQa5xqS4k4WhAAWXmMUB1FVJnd7Wju8ViQSqVMqxfqa5CmnCpE2KqqrPTxPUwsa/ClUqlAADr6+uYmpri19fX1/Ha176WP7OxsWH4XrfbRT6f5++befLJJ/HEE0/w/8vlMubm5rZ8jtaNKEiBos96vR5sNhsnrlIibjAYNBRHVWfvw0SD6h7S/4etNw2C3iOBoahA1e1FFsywmQ9lygPGYAZy47Xbbe7yTJZAo9Fgi9XlcqFQKKBQKMDlcnGUZTab5Y6og4Io6HjITUbnoeubCdEUhu9yuRCJRNBoNNBoNNBsNlEoFAauGarXg6xiEoHtrC2bzQan08khwLuJKlSv+7iYi54Om83eKPbD+hcOD+b7yLzuREEX2WyWhcvpdKJSqRjKv1GA17AlANpWvV5HLpfjprM3shfhbtlX4Tp+/DhSqRSeeeYZFqpyuYzvfe97+PCHPwwAuP/++1EsFvH888/j3nvvBQB861vfQr/fx3333Tdwu2pvqWHQrNPr9aLdbrMFQOtFFElHbqtAIAC/328I0bZYLBxsYJ55kPux3W4b3IgUCDJOsAaFp1NEH32HWnFvNxCrbkw1kIKEi0pVqTNwWrOjtS8q4kkJjq1WC7lcDoVCAW63G+FwmK+p+jetZZGwUlHdTqeDQqHA616hUMhwnBS4MSzYhFyNVE9tEOp1dTgccLvdLFzjzhLNlfPHgdYWaOY6LGJLEHaL2SNAzzMAniiS2NBz43Q6Ua1WObKUnj+KRBzW/YGEK5vNolgscqT1QU3CRmXsJ65areLixYv8/8XFRbzwwguIRCKYn5/HRz/6Ufyn//SfcOrUKQ6Hn56e5lyvO+64Aw8//DA++MEP4nOf+xw6nQ4ef/xxvPvd7951RCENiE6nE9PT06hUKtjY2ECpVOL1IgBYWVnB2toayuUyTp48iXg8zjkwJFpU12uQ26XZbKJYLKJYLPKAT2Jhs9lGjsihwAVKgKYbhArZ0nbMC+2apnE3YQBbyjcVi0Xk83kuhkvfq1ar3M/HarVygMpP/dRP8Tapl5Ou65iZmRkYEk+pAmRxUcfdarWKS5cucWBGIpFALpdDr9dDKBTi9a9htFotlEolg6twuwfH5/MhkUhsKeg7Kna7HeFweCzXmup6URNCg8HgSFaiIOwWcvWpk7xsNotz586hXq/zOmsmk+HJcDqd5g7d5jJR6rNSr9exvr6Ol156idfcY7HYob+PxxauH/zgB/jn//yf8//Jhfe+970PX/jCF/Drv/7rqNVq+NCHPoRisYg3v/nN+OY3v2lwP33xi1/E448/jgceeIATkP/gD/5g1ydBA4bD4UAikUChUECxWES5XDZE6KXTaWQyGVSrVRw7dgzhcJgT9yikmHpMDRq4qfZXpVLhPArVdbfTQEjbJIuHQtHpdYfDYeirM+jmKRQKfENS+DqwKVzkBqWWBnSDqnlpVqsVrVYL5XIZ0WgUmrZZqWNxcZH7SqmBCOY1NAriAIBisYhAIIBarYZr165xq5FoNIrFxUUAm1GlZMEOCu81Xw9y4ZmFTn3gPB4Ph/WbH8RRsFqtCAQCu44spDVIdVA47A+6MFmoz57H4+HnnCaPhUIBV65cYRe8zWZDLpfj9Ix8Ps8dMMyd3wlN09hyu3z5MiKRCPe4O+z389jC9Za3vGXbgULTNHz605/Gpz/96aGfiUQie042HgQtUpJVRRUwaKGzUCiwFZZKpQwDP1kjFDY/iE6ng1qtxhFylDdFCbmjDqCUnBuLxQyDHq0vmRfa1RtJzTFT+3ZRLhWVvFIH9FarxetyFouFgylCoRA0TUOtVsPq6ioSiQRmZ2fZVanevCRcqvVEgRrNZhMbGxuw2Wzw+XwsZsCr3Va3uzYUjhuNRreNKqR/u93uoa7HUaCIyN0+nGordbVl+qQsbAuThZp0TO56CjJrt9ucW0qeFQpCI4+EOk6Y7/l2u41SqYTV1VWsr6/zc3vYORLOedXiSqVS+P73v2/ouktrXGtra9jY2EClUsHx48e5tw193263G4TDnCdFN0y5XGZfM62jkatxlGOl6u6zs7MGoaRuxrR/s3AAm1YOuQrJtUYiWiqVUCqVMDU1ta3F1Ww2USqVkEwm2U138eJF+P1+zMzMbCm+S8dBqQJkcRUKBS6fdeXKFdjtdoRCIcRiMV4vCwaD20YrkeVaKpUwOztr6BU2DHKR7lRtfhj0MI/rKqTJACWIq1bvYZ+hCpMLBXHRc16r1ZDP57G4uMhLADabDdlsFolEAna7HYVCAfV6HZqmDbS46H6u1+vIZDI4f/48EokE8vn8gbbqGZUjIVwADMLl9Xqh65st40ulEucyUJkfv9+PqakpXk+iH8put3OCrrmtSL/fN7iIqIlit9vl0kO03jTM1KaBj75DlTMIc7PGTqfDwRq0PVpjoo7EatAKtTggy4WOhdp2O51OFItFtNttHvjz+Ty63S5arRby+TwKhcIWwVYtoGq1Cl3XYbVakc/nkclkOJn52LFjiEajBp96v983lLYadD1osXnUyhnkx9+phNQw9vpQqkLsdDonJmlTmDw0TUMgEEAgEIDX60Umk0Emk+EoXipgQM8tTXopx4uib9U1LhUaj1qtFjKZDE/KDztHRrgAcBmiSCQCp9OJUqnEycq0zkXBF+QLVn9Iq9UKj8fD7j91fYeS9ADwoE+i0mg04PF4RmqXbo6iU2dCdrvd0ORxUKFfElSn07klzJWCTMy5RV6vl62zbDaLXq/Hx09BJW63m91+29246gyPqrQXi0XY7XZEIhGEQiFuedJqtdBoNHjRd1BeivrgOJ3Okcon0e+yF/HZr9nkIKtYEPaKel+53W74fD54vV5sbGwgn8+jXq8jGAxy41bqdkHjB41fNE6YvUFqxDCNOxSJeCTXuA4jdJHtdjs31COLK5/PQ9M0dDod+P1+RKNRuFwu+P1+w49JP6Df7zfkPRFkcdlsNvj9fo5IBMCh5TsJF62DqMUvVYuJXIWapvGgHwwGtxwHCazZBUBWXzKZNFguJOYejwfXrl1Dr9fjVgd008bjcfafD3Lr0bYoMMThcKBcLuPatWvI5/MIhUKYn5/ngI94PI7V1VUUCgVeXDa75lSBpnqG9JvsJEx7FYm9PJzqfSNiJVwv1IlnKBRCJBLB4uIi6vU66vU65ufnkclkoGkaFhcXuRIQsDlRt1qtcLvdhnHCHMxEXp5YLGaopHPYOXLCNTMzg5MnTyKfzyOdTmNtbY3XdTweD1KpFGZmZhAOh7fk8lCBymq1ygEQakRdqVSCzWbj4rKUB0UD96CuxWbI2lJdYwTlUFAeBi2UqoMsCZff79/iKrRYLHC5XJibm+OgAV3XkUqlkEwmEQqFcP78efh8PiwsLMBms3Ek4/T0NFZXV9FoNAZG9NH+i8UirFYrHA4Hcrkcl46KRqM4efIkVyKZmZlBOp3GxsYGQqEQrx2a188o4KNer8Pr9Y7l/tuLaOxlRmnOqxGE/Ua1iAKBAD/DL7/8Mo9Hp06d4qLiZ8+eRTQaRTweZ4uLPEuUsqGm1RC05DA9PY1Lly4Z0ocOM0eqRgyJSiqVgs/n47D4UqmESqWCfr+PSCSC2dlZ7kul/piUx6WGuKvuwnq9zknOZLkBmyWaaH1pO9Q1LnKNqcJD1g/waisSQs0jslqt3CzTbHHZ7XZEo1FDwEAwGEQwGITL5cLq6ip6vR5isRisVitn3UejUQ6UGHSDq2tsZHFVKhWsrq4il8vB5/MhlUpx7cdIJMKh9rS4rFb5oG3SemK73ebKGeZ9myHX7V7+7IVhyZyCsF+ok0Wv14tgMIhQKISlpSVkMhnU63VMT08jGAxC0zQsLS3B7/dzwBJNcN1u9xZPhorNZuP0Ego2m4TJ2JEQLjWcPBwOY2pqCn6/n6PsyuUy5zSEw2HMzc0NrIRMVhSVi1KhtSxyFUYiEfYnU5WOUUzsYQM18GpFCE3TDL21zMESdJxm4SJBi8ViW4QrFArB5/NhfX0duq4jHo+zcLlcLu5OvN2NS+JNwlWr1ZBOp1EoFHhWSCH2JFyUV6aWRVIh12m73d62NM2w47nR4qVpGjePHPSeIOw3Ho+Hn+G1tTWumDEzM8Nrymtra+yFAV4VLq/XaxAuwCiKdrsdXq+XOzrQxPWw38tHwlVIUGLp1NQUMpkMnE4ntzShHz+RSCCVSrG1ZPb3xmIxNJtN1Go1tFotgzuR8sTIgrDb7RzgsNMsXA0/JQvN5/OxhUXbd7lc8Hq9qFaryOfzAIwDIll7VOtQtRpJtBOJBAsXRVHG43HMzs5y0nAymWQLzePxYHp6mjP01WOm4yYXZ7PZhM/nQzweZ/F3Op2YmppCPB6H3++Hpmk8E6xUKlwPkpp3qhZXtVrlaE2/38+/y2F9cKjEFaVYmCdAh/W4hcnF6XQiEolgZmaG0258Ph9mZ2cxOzuLRqOBTCaDeDzOE0ev14t4PM7BGcMiXx0OBwKBAMcFTEqF+CMjXDRIU3IpDeI+nw+hUAhzc3NIJBIcAq9+j/5QHhIN0I1Ggy0Aq9XKNwWFqVMoKgVLDLMW1BuhXC5zHpa6xkWfofUrKk6rHicAhEIhJJPJLSHmVqsVyWTSUDmDsFgsXLW9Wq3ydaDtUqmmSCTC9f/MUBtwXdcRCASQSCSQSCRYQM3J1FRDsNvtwu12c9URVajpeqjJyqpw7iQCNzqqkIRc7QIdDAbZmhTREq4X5EmZn5/nNSm/349EIsGTYZpUA+Alk50qxFBhcnp+J6XDwJERLuBVEXK5XAgGgyxc4XAYMzMziMfjWwZ1FZvNxgmzJFzkMyYrghZDqSAuuY12cnOR5UIWBgBDKReCRIaShGnfBFmNagItnXcsFkOj0dgywyLLYHp6GvV6HbFYjF2SFGgRCoUQDoc5P80MXRMAPJubmppiYYpGo1smBLSI7HQ60W63WTRVIVdbrpAluxMHIRBqcAzl0h2WDsjC0YfWoebm5nji6PV6ORqw2WxysJambbbboSjB7Z4XWgcLh8MTVbrsSAkXsDnAUBHWU6dOIRqNYnZ2FrfffjvXJxyW0+ByuZBIJDhBuFKpcJSOzWZDMpnk2nvmvlyBQICtp0FJfgQl+QIwVKenz1gsFsTjcVSrVUNjTdomhbpTtQ7CYrFgYWGBF2jVEi+U23XrrbfC4/Fgfn6eBZnWxVKpFAdXDCpf1Wq1eJ0wFovh2LFjOHbsGEcszc7ObrG4KBDF5/NxTbRbbrnFsMZEVakBcADJoGs47LfeLbu1uCi6tNVqwWazIZFIDP3dBWG/II/JnXfeCb/fj3A4zCkoXq+XrS8aT6amprjc2nZWFLkdZ2ZmJiqZ/kgJFw0cHo+H14HIQjlx4gSb1zQwm907lERLtfzIMiLLJBqNotvtssWluhl3shZooFaTotXq7qpwhUIhlMtl5HI5w3kBm3UeqTKI2aqanp7mIAzzuksgEMCxY8fg8XgQj8fZVUgWVzwe59fV6D/aNzWdBMCuioWFBd4X5YWp+6RAFI/Hw5X1zQEaxWLRUMV61HYjgwI9bgSDmmkOi9gShP3C4/HwZJyEiiaffr9/S8BYLBbjQI2dLC6v18sTMHEVHhCapnFQRigUgsfjQSAQQCwW43wiNUdChWYfwGZ0ndpunhY8qXmi+btqb6ztoEKYVIZJ7b9F1pHf70c2m0WpVNoSqUjRhOZahiR4tE5krjVIkYMkYlTjkNb2KPlZzf9SoTqFwOZDFAqFEI1GWcBVK0+Foptojcx83ahoMB3DOL2tbqRQqK7Per3OkxeqxCII1xPqvEBWlcfj4bGMnht1jZi6PNA4MexZoWAPWuoQi+sAoTp2FKEXCoU44o0G9EFdi0m47HY72u02crmcwRJSm06OgyoChUKBF1LVqs8EuQovXLiAcrnMHY3puL1e78CCvrQGp+v6wAAHyhmjpGOHw8EuT7KOqCEdrePRseu6jmazaSjuGw6H2S8OgAVfPQ/6vrkBp3pdqM6h+XocJutFzaFTLS6Hw7El1FgQrgcUxdzv9w3eHYqYprxOEh6Kdt3u3lS9TtSBXI1GPkzPoJnJsAt3Qbvd5hp6NGBvl79Dr1PZKF3XsbS0ZBC43eYEqQv7VGnixIkTW7LZySVJ1d1rtRrK5bJBRLabPY0S2UYWkjoYt9ttLC0tsSVhviaUv0UFi0n8AHBwyqD8LCKXy6Hb7W5JLibh6vf7OH78+FitYW40dMy9Xg+VSoWDMw7zw31UMD9j+5VIPmnQ8sCg0mmDPDCj3pvUT8/j8Rg8Lof5+h454aILTgEW1KuGBkVVKMyorjpgM3DA/OMNe2hG+ZF1XeeqzclkcqBZbrFYuDsv1StUmzDuVbgGdVbu9XosqIPOg8oyketUncmpybjmwYWOiaqWqAEjJJ7lchn9fh+JROLQ+9fJ4mo0GrzoLcJ1fVEnT+rf5vePOmq6jPn5pQmvyihjAX2/2Wxy3uukBGgc7pFiD3Q6HZTL5S0W1zDoPUoy1nUd6XTaYDKrD8+oFpd6c+m6zm1FZmdnefBXLSmr1Yp4PA6LxYJGo4FKpWKwgnYSrUHWj4rZ4gI2Q93X1tZ4QDYfv+oqJItLzW9T9znomhSLxYHNNvv9PrdjmJmZmYj6f9QPSYTrxjHq5PFmYFChg0HCNeq9Sd6UlZUVXl45zJ4P4sgJF93Q1F9KDYKgwApgcMg6BSpQpfQrV66wy2s7N+GorkJd15HNZtFqtXDy5MmBhS+tVitmZ2e5pw5ZK/T9YeWGgNFuVtXVQMfebrdx9epVHpDpPYIq4+fz+S2uQtWaNV9POh5yFardpkk8M5mMoWDoYXxg1OPt9/sol8ssxOaZ7WE8/klmOw/HzSZe9OwNsrjMJdXGsbiq1SoWFxe5PJQI1wHS6/VQr9e535T5xx4GBTnY7XYuzdTr9Ub6/nbv9ft9VKtVzr+KxWJD691RlXSLxcKDpPr+Tse/0/vmgbbb7aJSqfADYKbdbqPRaHCdQioAbF5zM58/lbCiZpKqcPV6Pa7CT6kGh91VSOdJzUQnKWFz0hnmKrzZGCZIg6ywUaCJa6VS4ajiSbinD/dIsQvUdZtms8nCNeqgSGHlTqcTzWZzrDL/2/3gvV6P3YRWqxWRSGRLyDr9rfbaKhaLhorte2WQpamKvFqtnt6nFiuUgNxsNrkT8nYPEvX6oVwxddvdbpfPjfLnDrtwEdQyQk2tuNkH1OvNsBSWm43tlgp2g7p+7ff7RbgOmn6/j2azCYfDMVJzQtUdNDU1xSWMstksJ96qnzP/2Ylms4mzZ88C2EzgjUQiA11NADiyMJVK4dq1a1wbb78hy4e2HYvFuEq8+pmNjQ1kMhmUSiVYrVasrKzg/PnzhoKc5vOw2WyIRCKYn5/H1NQUJ0TS5+v1Ol566SXY7XYEg0EOSBn1eh4EahqC3+/n1jA7Bf0Iu2fYMzbOs3dUGMWqGud6aNpm3y5aiqA2KeNu5yA4cgkoZAXQTEJNqhv1Rldr9pXLZa73pQZqEKP+wO12G8vLy+wKHFbMFgBXZAiHw8hkMmN3JR31mOg6kUUZCAQQCAS2WA/lcplbw1gsFuTzeayurg5NxqZz8Pv9SCaTcLvdCAQChjy4ZrOJpaUlTuKlqvKHGTXs2O12G35DWtMT9g9zYBNx2O+T68Eo5zzudVGff03TuEv6brZ1ozlywgUYGzZSlAy9vtN3ABjKJlUqlS2Js+POdGhdaGVlBQDYFagWyVUhd2UoFEI+nzcI1zChGBfVxUXC5ff74fV6DZ/RdR2VSoWL4VosFhQKBayvrw/slKyG7VIRULvdzm4IOv5Wq4XV1VUuUOzxeA69O4iEC9isa+nz+bY91v36rQSjq/BmF7H9pNfr8XMcDoe3PP+HlSM1RTRHwqmuwu3WYwh6L5VKsSsvm80aShXt1lXYarXwyiuvANi0bLZb07HZbNyqe1Bi8H5hdhVGo1HEYrEtn8tkMlyCymKxYG1tDRcvXtziKlT/tlqtCIfDmJ+fRzKZRCwW49pqwKar8JVXXuFWMpOwxqUWVvb5fFzyCpA1ruvFoGfsZnQTArsPwNhue1R7FdgszCuuwgNAvdhU9Zzqd40iWjT4UKvsSCTCtfTUJOBx6Xa7aDQaKBQK8Pv9HPyx3Xob7Z9qBDabTZ4N7XUmP8jCCwQCbBUN+pzD4WArlOocqp8zD9zUSiUajaLX6xlauKjXIxAI8PU4rKjuZxJ5atBH567+uRkH1esFRfX2ej34fD4eaMm9ZbFYDvW9MwnYbDZ+/kctcn3QHCnhIjRNM7S3H/fHsNvt3Mer0Wig3W5zXb/dQOHjrVYLU1NT3G57u1m6x+OB3++H3W7n714P6FpREzpzWSbAeD3InWiu2zhooHY6nQgEAuh2u1wDkToI0znNzs5ygd7D7lrr9XrodDoAwP3YVCjPS9O2JoQK40P5mK1Wi4WL7p12uw2LxcJ1Nw/zfXOY0TSNC/i6XK6Bz/9h5MgJF11waicfi8XYUhl1Jkxll44fP45arYZGo2FIoB0VEqZGo8Gh5DMzM5iamhoYhaauO1EhW2oOSXlW+z240xrTwsKCocmmeq2i0SgWFhbQarV43ZAqf2x3LC6XC6FQiAv40nZrtRq3jKHO1JPgaqOeZFS+ytz5mCwBcikKe4faALXbbe72S13EaXK601qjMByr1YpAIICFhYUtXSUOM0dWuDweD2ZmZhAMBkeu6k7fJeE6ceIEfvjDH+5auGib9Xp9oHBt97B5vV5EIhHEYjF+WK8H1AH12LFjXFHafPzRaBTHjh3j6iNUiHhYpQ56jTqrknDROau9zubn51m4DuvgQ8fW6XS47JUqTjShIItM+nPtHZrIlMtlbGxsoNFo4J577kG9XkehUEA2m4Xdbke328X09PTEDLiHDRKuY8eOTZTlemSFi5JaqeS/+t4o2/B6vUgmk2g2m1uqZ4yCakFQa21g03oJh8M7ftflcnGzOCq2O2gtaa+QqyCRSHC1Dnqd9uf3+7l+oq7rfLPvNFjY7XbuYaYGyJD7hyqI0ILwYafT6aDZbPJ5DLpW494nwlbUe5y6ElBLHbJ6C4UCnE4nXC7XRFjrhxWauCYSibEKNRw0R1a4XC4XEokE7Hb72JaSxWJBMBjE3NwcGo0G+9jHhQY4qjxhsViQSqUQj8eHWhg0AHo8Hg7QoEoVtE31PPeKxWJh1x+1NFC3r2kaQqEQJxSrrtidaiOqHVVVK6TRaHBTSkr2PswWF6FWyFeTkVVXIZWDEvaOrm92U8hmsyiXy1zFpVQqIZPJwO12w+12b5tPKGwPBVHNzMyIxXUYoIoM40Z4qW6uWCwGn89nqCy/2x/WZrMhHo9vCQvf7vh9Ph8SiQQn/l4PLBYL7HY7V64YhM/ng9Pp5OtJ39vpWtjtdh7c1ZwvssDi8Tg3+DysqFUx2u02D6AUsUqvNxoNZDIZdil3u12+d2S9a3yoZFs2m+VUjEwmg1wuh3w+j3w+D03TtkS4CuOh5lFOirUFHGHhArCnH4Jm1JFIxJAsvBtsNhs8Hg+mpqYMCdHbQZFpVFGCwtT3O9RaFaJh0LUg95jaOXnU7atuTnIhTk1NGayyww6F8QPGBORWq4VyuYyVlRUOz6biwW63W4RrF3Q6HdRqNZRKJS7GnE6nUSwWOUWl2WwiGo2OVVVGMDKpOXFHWrj2itVqxfT0NJd/GvfHVdfbfD4f5ubmDIVZt8vjov2HQiH+/kExaD1n1GsxqMqBw+FAIBDA/Py8ocL6YX942u026vU6dF3nkk8AOCftypUrcLvd3E49EAhsqYovbA/dLxQIk8/nWbiWl5eRzWZRq9XQbDaRz+dRLBZFuPbApN6TR164duv7Jovn5MmTiMViA9t97PR9egjdbjei0ShOnz5tKKmyEzabDYlEwpB8fL0sru1Qi9/ux77dbjfi8TjuuOMOQ8LzYYV+x1arhVKpxAErVGWkUqlgbW0NP/rRjxCPx7n/28zMDJf3Esaj1Wohl8thbW0NhUIB3W4XL730Eq+PNhoNrKyscG1NWeO6uTjywrUXqEo7JcjuFqrVt7CwsG3FDIJEj4JEaBZP7+03O22TognH3be50Z/Z4iJrZFIGHHJfARhqcVGytdVqRa1W44RlGVhHQ009KJfLKBQKnDu3tLQEYHP9q9VqIZvNolgsyhrXHpjUe1KEawjkHqMisXtZh7Hb7VwJg4RrlIeNKqx3Op0DXSfZD1feoEocJF7XI8z/etDtdjmtQQ3OaDQaKBaLWFtbg9frhdPp5K7RJFyAiNc40CSB3IQA+PpS/hZ1CJ+Ee0fYX0S4tkHTNITDYQ633c33gU2XH7XuML+3HRaLBR6P57oU2B2X/RhwVYuLah/u5/avN91ulzsFqMKVy+WQTqd5EKUmmRsbG0ilUgd5yBMHiXu73UaxWOTq5f1+HysrKzh+/Dg/R91u1zAxEG4eRLhGYK+Dqs1m29UC8qQELYzLpEbZqRGYtO6n6zpXz/f7/Zienka/30e5XMb6+jpKpdIBH/XkoE4OO50O6vU6F7qu1Wqo1+sIBoOYmZnhAs0ej2di7ydh94hw3QDUqLybiWGCS2tmkwoFqdB5FAoFlEol+Hw+xONxtFotWK1Wjoib5HM9KCj1wO/3s4u2Xq9zj7dut8tVcW7GZ+tm58j/4vvl4trLdqxW69izQtrfUWmToR7/JBahVRPQ6fhJuNLpNNbX1xEOh7GwsIBUKgWLxYKVlRUUCgXD94XtUYMzarUaotEoB0fVajWEQiEsLCwgFoshHA7D5/OJcN2EyC8+InsRjt0O1Oo+J1W4BonuboT8sKBaW6pwbWxsIBQKceNMq9WK1dVViXrbJZ1OB9VqlYXL4XCgWq3yNY7FYgiFQobC0HKdbx7EVXgD2KvoTKpoDWNSz4caSdrtdk5Ip+jBRqMBn8+HYDCIRqMBq9WKUqmEer0OYHLP+aCg3mbU3JACpLxeL0KhEPx+P4LBILxer1zbmxCxuG4AR8HVt59M0vUwBww0Gg24XC5DGD81G41EIgiFQlyjsFwuo9FoiCWwCzRN4868NpsNnU6HS4UFAgEEAgF2F45afkw4OojFJQhDUNe1qFJ5JpNBNBqFx+PhtRir1Qqv14tUKsU5fy6Xi1viiHCNjlrkOpFIIBQKodPpoFAoIJlMcgcD6iGlFn4Wbh5EuARhCKrg6LqOZrPJ0YNUf5CCCex2O0KhEAdt2O12tNttdLtdSTzeBVRdxePxsDs2FApxl16Px4NEImEopyXX+OZBXIWHHHkYDwYqV0Wio/aComABYHPdy2q1wuVyIRwOczM+cm91Oh2xuEZEvdedTicHX/R6PVSrVYTDYYNwJZNJrhcp3FyIcAnCEMw92KrVKjY2NthVCGzWzaP2JdPT0xy4Qa5CEq5JWtc7SFRXYTKZRDgcZlfh9PQ0uwqDwSCOHz+Oubk5ua43IeIqFIQR0HUdnU4HzWaTI92ATYtL0zQ4HA5DB2mr1Yper4derycW1y6gQAyLxcJFdb1eLwfFUOHqo1pdRtgeES5B2AEKe6faeFToVR00qYGk+tpua1zezKi96FSrttvtchNWijgct9WQcHQQV6EgDEF175EIWSwWhEIhbjMz7PPiHtw9uq5z1/B6vY5Wq8X98ciileaRNzciXIIwAHPjTKrQr2kafD4f7Hb7wO8Qg/qQCTujWlxOpxPNZhO9Xo+tLHViINbszYsIlyAMwSw4ZEV5PJ4tbipzEqxYBHvDarXC4XCg3W6zcKkV+UW0bm7ESSwIQzALUb/f5+RXCs4Y9nkZWHePruscqVmv19HpdNhVqFpcws2LCJcgmKAIwna7DZvNBqfTyd2MhwmXiNb+Qrlw9Xod7XZ7WxetcPMhrkJBMNHv99Fqtbh4LgAeQIHNHCPVVTiorYaI1+5R8+eazSa63S40TTOEw8u64c2NCJcg/D9owKRKDZlMBuVymZtF1mo1mfnfQHRdN7gKI5HIwGhO4eZDhEsQTPR6PZTLZaTTaeTzefT7fWQyGVSrVWiahmAwyAPodjN/sQp2h1puq1KpoNVqwWKxIJFIwOVyAZBre7Mja1yC8P+gqMFer4d6vY5iscgCVS6X0Ww2oWnaFlehcP2gslmapiEQCLClK+7Cmxt5+gTBBFUjz+fzcLlc6Pf7KJVKaDQavNYirsLrj9lVGAwGOShGROvmRoRLEP4f6hpXuVzG6uoqbDYbisUi1tbW0Gg04PV6ufQQ1SlUv2suzCvsHuqB1m63YbFYuEGnIMhdIAgK1DK+0Wggn8/D4/Egl8shm82i1+shFApt+Q7leAFbK24Iu0fXdbRaLc6fk07HAiHCJQgmer0eGo0GisUiC1cul0Ov14Pf7x9pG1ar9Tof5dFFtWLVkk8iWgIxdlTht7/9bbzjHe/A9PQ0NE3DX/3VXxnef//732+YdWqahocfftjwmXw+j/e+970IBAIIhUL4wAc+gGq1uqcTEYS9YnYVrq2tYXl5GcvLy1hZWUGn00EymdySt0UWF33fYrHAarVucR8Ko6EKV71eR7fbFWtLMDC2cNVqNbzmNa/BZz/72aGfefjhh7G2tsZ/vvzlLxvef+9734uzZ8/i6aefxje+8Q18+9vfxoc+9KHxj14QrgO9Xg+1Wg3ZbBYbGxv8p9/vIxwOD6xhaC6qS+ImorV7+v0+JyCTcIl4CcAuXIWPPPIIHnnkkW0/43Q6kUqlBr738ssv45vf/Ca+//3v4/Wvfz0A4A//8A/xtre9Db/3e7+H6enpcQ9JEPadbreLarUKp9OJQqGAarWKaDSKQCBgGDxV66Db7RpETAbavUFrXFS7cNB1F25OrksC8rPPPotEIoHbbrsNH/7wh5HL5fi9M2fOIBQKsWgBwIMPPgiLxYLvfe97A7fXarVQLpcNfwTheuJwOKDrOmq1GtbX19Hv9+H1epFIJLYMoBaLBf1+H5VKBd1uF71eD81mE3a73dCKQxgNdTJQq9UAAD6fb2BpLeHmZN/vhIcffhh/+qd/imeeeQb/5b/8Fzz33HN45JFHuJ9ROp1GIpEwfMdmsyESiSCdTg/c5lNPPYVgMMh/5ubm9vuwBYEHTIfDgWQyiXA4DKvVilwuh1gshqmpKRYusqqsVitsNhv6/T7W1tbQbDbRbDY5sIO6IovLcDwourNSqcButyMSiUhleIHZ96jCd7/73fzvu+++G/fccw9uueUWPPvss3jggQd2tc0nn3wSTzzxBP+/XC6LeAnXDZvNhlAoBK/Xi1KphEqlgkAggHA4jFAoZJj5U7sNXdeRz+fR6XTQ6XRQr9dht9vhcDjE4toluq6j0WjAZrMhGAyKxSUw1/1OOHHiBGKxGC5evAgASKVS2NjYMHym2+0in88PXRdzOp0IBAKGP4Kw35DA2Gw2xONxhEIh2Gw2FAoFRKNRJJNJxONxw8xftbg2NjbQarXQbDZRKpUMFhd9XtgZul79fh/VahU2mw3hcFgCXgTmugvX8vIycrkcpqamAAD3338/isUinn/+ef7Mt771LfT7fdx3333X+3AEYSiqq3B6ehqRSARWqxXZbBaJRAIzMzOYmpoyzPypb1S/38fy8jIajQaazSYnL7tcLrG4dgmV2rLb7YY0BBEuYWxXYbVaZesJABYXF/HCCy8gEokgEongU5/6FB599FGkUilcunQJv/7rv46TJ0/ioYceAgDccccdePjhh/HBD34Qn/vc59DpdPD444/j3e9+t0QUCocCq9UKr9eLUCgEn8+HYrGIUCiEYDAIr9dr+KzFYoHL5YLdbkelUkG1WkWtVkO73UYgEIDb7T6gszgaUFFjcRUKKmPfCT/4wQ/wute9Dq973esAAE888QRe97rX4ROf+ASsVit+/OMf41//63+NW2+9FR/4wAdw77334u/+7u8MfXS++MUv4vbbb8cDDzyAt73tbXjzm9+M//k//+f+nZUg7AKyjCwWCzweDyKRCMLhMLxeL6LRKEKhEDwez5aoQrKsKpUK8vk8yuUyut0uIpEIvF6vWFx7wOl0wufzIRQKyXUUmLEtrre85S3bmur/5//8nx23EYlE8KUvfWncXQvCDYFC3I8dO4Zut4tgMIj5+fktgRn02ampKaTTaXS7XZw/fx4bGxuw2Ww4efIkksmkDLi7xGq1Ym5uDvPz80ilUnIdBUZqFQrCAKhxYavVgsPhQDweh9frHShcoVAIkUgEq6urWF5eRrlchs1mw/T0NILBoOGzwuhomoZYLIZYLDawuLFw8yLCJQgmqOJFPB6HrutwOp2IxWLweDxbQuEBIBQKIRQK4dq1a0in0xzCnUqlDMIljA79BtFoFJFIBMFgUIRfYES4BEGBkoUtFgsWFhYQjUY5mtAsXPT5ZDKJVCqFH/zgB7hw4QJ0XYfH48HJkye3VNoQRodchbOzs+xylYhCARDhEoQtkND4fD7Y7XYOEBhWvsnv9yMcDsPv96NQKMBisXCyMkUViniND7kKw+EwPB4PvyYIIlyCMASHw8FVybergOFyueD3+zlcnkqYeTwe2O12GWx3AQXI0DV1OBwHfUjCIUKESxC2QS3pZBYg+r/L5UI4HMbJkyfhcDh4fctutx/EIR8ZrFYr4vE4fD6fNOYUDIhwCcI2kHABMIiXut7icrkQCoVw4sQJeDweWK1WxGIx2GzyeO0WsrhIuCT5WFCRJ0sQtkFtCjnoPV3XYbfb4fP5kEqluAQUVZYXN+HuoOseCATgcrlEuAQDIlyCsA1kcfX7ff6/+X2n0wmLxQKr1YpgMAir1YpAICCuwj1isVgQiUR4rVEQCBEuQdiBnawmm80Gm80Gp9PJg+ygZGVhPDRN45JZYrkKKiJcgjAC2w2c9B61OLFarRJMsE+I+AuDkLtCEEZklFm/zWaTwXYfkYRjYRBicQnCCIzqqpKAjP2FhEuuqaAiwiUII7DdwKkOrpSwLAjC9UOESxD2EXET7i9icQmDEOEShH1EBlhBuP7I9FAQ9oiI1fVDQuGFQYhwCcI+IIOrINw4RLgEQTi0iMUlDEKESxCEQ4uIljAIES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohDhEgRBECYKES5BEARhohhLuJ566im84Q1vgN/vRyKRwDvf+U6cO3fO8Jlms4nHHnsM0WgUPp8Pjz76KNbX1w2fuXbtGt7+9rfD4/EgkUjg137t19Dtdvd+NoIgCMKRZyzheu655/DYY4/hu9/9Lp5++ml0Oh289a1vRa1W48987GMfw9e//nV85StfwXPPPYfV1VW8613v4vd7vR7e/va3o91u4x/+4R/wJ3/yJ/jCF76AT3ziE/t3VoIgCMLRRd8DGxsbOgD9ueee03Vd14vFom632/WvfOUr/JmXX35ZB6CfOXNG13Vd/+u//mvdYrHo6XSaP/PHf/zHeiAQ0Fut1kj7LZVKOgC9VCrt5fAFQRCEA2Iv4/ie1rhKpRIAIBKJAACef/55dDodPPjgg/yZ22+/HfPz8zhz5gwA4MyZM7j77ruRTCb5Mw899BDK5TLOnj07cD+tVgvlctnwRxAEQbg52bVw9ft9fPSjH8Wb3vQm3HXXXQCAdDoNh8OBUChk+GwymUQ6nebPqKJF79N7g3jqqacQDAb5z9zc3G4PWxAEQZhwdi1cjz32GF588UX82Z/92X4ez0CefPJJlEol/rO0tHTd9ykIgiAcTmy7+dLjjz+Ob3zjG/j2t7+N2dlZfj2VSqHdbqNYLBqsrvX1daRSKf7MP/7jPxq2R1GH9BkzTqcTTqdzN4cqCIIgHDHGsrh0Xcfjjz+Or371q/jWt76F48ePG96/9957Ybfb8cwzz/Br586dw7Vr13D//fcDAO6//3785Cc/wcbGBn/m6aefRiAQwOnTp/dyLoIgCMJNwFgW12OPPYYvfelL+NrXvga/389rUsFgEG63G8FgEB/4wAfwxBNPIBKJIBAI4CMf+Qjuv/9+/PRP/zQA4K1vfStOnz6NX/zFX8RnPvMZpNNpfPzjH8djjz0mVpUgCIKwI5qu6/rIH9a0ga9//vOfx/vf/34AmwnIv/Irv4Ivf/nLaLVaeOihh/BHf/RHBjfg1atX8eEPfxjPPvssvF4v3ve+9+F3fud3YLONpqPlchnBYBClUgmBQGDUwxcEQRAOCXsZx8cSrsOCCJcgCMJks5dxXGoVCoIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBOFCJcgCIIwUYhwCYIgCBPFWML11FNP4Q1veAP8fj8SiQTe+c534ty5c4bPvOUtb4GmaYY/v/zLv2z4zLVr1/D2t78dHo8HiUQCv/Zrv4Zut7v3sxEEQRCOPLZxPvzcc8/hsccewxve8AZ0u1385m/+Jt761rfipZdegtfr5c998IMfxKc//Wn+v8fj4X/3ej28/e1vRyqVwj/8wz9gbW0N//bf/lvY7Xb85//8n/fhlARBEISjjKbrur7bL2cyGSQSCTz33HP4mZ/5GQCbFtdrX/ta/P7v//7A7/zN3/wN/tW/+ldYXV1FMpkEAHzuc5/Db/zGbyCTycDhcOy433K5jGAwiFKphEAgsNvDFwRBEA6IvYzje1rjKpVKAIBIJGJ4/Ytf/CJisRjuuusuPPnkk6jX6/zemTNncPfdd7NoAcBDDz2EcrmMs2fPDtxPq9VCuVw2/BEEQRBuTsZyFar0+3189KMfxZve9Cbcdddd/Pov/MIvYGFhAdPT0/jxj3+M3/iN38C5c+fwl3/5lwCAdDptEC0A/P90Oj1wX0899RQ+9alP7fZQBUEQhCPEroXrsccew4svvojvfOc7htc/9KEP8b/vvvtuTE1N4YEHHsClS5dwyy237GpfTz75JJ544gn+f7lcxtzc3O4OXBAEQZhoduUqfPzxx/GNb3wDf/u3f4vZ2dltP3vfffcBAC5evAgASKVSWF9fN3yG/p9KpQZuw+l0IhAIGP4IgiAINydjCZeu63j88cfx1a9+Fd/61rdw/PjxHb/zwgsvAACmpqYAAPfffz9+8pOfYGNjgz/z9NNPIxAI4PTp0+McjiAIgnATMpar8LHHHsOXvvQlfO1rX4Pf7+c1qWAwCLfbjUuXLuFLX/oS3va2tyEajeLHP/4xPvaxj+FnfuZncM899wAA3vrWt+L06dP4xV/8RXzmM59BOp3Gxz/+cTz22GNwOp37f4aCIAjCkWKscHhN0wa+/vnPfx7vf//7sbS0hH/zb/4NXnzxRdRqNczNzeHnfu7n8PGPf9zg3rt69So+/OEP49lnn4XX68X73vc+/M7v/A5sttF0VMLhBUEQJpu9jON7yuM6KES4BEEQJpu9jOO7jio8SEhrJZ9LEARhMqHxeze200QKV6VSAQAJiRcEQZhwKpUKgsHgWN+ZSFdhv9/HuXPncPr0aSwtLYm7cACU6ybXZzByfbZHrs/OyDXanp2uj67rqFQqmJ6ehsUyXmbWRFpcFosFMzMzACB5XTsg12d75Ppsj1yfnZFrtD3bXZ9xLS1C+nEJgiAIE4UIlyAIgjBRTKxwOZ1OfPKTn5Sk5SHI9dkeuT7bI9dnZ+Qabc/1vD4TGZwhCIIg3LxMrMUlCIIg3JyIcAmCIAgThQiXIAiCMFGIcAmCIAgTxUQK12c/+1kcO3YMLpcL9913H/7xH//xoA/pQPjt3/5taJpm+HP77bfz+81mE4899hii0Sh8Ph8effTRLU08jxrf/va38Y53vAPT09PQNA1/9Vd/ZXhf13V84hOfwNTUFNxuNx588EFcuHDB8Jl8Po/3vve9CAQCCIVC+MAHPoBqtXoDz+L6sdP1ef/737/lnnr44YcNnzmq1+epp57CG97wBvj9fiQSCbzzne/EuXPnDJ8Z5Zm6du0a3v72t8Pj8SCRSODXfu3X0O12b+SpXDdGuUZvectbttxDv/zLv2z4zF6v0cQJ15//+Z/jiSeewCc/+Un80z/9E17zmtfgoYceMjSmvJm48847sba2xn++853v8Hsf+9jH8PWvfx1f+cpX8Nxzz2F1dRXvete7DvBorz+1Wg2vec1r8NnPfnbg+5/5zGfwB3/wB/jc5z6H733ve/B6vXjooYfQbDb5M+9973tx9uxZPP300/jGN76Bb3/72/jQhz50o07hurLT9QGAhx9+2HBPffnLXza8f1Svz3PPPYfHHnsM3/3ud/H000+j0+ngrW99K2q1Gn9mp2eq1+vh7W9/O9rtNv7hH/4Bf/Inf4IvfOEL+MQnPnEQp7TvjHKNAOCDH/yg4R76zGc+w+/tyzXSJ4w3vvGN+mOPPcb/7/V6+vT0tP7UU08d4FEdDJ/85Cf117zmNQPfKxaLut1u17/yla/way+//LIOQD9z5swNOsKDBYD+1a9+lf/f7/f1VCql/+7v/i6/ViwWdafTqX/5y1/WdV3XX3rpJR2A/v3vf58/8zd/8ze6pmn6ysrKDTv2G4H5+ui6rr/vfe/Tf/Znf3bod26m67OxsaED0J977jld10d7pv76r/9at1gsejqd5s/88R//sR4IBPRWq3VjT+AGYL5Guq7r/9//9//p/+E//Ieh39mPazRRFle73cbzzz+PBx98kF+zWCx48MEHcebMmQM8soPjwoULmJ6exokTJ/De974X165dAwA8//zz6HQ6hmt1++23Y35+/qa9VouLi0in04ZrEgwGcd999/E1OXPmDEKhEF7/+tfzZx588EFYLBZ873vfu+HHfBA8++yzSCQSuO222/DhD38YuVyO37uZrk+pVAIARCIRAKM9U2fOnMHdd9+NZDLJn3nooYdQLpdx9uzZG3j0NwbzNSK++MUvIhaL4a677sKTTz6Jer3O7+3HNZqoIrvZbBa9Xs9wwgCQTCbxyiuvHNBRHRz33XcfvvCFL+C2227D2toaPvWpT+Gf/bN/hhdffBHpdBoOhwOhUMjwnWQyiXQ6fTAHfMDQeQ+6f+i9dDqNRCJheN9msyESidwU1+3hhx/Gu971Lhw/fhyXLl3Cb/7mb+KRRx7BmTNnYLVab5rr0+/38dGPfhRvetObcNdddwHASM9UOp0eeH/Re0eJQdcIAH7hF34BCwsLmJ6exo9//GP8xm/8Bs6dO4e//Mu/BLA/12iihEsw8sgjj/C/77nnHtx3331YWFjAX/zFX8Dtdh/gkQmTyrvf/W7+991334177rkHt9xyC5599lk88MADB3hkN5bHHnsML774omHNWDAy7Bqp65133303pqam8MADD+DSpUu45ZZb9mXfE+UqjMVisFqtW6J41tfXkUqlDuioDg+hUAi33norLl68iFQqhXa7jWKxaPjMzXyt6Ly3u39SqdSWQJ9ut4t8Pn9TXrcTJ04gFovh4sWLAG6O6/P444/jG9/4Bv72b/8Ws7Oz/Pooz1QqlRp4f9F7R4Vh12gQ9913HwAY7qG9XqOJEi6Hw4F7770XzzzzDL/W7/fxzDPP4P777z/AIzscVKtVXLp0CVNTU7j33ntht9sN1+rcuXO4du3aTXutjh8/jlQqZbgm5XIZ3/ve9/ia3H///SgWi3j++ef5M9/61rfQ7/f5AbyZWF5eRi6Xw9TUFICjfX10Xcfjjz+Or371q/jWt76F48ePG94f5Zm6//778ZOf/MQg7k8//TQCgQBOnz59Y07kOrLTNRrECy+8AACGe2jP12iXwSQHxp/92Z/pTqdT/8IXvqC/9NJL+oc+9CE9FAoZIlRuFn7lV35Ff/bZZ/XFxUX97//+7/UHH3xQj8Vi+sbGhq7ruv7Lv/zL+vz8vP6tb31L/8EPfqDff//9+v3333/AR319qVQq+g9/+EP9hz/8oQ5A/6//9b/qP/zhD/WrV6/quq7rv/M7v6OHQiH9a1/7mv7jH/9Y/9mf/Vn9+PHjeqPR4G08/PDD+ute9zr9e9/7nv6d73xHP3XqlP6e97znoE5pX9nu+lQqFf1Xf/VX9TNnzuiLi4v6//2//1f/qZ/6Kf3UqVN6s9nkbRzV6/PhD39YDwaD+rPPPquvra3xn3q9zp/Z6Znqdrv6XXfdpb/1rW/VX3jhBf2b3/ymHo/H9SeffPIgTmnf2ekaXbx4Uf/0pz+t/+AHP9AXFxf1r33ta/qJEyf0n/mZn+Ft7Mc1mjjh0nVd/8M//EN9fn5edzgc+hvf+Eb9u9/97kEf0oHw8z//8/rU1JTucDj0mZkZ/ed//uf1ixcv8vuNRkP/9//+3+vhcFj3eDz6z/3cz+lra2sHeMTXn7/927/VAWz58773vU/X9c2Q+N/6rd/Sk8mk7nQ69QceeEA/d+6cYRu5XE5/z3veo/t8Pj0QCOi/9Eu/pFcqlQM4m/1nu+tTr9f1t771rXo8Htftdru+sLCgf/CDH9wyKTyq12fQdQGgf/7zn+fPjPJMXblyRX/kkUd0t9utx/7/9u7QhoEYiKIgPG5dCa7DTVnXsDswNtjgoIAokb40U8BKXvLAAt93zTnrnPPn1/zGpx2ttWqMUa21uq6reu/1PE/tvd/mfLsj35oAECXqxgUAwgVAFOECIIpwARBFuACIIlwARBEuAKIIFwBRhAuAKMIFQBThAiCKcAEQ5QWHUFYdpnBJJQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.choice(range(train_images.shape[0]))\n",
    "image = train_images[idx]\n",
    "plt.imshow(image.numpy(), cmap=\"gray\")\n",
    "print(train_latex_texts[idx])\n",
    "print(input_labels[idx])\n",
    "print(output_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yqt7tRm5AAI5"
   },
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Accx5CSIGji3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = EMBEDDING_DIM\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "\n",
    "lstm_units = 256\n",
    "max_seq_len_1 = max(len(seq) for seq in latex_labels) - 1\n",
    "print(num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XW0usbBPGATy"
   },
   "outputs": [],
   "source": [
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = tf.raw_ops.Shape(input=images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = tf.image.extract_patches(images=images, sizes=[1,self.patch_size, self.patch_size,1], strides=[1,self.patch_size, self.patch_size,1], padding='VALID', rates=[1, 1, 1, 1])\n",
    "        new_shape = (batch_size, num_patches_h * num_patches_w, self.patch_size * self.patch_size * channels)\n",
    "        # print(f\"PATCHES: {patches.shape}\")\n",
    "        # print(f\"RESHAPE: {new_shape}\")\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            shape=new_shape,\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image.numpy(), cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# image = tf.expand_dims(image, axis = 0)\n",
    "\n",
    "# print(f\"Image size: {image.shape}\")\n",
    "# patches = Patches(patch_size)(image)\n",
    "# print(f\"Image size: {image.shape}\")\n",
    "# print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "# print(f\"Patches per image: {patches.shape[1]}\")\n",
    "# print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "# n = int(np.sqrt(patches.shape[1]))\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# for i, patch in enumerate(patches[0]):\n",
    "#     ax = plt.subplot(n, n, i + 1)\n",
    "#     patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "#     plt.imshow(patch_img.numpy(), cmap='gray')\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xnbps4-JHS4b"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = Dense(units, activation=tf.keras.activations.gelu)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DWhOTG_7GRpx"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.raw_ops.ExpandDims(\n",
    "           input = tf.experimental.numpy.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        #print(projected_patches.shape)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        #print(position_embeddings.shape)\n",
    "        encoded = projected_patches + position_embeddings\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NHZEjUHRGYcS"
   },
   "outputs": [],
   "source": [
    "def vision_transformer_encoder(input_shape):\n",
    "    inputs =  Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    # representation = Flatten()(representation)\n",
    "    # representation = Dropout(0.2)(representation)\n",
    "    # Add MLP.\n",
    "    # features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.2)\n",
    "    # Classify outputs.\n",
    "    #logits = tf.keras.layers.Dense(2)(features)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=representation)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "T8jXN2GlGyNC"
   },
   "outputs": [],
   "source": [
    "vit = vision_transformer_encoder(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_1htX_wlLj4W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " patches (Patches)           (None, 1764, 36)             0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncode  (None, 1764, 256)            461056    ['patches[0][0]']             \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 1764, 256)            512       ['patch_encoder[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 1764, 256)            1051904   ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1764, 256)            0         ['multi_head_attention[0][0]',\n",
      "                                                                     'patch_encoder[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 1764, 256)            512       ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1764, 512)            0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1764, 256)            131328    ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 1764, 256)            0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1764, 256)            0         ['dropout_1[0][0]',           \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 1764, 256)            512       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 1764, 256)            512       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 1764, 512)            0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1764, 256)            131328    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 1764, 256)            0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1764, 256)            0         ['dropout_3[0][0]',           \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 1764, 256)            512       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 1764, 256)            512       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 1764, 512)            0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 1764, 256)            131328    ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 1764, 256)            0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 1764, 256)            0         ['dropout_5[0][0]',           \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 1764, 256)            512       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 1764, 256)            512       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 1764, 512)            0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1764, 256)            131328    ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 1764, 256)            0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 1764, 256)            0         ['dropout_7[0][0]',           \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 1764, 256)            512       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_8[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_7[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 1764, 256)            512       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 1764, 512)            0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1764, 256)            131328    ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 1764, 256)            0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 1764, 256)            0         ['dropout_9[0][0]',           \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 1764, 256)            512       ['add_9[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_10[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 1764, 256)            0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 1764, 256)            512       ['add_10[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1764, 512)            131584    ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 1764, 512)            0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 1764, 256)            131328    ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 1764, 256)            0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 1764, 256)            0         ['dropout_11[0][0]',          \n",
      "                                                                     'add_10[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 1764, 256)            512       ['add_11[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_12[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 1764, 256)            0         ['multi_head_attention_6[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_11[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 1764, 256)            512       ['add_12[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 1764, 512)            131584    ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 1764, 512)            0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 1764, 256)            131328    ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 1764, 256)            0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 1764, 256)            0         ['dropout_13[0][0]',          \n",
      "                                                                     'add_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 1764, 256)            512       ['add_13[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_14[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 1764, 256)            0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_13[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 1764, 256)            512       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 1764, 512)            131584    ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 1764, 512)            0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 1764, 256)            131328    ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 1764, 256)            0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 1764, 256)            0         ['dropout_15[0][0]',          \n",
      "                                                                     'add_14[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 1764, 256)            512       ['add_15[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10988288 (41.92 MB)\n",
      "Trainable params: 10988288 (41.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "\n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "\n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "    \n",
    "    image = self.feature_extractor(image)\n",
    "    #print(image.shape)\n",
    "    # Flatten the feature map\n",
    "    \n",
    "    #image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "    \n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "    \n",
    "    txt = self.seq_embedding(txt)\n",
    "    \n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "    \n",
    "    txt = self.output_layer(txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import tqdm\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]'), **kwargs):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in ds:\n",
    "      counts.update(tokens.flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniform entropy: 5.62\n",
      "Marginal entropy: 3.87\n"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = Captioner(tokenizer, feature_extractor=vision_transformer_encoder(IMG_SHAPE), output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=4, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=0):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  #img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((image[tf.newaxis, ...], tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = self.index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:48:01.688340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ } } - - } { } { } { } 2 = { } { } { } = { } { } 2 { { } 2 } { } } { } } } { } 2 } { } } { } = ^\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)\n",
    "result = transformer_model.simple_gen(image, temperature=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.image = image\n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    result = self.model.simple_gen(self.image, temperature=0)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{ } } - - } { } { } { } 2 = { } { } { } = { } { } 2 { { } 2 } { } } { } } } { } 2 } { } } { } = ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GenerateText()\n",
    "g.model = transformer_model\n",
    "g.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'transformer_model'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',  # Save the model with the best validation loss\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_format='tf'\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    model_checkpoint_callback,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True, monitor='val_masked_acc')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received incompatible tensor with shape (413,) when attempting to restore variable with shape (277,) and name output_layer/dense/bias/.ATTRIBUTES/VARIABLE_VALUE.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_filepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/training/saving/saveable_object_util.py:140\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    137\u001b[0m   assigned_variable \u001b[38;5;241m=\u001b[39m resource_variable_ops\u001b[38;5;241m.\u001b[39mshape_safe_assign_variable_handle(\n\u001b[1;32m    138\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_op, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape, restored_tensor)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived incompatible tensor with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrestored_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen attempting to restore variable with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m assigned_variable\n",
      "\u001b[0;31mValueError\u001b[0m: Received incompatible tensor with shape (413,) when attempting to restore variable with shape (277,) and name output_layer/dense/bias/.ATTRIBUTES/VARIABLE_VALUE."
     ]
    }
   ],
   "source": [
    "transformer_model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000000\n"
     ]
    }
   ],
   "source": [
    "print(int(100000 / 4 * 1000))\n",
    "learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(1e-4, 4000, 1e-6)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=0.0001)\n",
    "\n",
    "transformer_model.compile(optimizer=optimizer,\n",
    "           loss=masked_loss,\n",
    "           metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"captioner\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_1 (Functional)        (None, 1764, 256)         10988288  \n",
      "                                                                 \n",
      " text_vectorization (TextVe  multiple                  0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " string_lookup_1 (StringLoo  multiple                  0         \n",
      " kup)                                                            \n",
      "                                                                 \n",
      " string_lookup_2 (StringLoo  multiple                  0         \n",
      " kup)                                                            \n",
      "                                                                 \n",
      " seq_embedding (SeqEmbeddin  multiple                  83712     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " decoder_layer (DecoderLaye  multiple                  4471552   \n",
      " r)                                                              \n",
      "                                                                 \n",
      " decoder_layer_1 (DecoderLa  multiple                  4471552   \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " decoder_layer_2 (DecoderLa  multiple                  4471552   \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " decoder_layer_3 (DecoderLa  multiple                  4471552   \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " token_output (TokenOutput)  multiple                  71189     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29029397 (110.74 MB)\n",
      "Trainable params: 29029397 (110.74 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "int64\n",
      "[[ 13 180  65 ...   0   0   0]\n",
      " [ 13   7   3 ...   0   0   0]\n",
      " [ 13  73   4 ...   0   0   0]\n",
      " ...\n",
      " [ 13 111   3 ...   0   0   0]\n",
      " [ 13  64   4 ...   0   0   0]\n",
      " [ 13 157   3 ...   0   0   0]]\n",
      "[[180  65   4 ...   0   0   0]\n",
      " [  7   3  62 ...   0   0   0]\n",
      " [ 73   4   3 ...   0   0   0]\n",
      " ...\n",
      " [111   3  31 ...   0   0   0]\n",
      " [ 64   4   3 ...   0   0   0]\n",
      " [157   3  79 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images))\n",
    "print(len(train_sequences))\n",
    "print(train_sequences.dtype)\n",
    "print(input_labels)\n",
    "print(output_labels)\n",
    "# print(train_sequences)\n",
    "# print(train_sequences[..., :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(([train_images, input_labels], output_labels))\n",
    "# dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 02:50:12.680968: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7588004460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-30 02:50:12.681020: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2024-11-30 02:50:12.687185: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-30 02:50:12.845031: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 4.1239 - masked_acc: 0.1878\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } { } } } } } } } } } { } } } } } } } } } } } } } } } } } } } } }\n",
      "\n",
      "200/200 [==============================] - 614s 3s/step - loss: 4.1239 - masked_acc: 0.1878 - val_loss: 3.9752 - val_masked_acc: 0.2213\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.9941 - masked_acc: 0.2032\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } { } } } } { { } } } { } } } } } } } } } } } } } } } } } } } } }\n",
      "\n",
      "200/200 [==============================] - 542s 3s/step - loss: 3.9941 - masked_acc: 0.2032 - val_loss: 3.8987 - val_masked_acc: 0.2302\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.9360 - masked_acc: 0.2159\n",
      "\n",
      "{ } } { { } } } } } } } { } { } } { } } } } { { } } } { } } } } } } } } } } } } } } } } } } } } }\n",
      "\n",
      "200/200 [==============================] - 538s 3s/step - loss: 3.9360 - masked_acc: 0.2159 - val_loss: 3.8526 - val_masked_acc: 0.2399\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8955 - masked_acc: 0.2249\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } { } } } } { } } } } { } } } } } } } } } } } } } } } } } { } } }\n",
      "\n",
      "200/200 [==============================] - 541s 3s/step - loss: 3.8955 - masked_acc: 0.2249 - val_loss: 3.8179 - val_masked_acc: 0.2465\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8715 - masked_acc: 0.2304\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } } } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 537s 3s/step - loss: 3.8715 - masked_acc: 0.2304 - val_loss: 3.7860 - val_masked_acc: 0.2512\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8410 - masked_acc: 0.2353\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } } } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 542s 3s/step - loss: 3.8410 - masked_acc: 0.2353 - val_loss: 3.7581 - val_masked_acc: 0.2546\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.8116 - masked_acc: 0.2415\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } } } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 537s 3s/step - loss: 3.8116 - masked_acc: 0.2415 - val_loss: 3.7316 - val_masked_acc: 0.2584\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7873 - masked_acc: 0.2457\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } } } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 541s 3s/step - loss: 3.7873 - masked_acc: 0.2457 - val_loss: 3.7053 - val_masked_acc: 0.2620\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7644 - masked_acc: 0.2521\n",
      "\n",
      "{ } } { { } } { } } } } { } { } } } } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 538s 3s/step - loss: 3.7644 - masked_acc: 0.2521 - val_loss: 3.6800 - val_masked_acc: 0.2651\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7451 - masked_acc: 0.2534\n",
      "\n",
      "{ } } { { } } { } } } } } } { } } { } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 541s 3s/step - loss: 3.7451 - masked_acc: 0.2534 - val_loss: 3.6553 - val_masked_acc: 0.2691\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7185 - masked_acc: 0.2587\n",
      "\n",
      "{ } { } { } } { } } } } } } { } } { } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 538s 3s/step - loss: 3.7185 - masked_acc: 0.2587 - val_loss: 3.6317 - val_masked_acc: 0.2709\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7011 - masked_acc: 0.2613\n",
      "\n",
      "{ } } { { } } { } } } } } } { } } { } } } } { { } } } { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 542s 3s/step - loss: 3.7011 - masked_acc: 0.2613 - val_loss: 3.6091 - val_masked_acc: 0.2755\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.6787 - masked_acc: 0.2641\n",
      "\n",
      "{ } { } } } { { } } } } } } { } } { } { { } } } } } { { } } } } } } } } } } } } } } } } } { } { {\n",
      "\n",
      "200/200 [==============================] - 538s 3s/step - loss: 3.6787 - masked_acc: 0.2641 - val_loss: 3.5886 - val_masked_acc: 0.2767\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.6643 - masked_acc: 0.2667\n",
      "\n",
      "{ } { } } } { { } } } } } } { } } { } { { } } } } } { { } } } } } } } } } } } } } } } } } { - { {\n",
      "\n",
      "200/200 [==============================] - 542s 3s/step - loss: 3.6643 - masked_acc: 0.2667 - val_loss: 3.5683 - val_masked_acc: 0.2831\n",
      "Epoch 15/100\n",
      "166/200 [=======================>......] - ETA: 1:19 - loss: 3.6483 - masked_acc: 0.2707"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = transformer_model.fit([train_images, input_labels],\n",
    "                  output_labels,\n",
    "                  epochs=100,\n",
    "                  batch_size=4,\n",
    "                  validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSQExklGAAI7"
   },
   "outputs": [],
   "source": [
    "def build_cnn_encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(EMBEDDING_DIM, activation='relu')(x)\n",
    "    return Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "md8Xz7JzAAI7"
   },
   "outputs": [],
   "source": [
    "def build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1):\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=target_vocab_size, output_dim=EMBEDDING_DIM, input_length=max_seq_len_1)\n",
    "    embedded_seq = embedding_layer(decoder_input)\n",
    "\n",
    "    decoder_lstm_input = tf.keras.layers.Concatenate(axis=-1)([encoder_output, embedded_seq])\n",
    "    decoder_lstm = LSTM(lstm_units, return_sequences=True)(decoder_lstm_input)\n",
    "    decoder_output = TimeDistributed(Dense(target_vocab_size, activation=\"softmax\"))(decoder_lstm)\n",
    "\n",
    "    return Model(inputs=[decoder_input, encoder_output], outputs= decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "xWJwvgriAAI7"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_layers, d_model, num_heads, dff, target_vocab_size, max_seq_len_1):\n",
    "    #encoder = build_cnn_encoder(input_shape)\n",
    "    encoder = vision_transformer_encoder(input_shape)\n",
    "    image_input = Input(shape=input_shape, name=\"image_input\")\n",
    "\n",
    "    encoder_output = encoder(image_input)\n",
    "    encoder_output = RepeatVector(max_seq_len_1)(encoder_output)  # Repeat encoder output for each time step\n",
    "\n",
    "    decoder_input = Input(shape=(max_seq_len_1,), name=\"decoder_input\")\n",
    "    decoder = build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1)\n",
    "\n",
    "    decoder_output = decoder([decoder_input, encoder_output])\n",
    "    return Model(inputs=[image_input, decoder_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "DeU4cNTsAAI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 256)\n",
      "(1, 1764, 256)\n",
      "(None, None, 256)\n",
      "(1, 1764, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"repeat_vector_5\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (None, 1764, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SHAPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#transformer_model = Transformer(tokenizer, output_layer=output_layer, units=128, dropout_rate=0.5, num_layers=2, num_heads=2)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[193], line 7\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(input_shape, num_layers, d_model, num_heads, dff, target_vocab_size, max_seq_len_1)\u001b[0m\n\u001b[1;32m      4\u001b[0m image_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39minput_shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m encoder(image_input)\n\u001b[0;32m----> 7\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mRepeatVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_seq_len_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Repeat encoder output for each time step\u001b[39;00m\n\u001b[1;32m      9\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_seq_len_1,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m decoder \u001b[38;5;241m=\u001b[39m build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"repeat_vector_5\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (None, 1764, 256)"
     ]
    }
   ],
   "source": [
    "model = build_model(IMG_SHAPE, 2, 256, 2, 256, tokenizer.vocabulary_size(), max_seq_len_1)\n",
    "#transformer_model = Transformer(tokenizer, output_layer=output_layer, units=128, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ba5W_OZFAAI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, 50, 200, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " model_15 (Functional)       (None, 512)                  1380070   ['image_input[0][0]']         \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)  [(None, 91)]                 0         []                            \n",
      "                                                                                                  \n",
      " repeat_vector_4 (RepeatVec  (None, 91, 512)              0         ['model_15[0][0]']            \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " model_16 (Functional)       (None, 91, 385)              1247105   ['decoder_input[0][0]',       \n",
      "                                                                     'repeat_vector_4[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15047809 (57.40 MB)\n",
      "Trainable params: 15047809 (57.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "eDbtc_4eAAI8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "FZZrLb4DAAI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 91)\n",
      "(50000, 91)\n",
      "(50000, 92)\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[..., :-1].shape)\n",
    "print(train_sequences[..., 1:].shape)\n",
    "print(train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "lzmbXT3WAAI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "625/625 [==============================] - 611s 935ms/step - loss: 1.6553 - accuracy: 0.7445 - val_loss: 1.6281 - val_accuracy: 0.7468\n",
      "Epoch 2/100\n",
      "625/625 [==============================] - 575s 920ms/step - loss: 1.6359 - accuracy: 0.7457 - val_loss: 1.6268 - val_accuracy: 0.7468\n",
      "Epoch 3/100\n",
      "625/625 [==============================] - 573s 917ms/step - loss: 1.6351 - accuracy: 0.7457 - val_loss: 1.6265 - val_accuracy: 0.7468\n",
      "Epoch 4/100\n",
      "625/625 [==============================] - 573s 917ms/step - loss: 1.6345 - accuracy: 0.7457 - val_loss: 1.6269 - val_accuracy: 0.7468\n",
      "Epoch 5/100\n",
      "625/625 [==============================] - 572s 916ms/step - loss: 1.6337 - accuracy: 0.7457 - val_loss: 1.6250 - val_accuracy: 0.7468\n",
      "Epoch 6/100\n",
      "625/625 [==============================] - 573s 917ms/step - loss: 1.6411 - accuracy: 0.7457 - val_loss: 1.6317 - val_accuracy: 0.7468\n",
      "Epoch 7/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6391 - accuracy: 0.7457 - val_loss: 1.6297 - val_accuracy: 0.7468\n",
      "Epoch 8/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6367 - accuracy: 0.7457 - val_loss: 1.6270 - val_accuracy: 0.7468\n",
      "Epoch 9/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6348 - accuracy: 0.7457 - val_loss: 1.6258 - val_accuracy: 0.7468\n",
      "Epoch 10/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6333 - accuracy: 0.7457 - val_loss: 1.6238 - val_accuracy: 0.7468\n",
      "Epoch 11/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6316 - accuracy: 0.7457 - val_loss: 1.6225 - val_accuracy: 0.7468\n",
      "Epoch 12/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6303 - accuracy: 0.7457 - val_loss: 1.6213 - val_accuracy: 0.7468\n",
      "Epoch 13/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6293 - accuracy: 0.7457 - val_loss: 1.6207 - val_accuracy: 0.7468\n",
      "Epoch 14/100\n",
      "625/625 [==============================] - 571s 915ms/step - loss: 1.6282 - accuracy: 0.7457 - val_loss: 1.6199 - val_accuracy: 0.7468\n",
      "Epoch 15/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6273 - accuracy: 0.7457 - val_loss: 1.6182 - val_accuracy: 0.7468\n",
      "Epoch 16/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6268 - accuracy: 0.7457 - val_loss: 1.6180 - val_accuracy: 0.7468\n",
      "Epoch 17/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6262 - accuracy: 0.7457 - val_loss: 1.6174 - val_accuracy: 0.7468\n",
      "Epoch 18/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6255 - accuracy: 0.7457 - val_loss: 1.6174 - val_accuracy: 0.7468\n",
      "Epoch 19/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6252 - accuracy: 0.7457 - val_loss: 1.6163 - val_accuracy: 0.7468\n",
      "Epoch 20/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6403 - accuracy: 0.7453 - val_loss: 1.6250 - val_accuracy: 0.7468\n",
      "Epoch 21/100\n",
      "108/625 [====>.........................] - ETA: 7:17 - loss: 1.6285 - accuracy: 0.7468"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit([train_images, train_sequences[..., :-1]],\n",
    "              train_sequences[..., 1:],\n",
    "              epochs=100,\n",
    "              batch_size=64,\n",
    "              validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGz_H92VAAI8"
   },
   "outputs": [],
   "source": [
    "transformer_model.save('/home/ubuntu/model_av.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jdeua_LzAAI9"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePe5hVCKG0df"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit([train_images, train_sequences[:, :-1]],\n",
    "              train_sequences[:, 1:],\n",
    "              epochs=20,\n",
    "              batch_size=128,\n",
    "              validation_split=0.2)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('/home/ubuntu/latex_model.keras')\n",
    "\n",
    "#model = load_model('latex_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_u6gxZzWVRk"
   },
   "outputs": [],
   "source": [
    "#dot_img_file =\n",
    "import keras\n",
    "keras.utils.plot_model(model,\n",
    "                       show_shapes=True,\n",
    "                       show_dtype=True,\n",
    "                       show_layer_names=True,\n",
    "                       expand_nested=True,\n",
    "                       show_layer_activations=True,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_WwX-x7YWRb"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.utils.plot_model(model,\n",
    "                       show_shapes=True,\n",
    "                       show_dtype=True,\n",
    "                       show_layer_names=True,\n",
    "                       expand_nested=True,\n",
    "                       show_layer_activations=True,\n",
    "                       to_file='/Users/jayaprakash/latex_model.png'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixV3kbyDPr8X"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjCZXuZ_Jn-x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_latex_sequence(model, image, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict LaTeX sequence from a single image.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained Keras model for predicting LaTeX sequence.\n",
    "    - image: Input image (preprocessed to match training dimensions).\n",
    "    - tokenizer: Tokenizer fitted on LaTeX sequences for decoding predictions.\n",
    "    - max_seq_len: Maximum sequence length for the predicted sequence.\n",
    "\n",
    "    Returns:\n",
    "    - latex_sequence: Predicted LaTeX sequence as a string.\n",
    "    \"\"\"\n",
    "    # Prepare input image and initialize the sequence\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    start_token = tokenizer.word_index[\"<START>\"]\n",
    "    end_token = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "    # Initial sequence with the start token\n",
    "    sequence = [start_token]\n",
    "\n",
    "    for _ in range(max_seq_len_1):\n",
    "        # Pad the current sequence to match input length\n",
    "        padded_sequence = np.pad(sequence, (0, max_seq_len_1 - len(sequence)), mode='constant')\n",
    "        padded_sequence = np.expand_dims(padded_sequence, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict next token\n",
    "        preds = model.predict([image, padded_sequence])\n",
    "        next_token = np.argmax(preds[0, len(sequence) - 1, :])\n",
    "\n",
    "        # Break if end token is reached\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "\n",
    "        # Add the predicted token to the sequence\n",
    "        sequence.append(next_token)\n",
    "\n",
    "    # Decode the token sequence to a string\n",
    "    latex_sequence = tokenizer.sequences_to_texts([sequence[1:]])[0]  # Skip the start token\n",
    "    return latex_sequence\n",
    "\n",
    "predicted_latex = predict_latex_sequence(model, train_images[12], tokenizer)\n",
    "print(\"Predicted LaTeX:\", predicted_latex)\n",
    "#print(\"Original Seq:\", train_sequences[0])\n",
    "print(\"Original Seq:\", train_latex_texts[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIlv7RgvQZ9y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9MKLOc1REDl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
