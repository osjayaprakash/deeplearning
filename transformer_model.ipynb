{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osjayaprakash/deeplearning/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZgd8NVaFjfO"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "root_dir = kagglehub.dataset_download(\"shahrukhkhan/im2latex100k\")\n",
        "# path = kagglehub.dataset_download(\"gregoryeritsyan/im2latex-230k\")\n",
        "\n",
        "print(\"Path to dataset files:\", root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TKz3RA-FuOp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten,\n",
        "                                     Dense, GRU, Embedding, Bidirectional,\n",
        "                                     TimeDistributed, Concatenate, RepeatVector, LSTM, MultiHeadAttention, LayerNormalization, Add, Dropout )\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "import sys\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import scipy as sp\n",
        "import einops\n",
        "\n",
        "tf.config.experimental.list_physical_devices('GPU')\n",
        "print(f\"Python Platform: {platform.platform()}\")\n",
        "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
        "#print(f\"Keras Version: {tf.keras.__version__}\")\n",
        "print()\n",
        "print(f\"Python {sys.version}\")\n",
        "print(f\"Pandas {pd.__version__}\")\n",
        "print(f\"Scikit-Learn {sk.__version__}\")\n",
        "print(f\"SciPy {sp.__version__}\")\n",
        "print(tf.config.list_physical_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo6QNApGGgIk"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAMBW0OFF_VN"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    \"\"\"Preprocess the input image: Resize and normalize.\"\"\"\n",
        "    image = tf.image.resize(image, (50, 200))  # Resize to (50, 200)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_images(image_paths):\n",
        "    \"\"\"Load and preprocess a batch of images.\"\"\"\n",
        "    # Use Gray scale\n",
        "    images = [preprocess_image(tf.io.decode_image(tf.io.read_file(path), channels=1))\n",
        "              for path in image_paths]\n",
        "    return tf.stack(images)\n",
        "\n",
        "def prepare_sequences(latex_texts, max_seq_length):\n",
        "    \"\"\"Convert LaTeX texts to padded sequences of tokens.\"\"\"\n",
        "    sequences = [text_to_sequence(text) for text in latex_texts]\n",
        "    return pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NIjhtisIi1W"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# %%prun\n",
        "\n",
        "df = pd.read_csv(f\"{root_dir}/im2latex_train.csv\", nrows=5000)\n",
        "\n",
        "train_image_paths = []\n",
        "train_latex_texts = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    train_image_paths += [f\"{root_dir}//formula_images_processed/formula_images_processed/{row.image}\"]\n",
        "    train_latex_texts += [\"<START> \" + row.formula + \" <END>\"]\n",
        "\n",
        "train_images = load_and_preprocess_images(train_image_paths)\n",
        "# Enable Numpy behaviour of TF\n",
        "# tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
        "\n",
        "# vocab_size, max_seq_length = fit_tokenizer(train_latex_texts)\n",
        "\n",
        "# train_sequences = prepare_sequences(train_latex_texts, max_seq_length)\n",
        "# #train_sequences = np.expand_dims(train_sequences, -1)\n",
        "# print(\"train_images:\", train_images.shape)\n",
        "# print(\"train_sequences:\", train_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjYzQHDSAAI1"
      },
      "outputs": [],
      "source": [
        "vocabulary_size = 5000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpXdTtsaAAI1"
      },
      "outputs": [],
      "source": [
        "tokenizer.adapt(train_latex_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqmVc3a1AAI2"
      },
      "outputs": [],
      "source": [
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJgjaXn2AAI3"
      },
      "outputs": [],
      "source": [
        "latex_labels = tokenizer(train_latex_texts)\n",
        "train_sequences = np.asarray(latex_labels).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az_S8OFzIvc1"
      },
      "outputs": [],
      "source": [
        "print(latex_labels.shape)\n",
        "print(train_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqt7tRm5AAI5"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
        "patch_size = 4  # Size of the patches to be extract from the input images\n",
        "num_patches = 600\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [\n",
        "    2048,\n",
        "    1024,\n",
        "]  # Size\n",
        "\n",
        "IMG_SHAPE = (50, 200, 1)\n",
        "EMBEDDING_DIM = 256\n",
        "lstm_units = 256\n",
        "max_seq_len_1 = max(len(seq) for seq in latex_labels) - 1"
      ],
      "metadata": {
        "id": "Accx5CSIGji3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        input_shape = tf.keras.ops.shape(images)\n",
        "        batch_size = input_shape[0]\n",
        "        height = input_shape[1]\n",
        "        width = input_shape[2]\n",
        "        channels = input_shape[3]\n",
        "        num_patches_h = height // self.patch_size\n",
        "        num_patches_w = width // self.patch_size\n",
        "        patches = tf.keras.ops.image.extract_patches(images, size=self.patch_size)\n",
        "        patches = tf.keras.ops.reshape(\n",
        "            patches,\n",
        "            (\n",
        "                batch_size,\n",
        "                num_patches_h * num_patches_w,\n",
        "                self.patch_size * self.patch_size * channels,\n",
        "            ),\n",
        "        )\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config"
      ],
      "metadata": {
        "id": "XW0usbBPGATy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = Dense(units=projection_dim)\n",
        "        self.position_embedding = Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.keras.ops.expand_dims(\n",
        "            tf.keras.ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
        "        )\n",
        "        projected_patches = self.projection(patch)\n",
        "        encoded = projected_patches + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches})\n",
        "        return config"
      ],
      "metadata": {
        "id": "DWhOTG_7GRpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = Dense(units, activation=tf.keras.activations.gelu)(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xnbps4-JHS4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vision_transformer_encoder(input_shape):\n",
        "    inputs =  Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = Flatten()(representation)\n",
        "    representation = Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    #logits = tf.keras.layers.Dense(2)(features)\n",
        "    # Create the Keras model.\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=features)\n",
        "    return model"
      ],
      "metadata": {
        "id": "NHZEjUHRGYcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = vision_transformer_encoder(IMG_SHAPE)"
      ],
      "metadata": {
        "id": "T8jXN2GlGyNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.summary()"
      ],
      "metadata": {
        "id": "_1htX_wlLj4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oljShrHtAAI5"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dropout_rate)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs3tmEiHAAI6"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsljpmfIAAI6"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-h0-IiEAAI7"
      },
      "outputs": [],
      "source": [
        "sample_decoder = TransformerDecoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_m3Z88RAAI7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSQExklGAAI7"
      },
      "outputs": [],
      "source": [
        "def build_cnn_encoder(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(EMBEDDING_DIM, activation='relu')(x)\n",
        "    return Model(inputs, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md8Xz7JzAAI7"
      },
      "outputs": [],
      "source": [
        "def build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1):\n",
        "\n",
        "    embedding_layer = Embedding(input_dim=target_vocab_size, output_dim=EMBEDDING_DIM, input_length=max_seq_len_1)\n",
        "    embedded_seq = embedding_layer(decoder_input)\n",
        "\n",
        "    decoder_lstm_input = tf.keras.layers.Concatenate(axis=-1)([encoder_output, embedded_seq])\n",
        "    decoder_lstm = LSTM(lstm_units, return_sequences=True)(decoder_lstm_input)\n",
        "    decoder_output = TimeDistributed(Dense(target_vocab_size, activation=\"softmax\"))(decoder_lstm)\n",
        "\n",
        "    return Model(inputs=[decoder_input, encoder_output], outputs= decoder_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWJwvgriAAI7"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, num_layers, d_model, num_heads, dff, target_vocab_size, max_seq_len_1):\n",
        "    #encoder = build_cnn_encoder(input_shape)\n",
        "    encoder = vision_transformer_encoder(input_shape)\n",
        "    image_input = Input(shape=input_shape, name=\"image_input\")\n",
        "\n",
        "    encoder_output = encoder(image_input)\n",
        "    encoder_output = RepeatVector(max_seq_len_1)(encoder_output)  # Repeat encoder output for each time step\n",
        "\n",
        "    decoder_input = Input(shape=(max_seq_len_1,), name=\"decoder_input\")\n",
        "    decoder = build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1)\n",
        "\n",
        "    decoder_output = decoder([decoder_input, encoder_output])\n",
        "    return Model(inputs=[image_input, decoder_input], outputs=decoder_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeU4cNTsAAI7"
      },
      "outputs": [],
      "source": [
        "model = build_model(IMG_SHAPE, 2, 256, 2, 256, tokenizer.vocabulary_size(), max_seq_len_1)\n",
        "#transformer_model = Transformer(tokenizer, output_layer=output_layer, units=128, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba5W_OZFAAI8"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDbtc_4eAAI8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZZrLb4DAAI8"
      },
      "outputs": [],
      "source": [
        "print(train_sequences[..., :-1].shape)\n",
        "print(train_sequences[..., 1:].shape)\n",
        "print(train_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzmbXT3WAAI8"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    model.fit([train_images, train_sequences[..., :-1]],\n",
        "              train_sequences[..., 1:],\n",
        "              epochs=20,\n",
        "              batch_size=128,\n",
        "              validation_split=0.2)\n",
        "\n",
        "from tensorflow.keras.models import load_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGz_H92VAAI8"
      },
      "outputs": [],
      "source": [
        "transformer_model.save('/home/ubuntu/model_av.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdeua_LzAAI9"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePe5hVCKG0df"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    model.fit([train_images, train_sequences[:, :-1]],\n",
        "              train_sequences[:, 1:],\n",
        "              epochs=20,\n",
        "              batch_size=128,\n",
        "              validation_split=0.2)\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "model.save('/home/ubuntu/latex_model.keras')\n",
        "\n",
        "#model = load_model('latex_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_u6gxZzWVRk"
      },
      "outputs": [],
      "source": [
        "#dot_img_file =\n",
        "import keras\n",
        "keras.utils.plot_model(model,\n",
        "                       show_shapes=True,\n",
        "                       show_dtype=True,\n",
        "                       show_layer_names=True,\n",
        "                       expand_nested=True,\n",
        "                       show_layer_activations=True,\n",
        "                       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_WwX-x7YWRb"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "keras.utils.plot_model(model,\n",
        "                       show_shapes=True,\n",
        "                       show_dtype=True,\n",
        "                       show_layer_names=True,\n",
        "                       expand_nested=True,\n",
        "                       show_layer_activations=True,\n",
        "                       to_file='/Users/jayaprakash/latex_model.png'\n",
        "                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixV3kbyDPr8X"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjCZXuZ_Jn-x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_latex_sequence(model, image, tokenizer):\n",
        "    \"\"\"\n",
        "    Predict LaTeX sequence from a single image.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained Keras model for predicting LaTeX sequence.\n",
        "    - image: Input image (preprocessed to match training dimensions).\n",
        "    - tokenizer: Tokenizer fitted on LaTeX sequences for decoding predictions.\n",
        "    - max_seq_len: Maximum sequence length for the predicted sequence.\n",
        "\n",
        "    Returns:\n",
        "    - latex_sequence: Predicted LaTeX sequence as a string.\n",
        "    \"\"\"\n",
        "    # Prepare input image and initialize the sequence\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    start_token = tokenizer.word_index[\"<START>\"]\n",
        "    end_token = tokenizer.word_index[\"<END>\"]\n",
        "\n",
        "    # Initial sequence with the start token\n",
        "    sequence = [start_token]\n",
        "\n",
        "    for _ in range(max_seq_len_1):\n",
        "        # Pad the current sequence to match input length\n",
        "        padded_sequence = np.pad(sequence, (0, max_seq_len_1 - len(sequence)), mode='constant')\n",
        "        padded_sequence = np.expand_dims(padded_sequence, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Predict next token\n",
        "        preds = model.predict([image, padded_sequence])\n",
        "        next_token = np.argmax(preds[0, len(sequence) - 1, :])\n",
        "\n",
        "        # Break if end token is reached\n",
        "        if next_token == end_token:\n",
        "            break\n",
        "\n",
        "        # Add the predicted token to the sequence\n",
        "        sequence.append(next_token)\n",
        "\n",
        "    # Decode the token sequence to a string\n",
        "    latex_sequence = tokenizer.sequences_to_texts([sequence[1:]])[0]  # Skip the start token\n",
        "    return latex_sequence\n",
        "\n",
        "predicted_latex = predict_latex_sequence(model, train_images[12], tokenizer)\n",
        "print(\"Predicted LaTeX:\", predicted_latex)\n",
        "#print(\"Original Seq:\", train_sequences[0])\n",
        "print(\"Original Seq:\", train_latex_texts[12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIlv7RgvQZ9y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9MKLOc1REDl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}