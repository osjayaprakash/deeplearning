{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/osjayaprakash/deeplearning/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xZgd8NVaFjfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/shahrukhkhan/im2latex100k/versions/7\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "root_dir = kagglehub.dataset_download(\"shahrukhkhan/im2latex100k\")\n",
    "# path = kagglehub.dataset_download(\"gregoryeritsyan/im2latex-230k\")\n",
    "\n",
    "print(\"Path to dataset files:\", root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6TKz3RA-FuOp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:14:19.154508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 20:14:20.206142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Linux-5.15.0-1071-aws-x86_64-with-glibc2.29\n",
      "Tensor Flow Version: 2.13.1\n",
      "\n",
      "Python 3.8.10 (default, Nov  7 2024, 13:10:47) \n",
      "[GCC 9.4.0]\n",
      "Pandas 2.0.3\n",
      "Scikit-Learn 1.3.2\n",
      "SciPy 1.10.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:14:22.001896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:22.040924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:22.041794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten,\n",
    "                                     Dense, GRU, Embedding, Bidirectional,\n",
    "                                     TimeDistributed, Concatenate, RepeatVector, LSTM, MultiHeadAttention, LayerNormalization, Add, Dropout )\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "from tensorflow.python.ops.numpy_ops import np_config  \n",
    "import einops\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "#print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo6QNApGGgIk"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256  # We'll resize input images to this size\n",
    "IMG_SHAPE = (256, 256, 1)\n",
    "EMBEDDING_DIM = 256\n",
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wAMBW0OFF_VN"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess the input image: Resize and normalize.\"\"\"\n",
    "    image = tf.image.resize(image, (image_size, image_size))  # Resize to (50, 200)\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    \"\"\"Load and preprocess a batch of images.\"\"\"\n",
    "    # Use Gray scale\n",
    "    images = [preprocess_image(tf.io.decode_image(tf.io.read_file(path), channels=1))\n",
    "              for path in image_paths]\n",
    "    return tf.stack(images)\n",
    "\n",
    "def prepare_sequences(latex_texts, max_seq_length):\n",
    "    \"\"\"Convert LaTeX texts to padded sequences of tokens.\"\"\"\n",
    "    sequences = [text_to_sequence(text) for text in latex_texts]\n",
    "    return pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4NIjhtisIi1W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:14:27.788854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:27.789752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:27.790598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:28.460306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:28.461119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:28.461885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-29 20:14:28.462665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10786 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# %%prun\n",
    "\n",
    "df = pd.read_csv(f\"{root_dir}/im2latex_train.csv\", nrows=n)\n",
    "\n",
    "train_image_paths = []\n",
    "train_latex_texts = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    train_image_paths += [f\"{root_dir}//formula_images_processed/formula_images_processed/{row.image}\"]\n",
    "    train_latex_texts += [\"[START] \" + row.formula + \" [END]\"]\n",
    "\n",
    "train_images = load_and_preprocess_images(train_image_paths)\n",
    "# Enable Numpy behaviour of TF\n",
    "# tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
    "\n",
    "# vocab_size, max_seq_length = fit_tokenizer(train_latex_texts)\n",
    "\n",
    "# train_sequences = prepare_sequences(train_latex_texts, max_seq_length)\n",
    "# #train_sequences = np.expand_dims(train_sequences, -1)\n",
    "# print(\"train_images:\", train_images.shape)\n",
    "# print(\"train_sequences:\", train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GjYzQHDSAAI1"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, standardize = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WpXdTtsaAAI1"
   },
   "outputs": [],
   "source": [
    "tokenizer.adapt(train_latex_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WJgjaXn2AAI3"
   },
   "outputs": [],
   "source": [
    "latex_labels = tokenizer(train_latex_texts)\n",
    "train_sequences = np.asarray(latex_labels)\n",
    "input_labels = train_sequences[..., :-1]\n",
    "output_labels = train_sequences[..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "az_S8OFzIvc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 152)\n",
      "(10000, 256, 256, 1)\n",
      "{'': 0, '[UNK]': 1, '}': 2, '{': 3, '_': 4, '^': 5, '2': 6, '(': 7, ')': 8, '=': 9, '1': 10, '-': 11, ',': 12, '[START]': 13, '[END]': 14, '\\\\frac': 15, '+': 16, 'i': 17, '0': 18, 'x': 19, 'n': 20, '.': 21, 'a': 22, 'd': 23, '\\\\,': 24, '\\\\mu': 25, 'e': 26, 'k': 27, 'm': 28, 'c': 29, '\\\\partial': 30, 't': 31, 'r': 32, 'p': 33, '\\\\alpha': 34, 'A': 35, '\\\\;': 36, 's': 37, '~': 38, '3': 39, 'j': 40, 'l': 41, '\\\\right)': 42, '\\\\left(': 43, 'g': 44, '4': 45, '\\\\': 46, '\\\\nu': 47, '\\\\prime': 48, '\\\\pi': 49, 'b': 50, '\\\\phi': 51, 'z': 52, '|': 53, '\\\\mathrm': 54, '\\\\delta': 55, 'f': 56, '\\\\cal': 57, 'N': 58, 'q': 59, 'T': 60, '\\\\beta': 61, 'S': 62, 'R': 63, '\\\\lambda': 64, ']': 65, '\\\\int': 66, '[': 67, 'M': 68, 'B': 69, '\\\\bar': 70, 'L': 71, '\\\\operatorname': 72, 'D': 73, '\\\\theta': 74, 'F': 75, 'y': 76, '\\\\sigma': 77, '&': 78, 'h': 79, '\\\\psi': 80, '\\\\\\\\': 81, '\\\\hat': 82, '\\\\gamma': 83, '\\\\sum': 84, '/': 85, '\\\\sqrt': 86, 'u': 87, 'H': 88, '\\\\tilde': 89, '\\\\rho': 90, '\\\\tau': 91, 'C': 92, 'o': 93, 'P': 94, 'G': 95, '\\\\omega': 96, 'I': 97, 'V': 98, 'E': 99, '\\\\epsilon': 100, '\\\\Phi': 101, '\\\\xi': 102, 'X': 103, '\\\\bf': 104, 'J': 105, '\\\\eta': 106, '\\\\quad': 107, '\\\\vec': 108, 'Q': 109, 'v': 110, 'K': 111, '\\\\infty': 112, '\\\\Gamma': 113, '\\\\pm': 114, '5': 115, '\\\\right]': 116, '\\\\left[': 117, '\\\\dot': 118, 'U': 119, '\\\\varphi': 120, 'Z': 121, '\\\\Delta': 122, '*': 123, '\\\\end{array}': 124, '\\\\begin{array}': 125, 'W': 126, '\\\\rangle': 127, '6': 128, '\\\\Lambda': 129, '\\\\Omega': 130, 'w': 131, ';': 132, '\\\\Psi': 133, '\\\\chi': 134, '\\\\qquad': 135, '\\\\}': 136, '\\\\{': 137, '\\\\kappa': 138, '\\\\cdot': 139, '\\\\equiv': 140, '8': 141, '\\\\overline': 142, '>': 143, '\\\\langle': 144, '\\\\!': 145, '\\\\dagger': 146, '\\\\rightarrow': 147, '\\\\zeta': 148, 'Y': 149, '<': 150, '\\\\varepsilon': 151, '\\\\nabla': 152, '\\\\Sigma': 153, '\\\\ell': 154, '\\\\cdots': 155, 'O': 156, ':': 157, '\\\\mathcal': 158, '\\\\ldots': 159, '\\\\left\\\\{': 160, '\\\\vert': 161, '\\\\operatorname*': 162, '\\\\:': 163, '\\\\sim': 164, '\\\\otimes': 165, '!': 166, '\\\\hbar': 167, '\\\\wedge': 168, '\\\\hspace': 169, '7': 170, '\\\\Pi': 171, '\\\\prod': 172, '\\\\to': 173, '\\\\right\\\\}': 174, '\\\\right|': 175, '\\\\in': 176, '9': 177, '\\\\widetilde': 178, '\\\\times': 179, '\\\\left|': 180, '\\\\underline': 181, '\\\\Big': 182, '\\\\mid': 183, '\\\\dots': 184, '\\\\approx': 185, '\\\\leq': 186, '\\\\Theta': 187, '\\\\ast': 188, '\\\\perp': 189, '\\\\stackrel': 190, '\\\\displaystyle': 191, '\\\\left.': 192, '\\\\right\\\\rangle': 193, '\\\\mathbf': 194, '\\\\right.': 195, '\\\\star': 196, '\\\\widehat': 197, '\\\\geq': 198, '\\\\Bigr': 199, '\\\\bigg': 200, '\\\\Bigl': 201, '\\\\mp': 202, '\\\\vartheta': 203, '\\\\big': 204, '\\\\left\\\\langle': 205, \"'\": 206, '\\\\oint': 207, '\\\\dag': 208, '\\\\circ': 209, '\\\\simeq': 210, '\\\\ddot': 211, '\\\\longrightarrow': 212, '\\\\biggr': 213, '\\\\biggl': 214, '\\\\textstyle': 215, '\\\\neq': 216, '\\\\imath': 217, '\\\\boldmath': 218, '\\\\nonumber': 219, '\\\\Xi': 220, '\\\\propto': 221, '\\\\right>': 222, '--': 223, '\\\\bigr': 224, '\\\\oplus': 225, '\\\\triangle': 226, '\\\\bigl': 227, '\\\\varrho': 228, '\\\\le': 229, '\\\\check': 230, '\\\\lbrack': 231, '\\\\textrm': 232, '\\\\sp': 233, '\\\\ge': 234, '\\\\it': 235, '\\\\not': 236, '\\\\|': 237, '\\\\mapsto': 238, '\\\\forall': 239, '\\\\Rightarrow': 240, '\\\\leftrightarrow': 241, '\\\\parallel': 242, '\\\\overrightarrow': 243, '\\\\subset': 244, '\\\\phantom': 245, '\\\\l': 246, '\\\\hline': 247, '\\\\bot': 248, '\\\\rbrack': 249, '\\\\ne': 250, '\\\\Upsilon': 251, '\\\\Bigg': 252, '\\\\slash': 253, '\\\\gg': 254, '\\\\cong': 255, '\\\\breve': 256, '\\\\tt': 257, '\\\\rightharpoonup': 258, '\\\\kern': 259, '\\\\binom': 260, '\\\\Im': 261, '\\\\varsigma': 262, '\\\\small': 263, '\\\\scriptsize': 264, '\\\\ll': 265, '\\\\atop': 266, '\\\\supset': 267, '\\\\protect': 268, '\\\\jmath': 269, '\\\\bigoplus': 270, '\\\\#': 271, '\\\\vee': 272, '\\\\left\\\\vert': 273, '\\\\iota': 274, '\\\\bullet': 275, '\\\\L': 276, '\\\\Biggr': 277, '\\\\vspace': 278, '\\\\tiny': 279, '\\\\scriptscriptstyle': 280, '\\\\d': 281, '\\\\varpi': 282, '\\\\mathsf': 283, '\\\\left<': 284, '\\\\Biggl': 285, '\\\\wp': 286, '\\\\sb': 287, '\\\\mit': 288, '\\\\lbrace': 289, '\\\\sf': 290, '\\\\scriptstyle': 291, '\\\\rbrace': 292, '\\\\llap': 293, '\\\\bigotimes': 294, '\\\\Leftrightarrow': 295, '\\\\right\\\\vert': 296, '\\\\i': 297, '\\\\cap': 298, '\\\\Re': 299, '\\\\right\\\\|': 300, '\\\\mathop': 301, '\\\\left\\\\|': 302, '\\\\doteq': 303, '\\\\Longrightarrow': 304, '\\\\rfloor': 305, '\\\\overleftarrow': 306, '\\\\o': 307, '\\\\leftarrow': 308, '\\\\hfill': 309, '\\\\bigtriangleup': 310, '\\\\Vert': 311, '\\\\vdots': 312, '\\\\underbrace': 313, '\\\\textup': 314, '\\\\subseteq': 315, '\\\\longleftrightarrow': 316, '\\\\cup': 317, '\\\\cdotp': 318, '\\\\acute': 319, '\\\\_': 320, '`': 321, '\\\\vphantom': 322, '\\\\space': 323, '\\\\sharp': 324, '\\\\right\\\\rfloor': 325, '\\\\footnotesize': 326, '\\\\c': 327, '\\\\Large': 328, 'ule': 329, '\\\\thinspace': 330, '\\\\textbf': 331, '\\\\raisebox': 332, '\\\\raise': 333, '\\\\ni': 334, '\\\\mathit': 335, '\\\\lfloor': 336, '\\\\label': 337, '\\\\flat': 338, '\\\\downarrow': 339, '\\\\ddots': 340, '\\\\colon': 341, '\\\\bigtriangledown': 342, '\\\\backslash': 343, '\\\\O': 344, '\\\\Longleftrightarrow': 345, '\"': 346, 'pt': 347, 'mm': 348, 'cm': 349, '\\\\vdash': 350, '\\\\upsilon': 351, '\\\\unitlength': 352, '\\\\smallskip': 353, '\\\\setminus': 354, '\\\\setlength': 355, '\\\\rightleftharpoons': 356, '\\\\relax': 357, '\\\\ref': 358, '\\\\put': 359, '\\\\pounds': 360, '\\\\overbrace': 361, '\\\\odot': 362, '\\\\makebox': 363, '\\\\left\\\\lfloor': 364, '\\\\large': 365, '\\\\land': 366, '\\\\enspace': 367, '\\\\emptyset': 368, '\\\\do': 369, '\\\\diamondsuit': 370, '\\\\diamond': 371, '\\\\circle': 372, '\\\\buildrel': 373, '\\\\bmod': 374, '\\\\bigcup': 375, '\\\\aleph': 376, '\\\\Huge': 377, \"\\\\'\": 378, '\\\\uparrow': 379, '\\\\thicklines': 380, '\\\\textsf': 381, '\\\\textit': 382, '\\\\succeq': 383, '\\\\sl': 384, '\\\\right\\\\rbrace': 385, '\\\\pmod': 386, '\\\\mathbin': 387, '\\\\mathaccent': 388, '\\\\longmapsto': 389, '\\\\lefteqn': 390, '\\\\left\\\\lbrace': 391, '\\\\left/': 392, '\\\\lceil': 393, '\\\\j': 394, '\\\\hphantom': 395, '\\\\grave': 396, '\\\\fbox': 397, '\\\\em': 398, '\\\\b': 399, '\\\\^': 400, '\\\\S': 401, '\\\\P': 402, '\\\\/': 403, '\\\\-': 404, '\\\\&': 405, '[object': 406, 'Object]': 407, '8.5': 408, '20': 409, '0.9': 410, '0.4': 411, '0.14': 412}\n"
     ]
    }
   ],
   "source": [
    "print(latex_labels.shape)\n",
    "print(train_images.shape)\n",
    "vocab_dict = {name: id for id, name in enumerate(tokenizer.get_vocabulary())}\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] \\psi ^ { \\dagger } \\nabla _ { i } \\nabla _ { j } \\psi [END]\n",
      "[ 13  80   5   3 146   2 152   4   3  17   2 152   4   3  40   2  80  14\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[ 80   5   3 146   2 152   4   3  17   2 152   4   3  40   2  80  14   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.choice(range(train_images.shape[0]))\n",
    "image = train_images[idx]\n",
    "plt.imshow(image.numpy(), cmap=\"gray\")\n",
    "print(train_latex_texts[idx])\n",
    "print(input_labels[idx])\n",
    "print(output_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yqt7tRm5AAI5"
   },
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Accx5CSIGji3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = EMBEDDING_DIM\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "\n",
    "lstm_units = 256\n",
    "max_seq_len_1 = max(len(seq) for seq in latex_labels) - 1\n",
    "print(num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XW0usbBPGATy"
   },
   "outputs": [],
   "source": [
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = tf.raw_ops.Shape(input=images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = tf.image.extract_patches(images=images, sizes=[1,self.patch_size, self.patch_size,1], strides=[1,self.patch_size, self.patch_size,1], padding='VALID', rates=[1, 1, 1, 1])\n",
    "        new_shape = (batch_size, num_patches_h * num_patches_w, self.patch_size * self.patch_size * channels)\n",
    "        # print(f\"PATCHES: {patches.shape}\")\n",
    "        # print(f\"RESHAPE: {new_shape}\")\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            shape=new_shape,\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image.numpy(), cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# image = tf.expand_dims(image, axis = 0)\n",
    "\n",
    "# print(f\"Image size: {image.shape}\")\n",
    "# patches = Patches(patch_size)(image)\n",
    "# print(f\"Image size: {image.shape}\")\n",
    "# print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "# print(f\"Patches per image: {patches.shape[1]}\")\n",
    "# print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "# n = int(np.sqrt(patches.shape[1]))\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# for i, patch in enumerate(patches[0]):\n",
    "#     ax = plt.subplot(n, n, i + 1)\n",
    "#     patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "#     plt.imshow(patch_img.numpy(), cmap='gray')\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xnbps4-JHS4b"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = Dense(units, activation=tf.keras.activations.gelu)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DWhOTG_7GRpx"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.raw_ops.ExpandDims(\n",
    "           input = tf.experimental.numpy.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        #print(projected_patches.shape)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        #print(position_embeddings.shape)\n",
    "        encoded = projected_patches + position_embeddings\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NHZEjUHRGYcS"
   },
   "outputs": [],
   "source": [
    "def vision_transformer_encoder(input_shape):\n",
    "    inputs =  Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    # representation = Flatten()(representation)\n",
    "    # representation = Dropout(0.2)(representation)\n",
    "    # Add MLP.\n",
    "    # features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.2)\n",
    "    # Classify outputs.\n",
    "    #logits = tf.keras.layers.Dense(2)(features)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=representation)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "T8jXN2GlGyNC"
   },
   "outputs": [],
   "source": [
    "vit = vision_transformer_encoder(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_1htX_wlLj4W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " patches (Patches)           (None, 1764, 36)             0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncode  (None, 1764, 256)            461056    ['patches[0][0]']             \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 1764, 256)            512       ['patch_encoder[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 1764, 256)            1051904   ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1764, 256)            0         ['multi_head_attention[0][0]',\n",
      "                                                                     'patch_encoder[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 1764, 256)            512       ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1764, 512)            0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1764, 256)            131328    ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 1764, 256)            0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1764, 256)            0         ['dropout_1[0][0]',           \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 1764, 256)            512       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 1764, 256)            512       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 1764, 512)            0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1764, 256)            131328    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 1764, 256)            0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1764, 256)            0         ['dropout_3[0][0]',           \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 1764, 256)            512       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 1764, 256)            512       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 1764, 512)            0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 1764, 256)            131328    ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 1764, 256)            0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 1764, 256)            0         ['dropout_5[0][0]',           \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 1764, 256)            512       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 1764, 256)            512       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 1764, 512)            0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1764, 256)            131328    ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 1764, 256)            0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 1764, 256)            0         ['dropout_7[0][0]',           \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 1764, 256)            512       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_8[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 1764, 256)            0         ['multi_head_attention_4[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_7[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 1764, 256)            512       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 1764, 512)            131584    ['layer_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 1764, 512)            0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1764, 256)            131328    ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 1764, 256)            0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 1764, 256)            0         ['dropout_9[0][0]',           \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 1764, 256)            512       ['add_9[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_10[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 1764, 256)            0         ['multi_head_attention_5[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 1764, 256)            512       ['add_10[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1764, 512)            131584    ['layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 1764, 512)            0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 1764, 256)            131328    ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 1764, 256)            0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 1764, 256)            0         ['dropout_11[0][0]',          \n",
      "                                                                     'add_10[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 1764, 256)            512       ['add_11[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_12[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 1764, 256)            0         ['multi_head_attention_6[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_11[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 1764, 256)            512       ['add_12[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 1764, 512)            131584    ['layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 1764, 512)            0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 1764, 256)            131328    ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 1764, 256)            0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 1764, 256)            0         ['dropout_13[0][0]',          \n",
      "                                                                     'add_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 1764, 256)            512       ['add_13[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 1764, 256)            1051904   ['layer_normalization_14[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 1764, 256)            0         ['multi_head_attention_7[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_13[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 1764, 256)            512       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 1764, 512)            131584    ['layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 1764, 512)            0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 1764, 256)            131328    ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 1764, 256)            0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 1764, 256)            0         ['dropout_15[0][0]',          \n",
      "                                                                     'add_14[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 1764, 256)            512       ['add_15[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10988288 (41.92 MB)\n",
      "Trainable params: 10988288 (41.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "\n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "\n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "    \n",
    "    image = self.feature_extractor(image)\n",
    "    #print(image.shape)\n",
    "    # Flatten the feature map\n",
    "    \n",
    "    #image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "    \n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "    \n",
    "    txt = self.seq_embedding(txt)\n",
    "    \n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "    \n",
    "    txt = self.output_layer(txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import tqdm\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]'), **kwargs):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in ds:\n",
    "      counts.update(tokens.flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniform entropy: 6.02\n",
      "Marginal entropy: 3.86\n"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = Captioner(tokenizer, feature_extractor=vision_transformer_encoder(IMG_SHAPE), output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=0):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  #img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((image[tf.newaxis, ...], tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = self.index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:15:15.672291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ { { { { { { ^ { { { { { { { { { { { { i { { { { { { { ) { { { { _ { { { { { { { { { { { { { { {\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)\n",
    "result = transformer_model.simple_gen(image, temperature=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.image = image\n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    result = self.model.simple_gen(self.image, temperature=0)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "^ { { { { { { ^ { { { { { { { { { { { { i { { { { { { { ) { { { { _ { { { { { { { { { { { { { { {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GenerateText()\n",
    "g.model = transformer_model\n",
    "g.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_filepath = 'transformer_model'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',  # Save the model with the best validation loss\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_format='tf'\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    model_checkpoint_callback,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True, monitor='val_masked_acc')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fd9b46fb580>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "           loss=masked_loss,\n",
    "           metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"captioner\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_1 (Functional)        (None, 1764, 256)         10988288  \n",
      "                                                                 \n",
      " text_vectorization (TextVe  multiple                  0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " string_lookup_1 (StringLoo  multiple                  0         \n",
      " kup)                                                            \n",
      "                                                                 \n",
      " string_lookup_2 (StringLoo  multiple                  0         \n",
      " kup)                                                            \n",
      "                                                                 \n",
      " seq_embedding (SeqEmbeddin  multiple                  118528    \n",
      " g)                                                              \n",
      "                                                                 \n",
      " decoder_layer (DecoderLaye  multiple                  1316608   \n",
      " r)                                                              \n",
      "                                                                 \n",
      " decoder_layer_1 (DecoderLa  multiple                  1316608   \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " token_output (TokenOutput)  multiple                  106141    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13846173 (52.82 MB)\n",
      "Trainable params: 13846173 (52.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "int64\n",
      "[[ 13 178  83 ...   0   0   0]\n",
      " [ 13   7   3 ...   0   0   0]\n",
      " [ 13  62   4 ...   0   0   0]\n",
      " ...\n",
      " [ 13  54   3 ...   0   0   0]\n",
      " [ 13  71   9 ...   0   0   0]\n",
      " [ 13  22   4 ...   0   0   0]]\n",
      "[[178  83   4 ...   0   0   0]\n",
      " [  7   3  57 ...   0   0   0]\n",
      " [ 62   4   3 ...   0   0   0]\n",
      " ...\n",
      " [ 54   3  23 ...   0   0   0]\n",
      " [ 71   9  75 ...   0   0   0]\n",
      " [ 22   4   3 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images))\n",
    "print(len(train_sequences))\n",
    "print(train_sequences.dtype)\n",
    "print(input_labels)\n",
    "print(output_labels)\n",
    "# print(train_sequences)\n",
    "# print(train_sequences[..., :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:18:13.146280: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1cd05a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-29 20:18:13.146326: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2024-11-29 20:18:13.152612: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-29 20:18:13.305876: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 0s - loss: 2.1026 - masked_acc: 0.4954\n",
      "\n",
      "{ \\cal L } _ { \\mu \\nu } = \\partial _ { \\mu } \\partial _ { \\mu }\n",
      "\n",
      "2000/2000 [==============================] - 3164s 2s/step - loss: 2.1026 - masked_acc: 0.4954 - val_loss: 2.1099 - val_masked_acc: 0.5078\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 2.0863 - masked_acc: 0.4974\n",
      "\n",
      "{ \\cal L } _ { \\mu \\nu } = \\partial _ { \\mu } \\partial _ { \\mu }\n",
      "\n",
      "2000/2000 [==============================] - 3124s 2s/step - loss: 2.0863 - masked_acc: 0.4974 - val_loss: 2.1029 - val_masked_acc: 0.5081\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 2.0702 - masked_acc: 0.5001\n",
      "\n",
      "{ \\cal L } = - \\nabla _ { \\mu } \\nabla _ { \\mu } \\nabla _ { \\mu }\n",
      "\n",
      "2000/2000 [==============================] - 3155s 2s/step - loss: 2.0702 - masked_acc: 0.5001 - val_loss: 2.1029 - val_masked_acc: 0.5091\n",
      "Epoch 4/100\n",
      "1910/2000 [===========================>..] - ETA: 2:09 - loss: 2.0507 - masked_acc: 0.5025"
     ]
    }
   ],
   "source": [
    "history = transformer_model.fit([train_images, input_labels],\n",
    "              output_labels,\n",
    "              epochs=100,\n",
    "              batch_size=4,\n",
    "              validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSQExklGAAI7"
   },
   "outputs": [],
   "source": [
    "def build_cnn_encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(EMBEDDING_DIM, activation='relu')(x)\n",
    "    return Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "md8Xz7JzAAI7"
   },
   "outputs": [],
   "source": [
    "def build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1):\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=target_vocab_size, output_dim=EMBEDDING_DIM, input_length=max_seq_len_1)\n",
    "    embedded_seq = embedding_layer(decoder_input)\n",
    "\n",
    "    decoder_lstm_input = tf.keras.layers.Concatenate(axis=-1)([encoder_output, embedded_seq])\n",
    "    decoder_lstm = LSTM(lstm_units, return_sequences=True)(decoder_lstm_input)\n",
    "    decoder_output = TimeDistributed(Dense(target_vocab_size, activation=\"softmax\"))(decoder_lstm)\n",
    "\n",
    "    return Model(inputs=[decoder_input, encoder_output], outputs= decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "xWJwvgriAAI7"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_layers, d_model, num_heads, dff, target_vocab_size, max_seq_len_1):\n",
    "    #encoder = build_cnn_encoder(input_shape)\n",
    "    encoder = vision_transformer_encoder(input_shape)\n",
    "    image_input = Input(shape=input_shape, name=\"image_input\")\n",
    "\n",
    "    encoder_output = encoder(image_input)\n",
    "    encoder_output = RepeatVector(max_seq_len_1)(encoder_output)  # Repeat encoder output for each time step\n",
    "\n",
    "    decoder_input = Input(shape=(max_seq_len_1,), name=\"decoder_input\")\n",
    "    decoder = build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1)\n",
    "\n",
    "    decoder_output = decoder([decoder_input, encoder_output])\n",
    "    return Model(inputs=[image_input, decoder_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "DeU4cNTsAAI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 256)\n",
      "(1, 1764, 256)\n",
      "(None, None, 256)\n",
      "(1, 1764, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"repeat_vector_5\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (None, 1764, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SHAPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#transformer_model = Transformer(tokenizer, output_layer=output_layer, units=128, dropout_rate=0.5, num_layers=2, num_heads=2)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[193], line 7\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(input_shape, num_layers, d_model, num_heads, dff, target_vocab_size, max_seq_len_1)\u001b[0m\n\u001b[1;32m      4\u001b[0m image_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39minput_shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m encoder(image_input)\n\u001b[0;32m----> 7\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mRepeatVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_seq_len_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Repeat encoder output for each time step\u001b[39;00m\n\u001b[1;32m      9\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_seq_len_1,), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m decoder \u001b[38;5;241m=\u001b[39m build_rnn_encoder(decoder_input, encoder_output, target_vocab_size, max_seq_len_1)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"repeat_vector_5\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (None, 1764, 256)"
     ]
    }
   ],
   "source": [
    "model = build_model(IMG_SHAPE, 2, 256, 2, 256, tokenizer.vocabulary_size(), max_seq_len_1)\n",
    "#transformer_model = Transformer(tokenizer, output_layer=output_layer, units=128, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ba5W_OZFAAI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, 50, 200, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " model_15 (Functional)       (None, 512)                  1380070   ['image_input[0][0]']         \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)  [(None, 91)]                 0         []                            \n",
      "                                                                                                  \n",
      " repeat_vector_4 (RepeatVec  (None, 91, 512)              0         ['model_15[0][0]']            \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " model_16 (Functional)       (None, 91, 385)              1247105   ['decoder_input[0][0]',       \n",
      "                                                                     'repeat_vector_4[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15047809 (57.40 MB)\n",
      "Trainable params: 15047809 (57.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "eDbtc_4eAAI8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "FZZrLb4DAAI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 91)\n",
      "(50000, 91)\n",
      "(50000, 92)\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[..., :-1].shape)\n",
    "print(train_sequences[..., 1:].shape)\n",
    "print(train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "lzmbXT3WAAI8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "625/625 [==============================] - 611s 935ms/step - loss: 1.6553 - accuracy: 0.7445 - val_loss: 1.6281 - val_accuracy: 0.7468\n",
      "Epoch 2/100\n",
      "625/625 [==============================] - 575s 920ms/step - loss: 1.6359 - accuracy: 0.7457 - val_loss: 1.6268 - val_accuracy: 0.7468\n",
      "Epoch 3/100\n",
      "625/625 [==============================] - 573s 917ms/step - loss: 1.6351 - accuracy: 0.7457 - val_loss: 1.6265 - val_accuracy: 0.7468\n",
      "Epoch 4/100\n",
      "625/625 [==============================] - 573s 917ms/step - loss: 1.6345 - accuracy: 0.7457 - val_loss: 1.6269 - val_accuracy: 0.7468\n",
      "Epoch 5/100\n",
      "625/625 [==============================] - 572s 916ms/step - loss: 1.6337 - accuracy: 0.7457 - val_loss: 1.6250 - val_accuracy: 0.7468\n",
      "Epoch 6/100\n",
      "625/625 [==============================] - 573s 917ms/step - loss: 1.6411 - accuracy: 0.7457 - val_loss: 1.6317 - val_accuracy: 0.7468\n",
      "Epoch 7/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6391 - accuracy: 0.7457 - val_loss: 1.6297 - val_accuracy: 0.7468\n",
      "Epoch 8/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6367 - accuracy: 0.7457 - val_loss: 1.6270 - val_accuracy: 0.7468\n",
      "Epoch 9/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6348 - accuracy: 0.7457 - val_loss: 1.6258 - val_accuracy: 0.7468\n",
      "Epoch 10/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6333 - accuracy: 0.7457 - val_loss: 1.6238 - val_accuracy: 0.7468\n",
      "Epoch 11/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6316 - accuracy: 0.7457 - val_loss: 1.6225 - val_accuracy: 0.7468\n",
      "Epoch 12/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6303 - accuracy: 0.7457 - val_loss: 1.6213 - val_accuracy: 0.7468\n",
      "Epoch 13/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6293 - accuracy: 0.7457 - val_loss: 1.6207 - val_accuracy: 0.7468\n",
      "Epoch 14/100\n",
      "625/625 [==============================] - 571s 915ms/step - loss: 1.6282 - accuracy: 0.7457 - val_loss: 1.6199 - val_accuracy: 0.7468\n",
      "Epoch 15/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6273 - accuracy: 0.7457 - val_loss: 1.6182 - val_accuracy: 0.7468\n",
      "Epoch 16/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6268 - accuracy: 0.7457 - val_loss: 1.6180 - val_accuracy: 0.7468\n",
      "Epoch 17/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6262 - accuracy: 0.7457 - val_loss: 1.6174 - val_accuracy: 0.7468\n",
      "Epoch 18/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6255 - accuracy: 0.7457 - val_loss: 1.6174 - val_accuracy: 0.7468\n",
      "Epoch 19/100\n",
      "625/625 [==============================] - 571s 914ms/step - loss: 1.6252 - accuracy: 0.7457 - val_loss: 1.6163 - val_accuracy: 0.7468\n",
      "Epoch 20/100\n",
      "625/625 [==============================] - 572s 915ms/step - loss: 1.6403 - accuracy: 0.7453 - val_loss: 1.6250 - val_accuracy: 0.7468\n",
      "Epoch 21/100\n",
      "108/625 [====>.........................] - ETA: 7:17 - loss: 1.6285 - accuracy: 0.7468"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit([train_images, train_sequences[..., :-1]],\n",
    "              train_sequences[..., 1:],\n",
    "              epochs=100,\n",
    "              batch_size=64,\n",
    "              validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGz_H92VAAI8"
   },
   "outputs": [],
   "source": [
    "transformer_model.save('/home/ubuntu/model_av.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jdeua_LzAAI9"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePe5hVCKG0df"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit([train_images, train_sequences[:, :-1]],\n",
    "              train_sequences[:, 1:],\n",
    "              epochs=20,\n",
    "              batch_size=128,\n",
    "              validation_split=0.2)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('/home/ubuntu/latex_model.keras')\n",
    "\n",
    "#model = load_model('latex_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_u6gxZzWVRk"
   },
   "outputs": [],
   "source": [
    "#dot_img_file =\n",
    "import keras\n",
    "keras.utils.plot_model(model,\n",
    "                       show_shapes=True,\n",
    "                       show_dtype=True,\n",
    "                       show_layer_names=True,\n",
    "                       expand_nested=True,\n",
    "                       show_layer_activations=True,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_WwX-x7YWRb"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.utils.plot_model(model,\n",
    "                       show_shapes=True,\n",
    "                       show_dtype=True,\n",
    "                       show_layer_names=True,\n",
    "                       expand_nested=True,\n",
    "                       show_layer_activations=True,\n",
    "                       to_file='/Users/jayaprakash/latex_model.png'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixV3kbyDPr8X"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjCZXuZ_Jn-x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_latex_sequence(model, image, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict LaTeX sequence from a single image.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained Keras model for predicting LaTeX sequence.\n",
    "    - image: Input image (preprocessed to match training dimensions).\n",
    "    - tokenizer: Tokenizer fitted on LaTeX sequences for decoding predictions.\n",
    "    - max_seq_len: Maximum sequence length for the predicted sequence.\n",
    "\n",
    "    Returns:\n",
    "    - latex_sequence: Predicted LaTeX sequence as a string.\n",
    "    \"\"\"\n",
    "    # Prepare input image and initialize the sequence\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    start_token = tokenizer.word_index[\"<START>\"]\n",
    "    end_token = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "    # Initial sequence with the start token\n",
    "    sequence = [start_token]\n",
    "\n",
    "    for _ in range(max_seq_len_1):\n",
    "        # Pad the current sequence to match input length\n",
    "        padded_sequence = np.pad(sequence, (0, max_seq_len_1 - len(sequence)), mode='constant')\n",
    "        padded_sequence = np.expand_dims(padded_sequence, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict next token\n",
    "        preds = model.predict([image, padded_sequence])\n",
    "        next_token = np.argmax(preds[0, len(sequence) - 1, :])\n",
    "\n",
    "        # Break if end token is reached\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "\n",
    "        # Add the predicted token to the sequence\n",
    "        sequence.append(next_token)\n",
    "\n",
    "    # Decode the token sequence to a string\n",
    "    latex_sequence = tokenizer.sequences_to_texts([sequence[1:]])[0]  # Skip the start token\n",
    "    return latex_sequence\n",
    "\n",
    "predicted_latex = predict_latex_sequence(model, train_images[12], tokenizer)\n",
    "print(\"Predicted LaTeX:\", predicted_latex)\n",
    "#print(\"Original Seq:\", train_sequences[0])\n",
    "print(\"Original Seq:\", train_latex_texts[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIlv7RgvQZ9y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9MKLOc1REDl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
